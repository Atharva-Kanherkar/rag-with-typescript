[
  {
    "id": "_index-p0",
    "content": "## What is a Pod?\n\n{{< note >}}\nYou need to install a [container runtime](/docs/setup/production-environment/container-runtimes/)\ninto each node in the cluster so that Pods can run there.\n{{< /note >}}\n\nThe shared context of a Pod is a set of Linux namespaces, cgroups, and\npotentially other facets of isolation - the same things that isolate a {{< glossary_tooltip text=\"container\" term_id=\"container\" >}}. Within a Pod's context, the individual applications may have\nfurther sub-isolations applied.\n\nA Pod is similar to a set of containers with shared namespaces and shared filesystem volumes.\n\nPods in a Kubernetes cluster are used in two main ways:\n\n* **Pods that run a single container**. The \"one-container-per-Pod\" model is the\n  most common Kubernetes use case; in this case, you can think of a Pod as a\n  wrapper around a single container; Kubernetes manages Pods rather than managing\n  the containers directly.\n* **Pods that run multiple containers that need to work together**. A Pod can\n  encapsulate an application composed of\n  [multiple co-located containers](#how-pods-manage-multiple-containers) that are\n  tightly coupled and need to share resources. These co-located containers\n  form a single cohesive unit.\n\n  Grouping multiple co-located and co-managed containers in a single Pod is a\n  relatively advanced use case. You should use this pattern only in specific\n  instances in which your containers are tightly coupled.\n\n  You don't need to run multiple containers to provide replication (for resilience\n  or capacity); if you need multiple replicas, see\n  [Workload management](/docs/concepts/workloads/controllers/).\n\n",
    "parent_id": null,
    "source_file": "data/kubernetes/content/en/docs/concepts/workloads/pods/_index.md",
    "section_path": [
      "concepts",
      "workloads",
      "pods"
    ],
    "chunk_type": "parent",
    "strategy": "recursive"
  },
  {
    "id": "_index-p1",
    "content": "## Using Pods\n\nThe following is an example of a Pod which consists of a container running the image `nginx:1.14.2`.\n\n{{% code_sample file=\"pods/simple-pod.yaml\" %}}\n\nTo create the Pod shown above, run the following command:\n```shell\nkubectl apply -f https://k8s.io/examples/pods/simple-pod.yaml\n```\n\nPods are generally not created directly and are created using workload resources.\nSee [Working with Pods](#working-with-pods) for more information on how Pods are used\nwith workload resources.\n\n### Workload resources for managing pods\n\nUsually you don't need to create Pods directly, even singleton Pods. Instead, create them using workload resources such as {{< glossary_tooltip text=\"Deployment\"\nterm_id=\"deployment\" >}} or {{< glossary_tooltip text=\"Job\" term_id=\"job\" >}}.\nIf your Pods need to track state, consider the\n{{< glossary_tooltip text=\"StatefulSet\" term_id=\"statefulset\" >}} resource.\n\n\nEach Pod is meant to run a single instance of a given application. If you want to\nscale your application horizontally (to provide more overall resources by running\nmore instances), you should use multiple Pods, one for each instance. In\nKubernetes, this is typically referred to as _replication_.\nReplicated Pods are usually created and managed as a group by a workload resource\nand its {{< glossary_tooltip text=\"controller\" term_id=\"controller\" >}}.\n\nSee [Pods and controllers](#pods-and-controllers) for more information on how\nKubernetes uses workload resources, and their controllers, to implement application\nscaling and auto-healing.\n\nPods natively provide two kinds of shared resources for their constituent containers:\n[networking](#pod-networking) and [storage](#pod-storage).\n\n\n",
    "parent_id": null,
    "source_file": "data/kubernetes/content/en/docs/concepts/workloads/pods/_index.md",
    "section_path": [
      "concepts",
      "workloads",
      "pods"
    ],
    "chunk_type": "parent",
    "strategy": "recursive"
  },
  {
    "id": "_index-p2",
    "content": "## Working with Pods\n\nYou'll rarely create individual Pods directly in Kubernetes—even singleton Pods. This\nis because Pods are designed as relatively ephemeral, disposable entities. When\na Pod gets created (directly by you, or indirectly by a\n{{< glossary_tooltip text=\"controller\" term_id=\"controller\" >}}), the new Pod is\nscheduled to run on a {{< glossary_tooltip term_id=\"node\" >}} in your cluster.\nThe Pod remains on that node until the Pod finishes execution, the Pod object is deleted,\nthe Pod is *evicted* for lack of resources, or the node fails.\n\n{{< note >}}\nRestarting a container in a Pod should not be confused with restarting a Pod. A Pod\nis not a process, but an environment for running container(s). A Pod persists until\nit is deleted.\n{{< /note >}}\n\nThe name of a Pod must be a valid\n[DNS subdomain](/docs/concepts/overview/working-with-objects/names#dns-subdomain-names)\nvalue, but this can produce unexpected results for the Pod hostname.  For best compatibility,\nthe name should follow the more restrictive rules for a\n[DNS label](/docs/concepts/overview/working-with-objects/names#dns-label-names).\n\n### Pod OS\n\n{{< feature-state state=\"stable\" for_k8s_version=\"v1.25\" >}}\n\nYou should set the `.spec.os.name` field to either `windows` or `linux` to indicate the OS on\nwhich you want the pod to run. These two are the only operating systems supported for now by\nKubernetes. In the future, this list may be expanded.\n\nIn Kubernetes v{{< skew currentVersion >}}, the value of `.spec.os.name` does not affect\nhow the {{< glossary_tooltip text=\"kube-scheduler\" term_id=\"kube-scheduler\" >}}\npicks a node for the Pod to run on. In any cluster where there is more than one operating system for\nrunning nodes, you should set the\n[kubernetes.io/os](/docs/reference/labels-annotations-taints/#kubernetes-io-os)\nlabel correctly on each node, and define pods with a `nodeSelector` based on the operating system\nlabel. The kube-scheduler assigns your pod to a node based on other criteria and may or may not\nsucceed in picking a suitable node placement where the node OS is right for the containers in that Pod.\nThe [Pod security standards](/docs/concepts/security/pod-security-standards/) also use this\nfield to avoid enforcing policies that aren't relevant to the operating system.\n\n### Pods and controllers\n\nYou can use workload resources to create and manage multiple Pods for you. A controller\nfor the resource handles replication and rollout and automatic healing in case of\nPod failure. For example, if a Node fails, a controller notices that Pods on that\nNode have stopped working and creates a replacement Pod. The scheduler places the\nreplacement Pod onto a healthy Node.\n\nHere are some examples of workload resources that manage one or more Pods:\n\n* {{< glossary_tooltip text=\"Deployment\" term_id=\"deployment\" >}}\n* {{< glossary_tooltip text=\"StatefulSet\" term_id=\"statefulset\" >}}\n* {{< glossary_tooltip text=\"DaemonSet\" term_id=\"daemonset\" >}}\n\n### Specifying a Workload reference\n\n{{< feature-state feature_gate_name=\"GenericWorkload\" >}}\n\nBy default, Kubernetes schedules every Pod individually. However, some tightly-coupled applications\nneed a group of Pods to be scheduled simultaneously to function correctly.\n\nYou can link a Pod to a [Workload](/docs/concepts/workloads/workload-api/) object\nusing a [Workload reference](/docs/concepts/workloads/pods/workload-reference/).\nThis tells the `kube-scheduler` that the Pod is part of a specific group,\nenabling it to make coordinated placement decisions for the entire group at once.\n\n### Pod templates\n\nControllers for {{< glossary_tooltip text=\"workload\" term_id=\"workload\" >}} resources create Pods\nfrom a _pod template_ and manage those Pods on your behalf.\n\nPodTemplates are specifications for creating Pods, and are included in workload resources such as\n[Deployments](/docs/concepts/workloads/controllers/deployment/),\n[Jobs](/docs/concepts/workloads/controllers/job/), and\n[DaemonSets](/docs/concepts/workloads/controllers/daemonset/).\n\nEach controller for a workload resource uses the `PodTemplate` inside the workload\nobject to make actual Pods. The `PodTemplate` is part of the desired state of whatever\nworkload resource you used to run your app.\n\nWhen you create a Pod, you can include\n[environment variables](/docs/tasks/inject-data-application/define-environment-variable-container/)\nin the Pod template for the containers that run in the Pod.\n\nThe sample below is a manifest for a simple Job with a `template` that starts one\ncontainer. The container in that Pod prints a message then pauses.\n\n```yaml\napiVersion: batch/v1\nkind: Job\nmetadata:\n  name: hello\nspec:\n  template:\n    # This is the pod template\n    spec:\n      containers:\n      - name: hello\n        image: busybox:1.28\n        command: ['sh', '-c', 'echo \"Hello, Kubernetes!\" && sleep 3600']\n      restartPolicy: OnFailure\n    # The pod template ends here\n```\n\nModifying the pod template or switching to a new pod template has no direct effect\non the Pods that already exist. If you change the pod template for a workload\nresource, that resource needs to create replacement Pods that use the updated template.\n\nFor example, the StatefulSet controller ensures that the running Pods match the current\npod template for each StatefulSet object. If you edit the StatefulSet to change its pod\ntemplate, the StatefulSet starts to create new Pods based on the updated template.\nEventually, all of the old Pods are replaced with new Pods, and the update is complete.\n\nEach workload resource implements its own rules for handling changes to the Pod template.\nIf you want to read more about StatefulSet specifically, read\n[Update strategy](/docs/tutorials/stateful-application/basic-stateful-set/#updating-statefulsets) in the StatefulSet Basics tutorial.\n\nOn Nodes, the {{< glossary_tooltip term_id=\"kubelet\" text=\"kubelet\" >}} does not\ndirectly observe or manage any of the details around pod templates and updates; those\ndetails are abstracted away. That abstraction and separation of concerns simplifies\nsystem semantics, and makes it feasible to extend the cluster's behavior without\nchanging existing code.\n\n",
    "parent_id": null,
    "source_file": "data/kubernetes/content/en/docs/concepts/workloads/pods/_index.md",
    "section_path": [
      "concepts",
      "workloads",
      "pods"
    ],
    "chunk_type": "parent",
    "strategy": "recursive"
  },
  {
    "id": "_index-p3",
    "content": "## Pod update and replacement\n\nAs mentioned in the previous section, when the Pod template for a workload\nresource is changed, the controller creates new Pods based on the updated\ntemplate instead of updating or patching the existing Pods.\n\nKubernetes doesn't prevent you from managing Pods directly. It is possible to\nupdate some fields of a running Pod, in place. However, Pod update operations\nlike\n[`patch`](/docs/reference/generated/kubernetes-api/{{< param \"version\" >}}/#patch-pod-v1-core), and\n[`replace`](/docs/reference/generated/kubernetes-api/{{< param \"version\" >}}/#replace-pod-v1-core)\nhave some limitations:\n\n- Most of the metadata about a Pod is immutable. For example, you cannot\n  change the `namespace`, `name`, `uid`, or `creationTimestamp` fields.\n\n- If the `metadata.deletionTimestamp` is set, no new entry can be added to the\n  `metadata.finalizers` list.\n- Pod updates may not change fields other than `spec.containers[*].image`,\n  `spec.initContainers[*].image`, `spec.activeDeadlineSeconds`, `spec.terminationGracePeriodSeconds`,\n  `spec.tolerations` or `spec.schedulingGates`. For `spec.tolerations`, you can only add new entries.\n- When updating the `spec.activeDeadlineSeconds` field, two types of updates\n  are allowed:\n\n  1. setting the unassigned field to a positive number;\n  1. updating the field from a positive number to a smaller, non-negative\n     number.\n\n### Pod subresources\n\nThe above update rules apply to regular pod updates, but other pod fields can be updated through _subresources_.\n\n- **Resize:** The `resize` subresource allows container resources (`spec.containers[*].resources`) to be updated.\n  See [Resize Container Resources](/docs/tasks/configure-pod-container/resize-container-resources/) for more details.\n- **Ephemeral Containers:** The `ephemeralContainers` subresource allows\n  {{< glossary_tooltip text=\"ephemeral containers\" term_id=\"ephemeral-container\" >}}\n  to be added to a Pod.\n  See [Ephemeral Containers](/docs/concepts/workloads/pods/ephemeral-containers/) for more details.\n- **Status:** The `status` subresource allows the pod status to be updated.\n  This is typically only used by the Kubelet and other system controllers.\n- **Binding:** The `binding` subresource allows setting the pod's `spec.nodeName` via a `Binding` request.\n  This is typically only used by the {{< glossary_tooltip text=\"scheduler\" term_id=\"kube-scheduler\" >}}.\n\n### Pod generation\n\n- The `metadata.generation` field is unique. It will be automatically set by the\n  system such that new pods have a `metadata.generation` of 1, and every update to\n  mutable fields in the pod's spec will increment the `metadata.generation` by 1.\n\n{{< feature-state feature_gate_name=\"PodObservedGenerationTracking\" >}}\n\n- `observedGeneration` is a field that is captured in the `status` section of the Pod\n  object. The Kubelet will set `status.observedGeneration`\n  to track the pod state to the current pod status. The pod's `status.observedGeneration` will reflect the\n  `metadata.generation` of the pod at the point that the pod status is being reported.\n\n{{< note >}}\nThe `status.observedGeneration` field is managed by the kubelet and external controllers should **not** modify this field.\n{{< /note >}}\n\nDifferent status fields may either be associated with the `metadata.generation` of the current sync loop, or with the\n`metadata.generation` of the previous sync loop. The key distinction is whether a change in the `spec` is reflected\ndirectly in the `status` or is an indirect result of a running process.\n\n#### Direct Status Updates\n\nFor status fields where the allocated spec is directly reflected, the `observedGeneration` will\nbe associated with the current `metadata.generation` (Generation N).\n\nThis behavior applies to:\n\n- **Resize Status**: The status of a resource resize operation.\n- **Allocated Resources**: The resources allocated to the Pod after a resize.\n- **Ephemeral Containers**: When a new ephemeral container is added, and it is in `Waiting` state.\n\n#### Indirect Status Updates\n\nFor status fields that are an indirect result of running the spec, the `observedGeneration` will be associated\nwith the `metadata.generation` of the previous sync loop (Generation N-1).\n\nThis behavior applies to:\n\n- **Container Image**: The `ContainerStatus.ImageID` reflects the image from the previous generation until the new image\n  is pulled and the container is updated.\n- **Actual Resources**: During an in-progress resize, the actual resources in use still belong to the previous generation's\n  request.\n- **Container state**: During an in-progress resize, with require restart policy reflects the previous generation's\n  request.\n- **activeDeadlineSeconds** & **terminationGracePeriodSeconds** & **deletionTimestamp**: The effects of these fields on the\n  Pod's status are a result of the previously observed specification.\n\n",
    "parent_id": null,
    "source_file": "data/kubernetes/content/en/docs/concepts/workloads/pods/_index.md",
    "section_path": [
      "concepts",
      "workloads",
      "pods"
    ],
    "chunk_type": "parent",
    "strategy": "recursive"
  },
  {
    "id": "_index-p4",
    "content": "## Resource sharing and communication\n\nPods enable data sharing and communication among their constituent\ncontainers.\n\n### Storage in Pods {#pod-storage}\n\nA Pod can specify a set of shared storage\n{{< glossary_tooltip text=\"volumes\" term_id=\"volume\" >}}. All containers\nin the Pod can access the shared volumes, allowing those containers to\nshare data. Volumes also allow persistent data in a Pod to survive\nin case one of the containers within needs to be restarted. See\n[Storage](/docs/concepts/storage/) for more information on how\nKubernetes implements shared storage and makes it available to Pods.\n\n### Pod networking\n\nEach Pod is assigned a unique IP address for each address family. Every\ncontainer in a Pod shares the network namespace, including the IP address and\nnetwork ports. Inside a Pod (and **only** then), the containers that belong to the Pod\ncan communicate with one another using `localhost`. When containers in a Pod communicate\nwith entities *outside the Pod*,\nthey must coordinate how they use the shared network resources (such as ports).\nWithin a Pod, containers share an IP address and port space, and\ncan find each other via `localhost`. The containers in a Pod can also communicate\nwith each other using standard inter-process communications like SystemV semaphores\nor POSIX shared memory.  Containers in different Pods have distinct IP addresses\nand can not communicate by OS-level IPC without special configuration.\nContainers that want to interact with a container running in a different Pod can\nuse IP networking to communicate.\n\nContainers within the Pod see the system hostname as being the same as the configured\n`name` for the Pod. There's more about this in the [networking](/docs/concepts/cluster-administration/networking/)\nsection.\n\n",
    "parent_id": null,
    "source_file": "data/kubernetes/content/en/docs/concepts/workloads/pods/_index.md",
    "section_path": [
      "concepts",
      "workloads",
      "pods"
    ],
    "chunk_type": "parent",
    "strategy": "recursive"
  },
  {
    "id": "_index-p5",
    "content": "## Pod security settings {#pod-security}\n\nTo set security constraints on Pods and containers, you use the\n`securityContext` field in the Pod specification. This field gives you\ngranular control over what a Pod or individual containers can do. See [Advanced Pod Configuration](/docs/concepts/workloads/pods/advanced-pod-config/) for more details.\n\nFor basic security configuration, you should meet the Baseline Pod security standard and run containers as non-root. You can set simple security contexts:\n\n```yaml\napiVersion: v1\nkind: Pod\nmetadata:\n  name: security-context-demo\nspec:\n  securityContext:\n    runAsUser: 1000\n    runAsGroup: 3000\n    fsGroup: 2000\n  containers:\n  - name: sec-ctx-demo\n    image: busybox\n    command: [\"sh\", \"-c\", \"sleep 1h\"]\n```\n\nFor advanced security context configuration including capabilities, seccomp profiles, and detailed security options, see the [security concepts](/docs/concepts/security/) section.\n\n* To learn about kernel-level security constraints that you can use,\n  see [Linux kernel security constraints for Pods and containers](/docs/concepts/security/linux-kernel-security-constraints).\n* To learn more about the Pod security context, see\n  [Configure a Security Context for a Pod or Container](/docs/tasks/configure-pod-container/security-context/).\n\n",
    "parent_id": null,
    "source_file": "data/kubernetes/content/en/docs/concepts/workloads/pods/_index.md",
    "section_path": [
      "concepts",
      "workloads",
      "pods"
    ],
    "chunk_type": "parent",
    "strategy": "recursive"
  },
  {
    "id": "_index-p6",
    "content": "## Static Pods\n\n_Static Pods_ are managed directly by the kubelet daemon on a specific node,\nwithout the {{< glossary_tooltip text=\"API server\" term_id=\"kube-apiserver\" >}}\nobserving them.\nWhereas most Pods are managed by the control plane (for example, a\n{{< glossary_tooltip text=\"Deployment\" term_id=\"deployment\" >}}), for static\nPods, the kubelet directly supervises each static Pod (and restarts it if it fails).\n\nStatic Pods are always bound to one {{< glossary_tooltip term_id=\"kubelet\" >}} on a specific node.\nThe main use for static Pods is to run a self-hosted control plane: in other words,\nusing the kubelet to supervise the individual [control plane components](/docs/concepts/architecture/#control-plane-components).\n\nThe kubelet automatically tries to create a {{< glossary_tooltip text=\"mirror Pod\" term_id=\"mirror-pod\" >}}\non the Kubernetes API server for each static Pod.\nThis means that the Pods running on a node are visible on the API server,\nbut cannot be controlled from there. See the guide [Create static Pods](/docs/tasks/configure-pod-container/static-pod)\nfor more information.\n\n{{< note >}}\nThe `spec` of a static Pod cannot refer to other API objects\n(e.g., {{< glossary_tooltip text=\"ServiceAccount\" term_id=\"service-account\" >}},\n{{< glossary_tooltip text=\"ConfigMap\" term_id=\"configmap\" >}},\n{{< glossary_tooltip text=\"Secret\" term_id=\"secret\" >}}, etc).\n{{< /note >}}\n\n",
    "parent_id": null,
    "source_file": "data/kubernetes/content/en/docs/concepts/workloads/pods/_index.md",
    "section_path": [
      "concepts",
      "workloads",
      "pods"
    ],
    "chunk_type": "parent",
    "strategy": "recursive"
  },
  {
    "id": "_index-p7",
    "content": "## Pods with multiple containers {#how-pods-manage-multiple-containers}\n\nPods are designed to support multiple cooperating processes (as containers) that form\na cohesive unit of service. The containers in a Pod are automatically co-located and\nco-scheduled on the same physical or virtual machine in the cluster. The containers\ncan share resources and dependencies, communicate with one another, and coordinate\nwhen and how they are terminated.\n\n<!--intentionally repeats some text from earlier in the page, with more detail -->\nPods in a Kubernetes cluster are used in two main ways:\n\n* **Pods that run a single container**. The \"one-container-per-Pod\" model is the\n  most common Kubernetes use case; in this case, you can think of a Pod as a\n  wrapper around a single container; Kubernetes manages Pods rather than managing\n  the containers directly.\n* **Pods that run multiple containers that need to work together**. A Pod can\n  encapsulate an application composed of\n  multiple co-located containers that are\n  tightly coupled and need to share resources. These co-located containers\n  form a single cohesive unit of service—for example, one container serving data\n  stored in a shared volume to the public, while a separate\n  {{< glossary_tooltip text=\"sidecar container\" term_id=\"sidecar-container\" >}}\n  refreshes or updates those files.\n  The Pod wraps these containers, storage resources, and an ephemeral network\n  identity together as a single unit.\n\nFor example, you might have a container that\nacts as a web server for files in a shared volume, and a separate\n[sidecar container](/docs/concepts/workloads/pods/sidecar-containers/)\nthat updates those files from a remote source, as in the following diagram:\n\n{{< figure src=\"/images/docs/pod.svg\" alt=\"Pod creation diagram\" class=\"diagram-medium\" >}}\n\nSome Pods have {{< glossary_tooltip text=\"init containers\" term_id=\"init-container\" >}}\nas well as {{< glossary_tooltip text=\"app containers\" term_id=\"app-container\" >}}.\nBy default, init containers run and complete before the app containers are started.\n\nYou can also have [sidecar containers](/docs/concepts/workloads/pods/sidecar-containers/)\nthat provide auxiliary services to the main application Pod (for example: a service mesh).\n\n{{< feature-state feature_gate_name=\"SidecarContainers\" >}}\n\nEnabled by default, the `SidecarContainers` [feature gate](/docs/reference/command-line-tools-reference/feature-gates/)\nallows you to specify `restartPolicy: Always` for init containers.\nSetting the `Always` restart policy ensures that the containers where you set it are\ntreated as _sidecars_ that are kept running during the entire lifetime of the Pod.\nContainers that you explicitly define as sidecar containers\nstart up before the main application Pod and remain running until the Pod is\nshut down.\n\n\n",
    "parent_id": null,
    "source_file": "data/kubernetes/content/en/docs/concepts/workloads/pods/_index.md",
    "section_path": [
      "concepts",
      "workloads",
      "pods"
    ],
    "chunk_type": "parent",
    "strategy": "recursive"
  },
  {
    "id": "_index-p8",
    "content": "## Container probes\n\nA _probe_ is a diagnostic performed periodically by the kubelet on a container.\nTo perform a diagnostic, the kubelet can invoke different actions:\n\n- `ExecAction` (performed with the help of the container runtime)\n- `TCPSocketAction` (checked directly by the kubelet)\n- `HTTPGetAction` (checked directly by the kubelet)\n\nYou can read more about [probes](/docs/concepts/workloads/pods/pod-lifecycle/#container-probes)\nin the Pod Lifecycle documentation.\n\n",
    "parent_id": null,
    "source_file": "data/kubernetes/content/en/docs/concepts/workloads/pods/_index.md",
    "section_path": [
      "concepts",
      "workloads",
      "pods"
    ],
    "chunk_type": "parent",
    "strategy": "recursive"
  },
  {
    "id": "_index-p9",
    "content": "## {{% heading \"whatsnext\" %}}\n\n* Learn about the [lifecycle of a Pod](/docs/concepts/workloads/pods/pod-lifecycle/).\n* Read about [PodDisruptionBudget](/docs/concepts/workloads/pods/disruptions/)\n  and how you can use it to manage application availability during disruptions.\n* Pod is a top-level resource in the Kubernetes REST API.\n  The {{< api-reference page=\"workload-resources/pod-v1\" >}}\n  object definition describes the object in detail.\n* [The Distributed System Toolkit: Patterns for Composite Containers](/blog/2015/06/the-distributed-system-toolkit-patterns/) explains common layouts for Pods with more than one container.\n* Read about [Pod topology spread constraints](/docs/concepts/scheduling-eviction/topology-spread-constraints/)\n* Read [Advanced Pod Configuration](/docs/concepts/workloads/pods/advanced-pod-config/) to learn the topic in detail.\n  That page covers aspects of Pod configuration beyond the essentials, including:\n  * PriorityClasses\n  * RuntimeClasses\n  * advanced ways to configure _scheduling_: the way that Kubernetes decides which node a Pod should run on.\n\nTo understand the context for why Kubernetes wraps a common Pod API in other resources\n(such as {{< glossary_tooltip text=\"StatefulSets\" term_id=\"statefulset\" >}} or\n{{< glossary_tooltip text=\"Deployments\" term_id=\"deployment\" >}}),\nyou can read about the prior art, including:\n\n* [Aurora](https://aurora.apache.org/documentation/latest/reference/configuration/#job-schema)\n* [Borg](https://research.google/pubs/large-scale-cluster-management-at-google-with-borg/)\n* [Marathon](https://github.com/d2iq-archive/marathon)\n* [Omega](https://research.google/pubs/pub41684/)\n* [Tupperware](https://engineering.fb.com/data-center-engineering/tupperware/).\n",
    "parent_id": null,
    "source_file": "data/kubernetes/content/en/docs/concepts/workloads/pods/_index.md",
    "section_path": [
      "concepts",
      "workloads",
      "pods"
    ],
    "chunk_type": "parent",
    "strategy": "recursive"
  },
  {
    "id": "advanced-pod-config-p0",
    "content": "## PriorityClasses\n\n_PriorityClasses_ allow you to set the importance of Pods relative to other Pods.\nIf you assign a priority class to a Pod, Kubernetes sets the `.spec.priority` field for that Pod\nbased on the PriorityClass you specified (you cannot set `.spec.priority` directly).\nIf or when a Pod cannot be scheduled, and the problem is due to a lack of resources, the {{< glossary_tooltip term_id=\"kube-scheduler\" text=\"kube-scheduler\" >}}\ntries to {{< glossary_tooltip text=\"preempt\" term_id=\"preemption\" >}} lower priority\nPods, in order to make scheduling of the higher priority Pod possible.\n\nA PriorityClass is a cluster-scoped API object that maps a priority class name to an integer priority value. Higher numbers indicate higher priority.\n\n### Defining a PriorityClass\n\n```yaml\napiVersion: scheduling.k8s.io/v1\nkind: PriorityClass\nmetadata:\n  name: high-priority\nvalue: 10000\nglobalDefault: false\ndescription: \"Priority class for high-priority workloads\"\n```\n\n### Specify pod priority using a PriorityClass\n\n{{< highlight yaml \"hl_lines=9\" >}}\napiVersion: v1\nkind: Pod\nmetadata:\n  name: nginx\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n  priorityClassName: high-priority\n{{< /highlight >}}\n\n### Built-in PriorityClasses\n\nKubernetes provides two built-in PriorityClasses:\n- `system-cluster-critical`: For system components that are critical to the cluster\n- `system-node-critical`: For system components that are critical to individual nodes. This is the highest priority that Pods can have in Kubernetes.\n\nFor more information, see [Pod Priority and Preemption](/docs/concepts/scheduling-eviction/pod-priority-preemption/).\n\n",
    "parent_id": null,
    "source_file": "data/kubernetes/content/en/docs/concepts/workloads/pods/advanced-pod-config.md",
    "section_path": [
      "concepts",
      "workloads",
      "pods"
    ],
    "chunk_type": "parent",
    "strategy": "recursive"
  },
  {
    "id": "advanced-pod-config-p1",
    "content": "## RuntimeClasses\n\nA _RuntimeClass_ allows you to specify the low-level container runtime for a Pod. It is useful when you want to specify different container runtimes for different kinds of Pod, such as when you need different isolation levels or runtime features.\n\n### Example Pod {#runtimeclass-pod-example}\n\n{{< highlight yaml \"hl_lines=6\" >}}\napiVersion: v1\nkind: Pod\nmetadata:\n  name: mypod\nspec:\n  runtimeClassName: myclass\n  containers:\n  - name: mycontainer\n    image: nginx\n{{< /highlight >}}\n\nA [RuntimeClass](/docs/concepts/containers/runtime-class/) is a cluster-scoped object that represents a container runtime that is available on some or all of your node.\n\nThe cluster administrator installs and configures the concrete runtimes backing the RuntimeClass.\n\nThey might set up that special container runtime configuration on all nodes, or perhaps just on some of them.\n\nFor more information, see the [RuntimeClass](/docs/concepts/containers/runtime-class/) documentation.\n\n",
    "parent_id": null,
    "source_file": "data/kubernetes/content/en/docs/concepts/workloads/pods/advanced-pod-config.md",
    "section_path": [
      "concepts",
      "workloads",
      "pods"
    ],
    "chunk_type": "parent",
    "strategy": "recursive"
  },
  {
    "id": "advanced-pod-config-p2",
    "content": "## Pod and container level security context configuration {#security-context}\n\nThe `Security context` field in the Pod specification provides granular control over security settings for Pods and containers.\n\n### Pod-wide `securityContext` {#pod-level-security-context}\n\nSome aspects of security apply to the whole Pod; for other aspects,\nyou might want to set a default, without any container-level overrides.\n\nHere's an example of using `securityContext` at the Pod level:\n\n#### Example Pod {#pod-level-security-context-example}\n\n{{< highlight yaml \"hl_lines=5-9\" >}}\napiVersion: v1\nkind: Pod\nmetadata:\n  name: security-context-demo\nspec:\n  securityContext:  # This applies to the entire Pod\n    runAsUser: 1000\n    runAsGroup: 3000\n    fsGroup: 2000\n  containers:\n  - name: sec-ctx-demo\n    image: registry.k8s.io/e2e-test-images/agnhost:2.45\n    command: [\"sh\", \"-c\", \"sleep 1h\"]\n{{< /highlight >}}\n\n### Container-level security context {#container-level-security-context}\n\nYou can specify the security context just for a specific container.\nHere's an example:\n\n#### Example Pod {#container-level-security-context-example}\n\n{{< highlight yaml \"hl_lines=9-17\" >}}\napiVersion: v1\nkind: Pod\nmetadata:\n  name: security-context-demo-2\nspec:\n  containers:\n  - name: sec-ctx-demo-2\n    image: gcr.io/google-samples/node-hello:1.0\n    securityContext:\n      allowPrivilegeEscalation: false\n      runAsNonRoot: true\n      runAsUser: 1000\n      capabilities:\n        drop:\n        - ALL\n      seccompProfile:\n        type: RuntimeDefault\n{{< /highlight >}}\n\n### Security context options\n\n- **User and Group IDs**: Control which user/group the container runs as\n- **Capabilities**: Add or drop Linux capabilities\n- **Seccomp Profiles**: Set security computing profiles\n- **SELinux Options**: Configure SELinux context\n- **AppArmor**: Configure AppArmor profiles for additional access control\n- **Windows Options**: Configure Windows-specific security settings\n\n{{< caution >}}\nYou can also use the Pod `securityContext` to allow\n[_privileged mode_](/docs/concepts/security/linux-kernel-security-constraints/#privileged-containers)\nin Linux containers. Privileged mode overrides many of the other security settings in the `securityContext`.\nAvoid using this setting unless you can't grant the equivalent permissions by using other fields in the `securityContext`.\nYou can run Windows containers in a similarly\nprivileged mode by setting the `windowsOptions.hostProcess` flag on the\nPod-level security context. For details and instructions, see\n[Create a Windows HostProcess Pod](/docs/tasks/configure-pod-container/create-hostprocess-pod/).\n{{< /caution >}}\n\nFor more information, see [Configure a Security Context for a Pod or Container](/docs/tasks/configure-pod-container/security-context/).\n\n",
    "parent_id": null,
    "source_file": "data/kubernetes/content/en/docs/concepts/workloads/pods/advanced-pod-config.md",
    "section_path": [
      "concepts",
      "workloads",
      "pods"
    ],
    "chunk_type": "parent",
    "strategy": "recursive"
  },
  {
    "id": "advanced-pod-config-p3",
    "content": "## Influencing Pod scheduling decisions {#scheduling}\n\nKubernetes provides several mechanisms to control which nodes your Pods are scheduled on.\n\n### Node selectors\n\nThe simplest form of node selection constraint:\n\n{{< highlight yaml \"hl_lines=9-11\" >}}\napiVersion: v1\nkind: Pod\nmetadata:\n  name: nginx\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n  nodeSelector:\n    disktype: ssd\n{{< /highlight >}}\n\n### Node affinity\n\nNode affinity allows you to specify rules that constrain which nodes your Pod can be scheduled on. Here's an example of a Pod that prefers running on nodes labelled as being on a particular continent, selecting based on the value of [`topology.kubernetes.io/zone`](/docs/reference/labels-annotations-taints/#topologykubernetesiozone) label.\n\n{{< highlight yaml \"hl_lines=6-15\" >}}\napiVersion: v1\nkind: Pod\nmetadata:\n  name: with-node-affinity\nspec:\n  affinity:\n    nodeAffinity:\n      requiredDuringSchedulingIgnoredDuringExecution:\n        nodeSelectorTerms:\n        - matchExpressions:\n          - key: topology.kubernetes.io/zone\n            operator: In\n            values:\n            - antarctica-east1\n            - antarctica-west1\n  containers:\n  - name: with-node-affinity\n    image: registry.k8s.io/pause:3.8\n{{< /highlight >}}\n\n### Pod affinity and anti-affinity\n\nIn addition to node affinity, you can also constrain which nodes a Pod can be scheduled on based on the labels of _other Pods_ that are already running on nodes. Pod affinity allows you to specify rules about where a Pod should be placed relative to other Pods.\n\n{{< highlight yaml \"hl_lines=6-15\" >}}\napiVersion: v1\nkind: Pod\nmetadata:\n  name: with-pod-affinity\nspec:\n  affinity:\n    podAffinity:\n      requiredDuringSchedulingIgnoredDuringExecution:\n      - labelSelector:\n          matchExpressions:\n          - key: app\n            operator: In\n            values:\n            - database\n        topologyKey: topology.kubernetes.io/zone\n  containers:\n  - name: with-pod-affinity\n    image: registry.k8s.io/pause:3.8\n{{< /highlight >}}\n\n### Tolerations\n\n_Tolerations_ allow Pods to be scheduled on nodes with matching taints:\n\n{{< highlight yaml \"hl_lines=9-13\" >}}\napiVersion: v1\nkind: Pod\nmetadata:\n  name: mypod\nspec:\n  containers:\n  - name: myapp\n    image: nginx\n  tolerations:\n  - key: \"key\"\n    operator: \"Equal\"\n    value: \"value\"\n    effect: \"NoSchedule\"\n{{< /highlight >}}\n\nFor more information, see [Assign Pods to Nodes](/docs/concepts/scheduling-eviction/assign-pod-node/).\n\n",
    "parent_id": null,
    "source_file": "data/kubernetes/content/en/docs/concepts/workloads/pods/advanced-pod-config.md",
    "section_path": [
      "concepts",
      "workloads",
      "pods"
    ],
    "chunk_type": "parent",
    "strategy": "recursive"
  },
  {
    "id": "advanced-pod-config-p4",
    "content": "## Pod overhead\n\nPod overhead allows you to account for the resources consumed by the Pod infrastructure on top of the container requests and limits.\n\n{{< highlight yaml \"hl_lines=7-10\" >}}\n---\napiVersion: node.k8s.io/v1\nkind: RuntimeClass\nmetadata:\n  name: kvisor-runtime\nhandler: kvisor-runtime\noverhead:\n  podFixed:\n    memory: \"2Gi\"\n    cpu: \"500m\"\n---\napiVersion: v1\nkind: Pod\nmetadata:\n  name: mypod\nspec:\n  runtimeClassName: kvisor-runtime\n  containers:\n  - name: myapp\n    image: nginx\n    resources:\n      requests:\n        memory: \"64Mi\"\n        cpu: \"250m\"\n      limits:\n        memory: \"128Mi\"\n        cpu: \"500m\"\n{{< /highlight >}}\n\n\n",
    "parent_id": null,
    "source_file": "data/kubernetes/content/en/docs/concepts/workloads/pods/advanced-pod-config.md",
    "section_path": [
      "concepts",
      "workloads",
      "pods"
    ],
    "chunk_type": "parent",
    "strategy": "recursive"
  },
  {
    "id": "advanced-pod-config-p5",
    "content": "## {{% heading \"whatsnext\" %}}\n\n* Read about [Pod Priority and Preemption](/docs/concepts/scheduling-eviction/pod-priority-preemption/)\n* Read about [RuntimeClasses](/docs/concepts/containers/runtime-class/)\n* Explore [Configure a Security Context for a Pod or Container](/docs/tasks/configure-pod-container/security-context/)\n* Learn how Kubernetes [assigns Pods to Nodes](/docs/concepts/scheduling-eviction/assign-pod-node/)\n* [Pod Overhead](/docs/concepts/scheduling-eviction/pod-overhead/)\n",
    "parent_id": null,
    "source_file": "data/kubernetes/content/en/docs/concepts/workloads/pods/advanced-pod-config.md",
    "section_path": [
      "concepts",
      "workloads",
      "pods"
    ],
    "chunk_type": "parent",
    "strategy": "recursive"
  },
  {
    "id": "disruptions-p0",
    "content": "## Voluntary and involuntary disruptions\n\nPods do not disappear until someone (a person or a controller) destroys them, or\nthere is an unavoidable hardware or system software error.\n\nWe call these unavoidable cases *involuntary disruptions* to\nan application.  Examples are:\n\n- a hardware failure of the physical machine backing the node\n- cluster administrator deletes VM (instance) by mistake\n- cloud provider or hypervisor failure makes VM disappear\n- a kernel panic\n- the node disappears from the cluster due to cluster network partition\n- eviction of a pod due to the node being [out-of-resources](/docs/concepts/scheduling-eviction/node-pressure-eviction/).\n\nExcept for the out-of-resources condition, all these conditions\nshould be familiar to most users; they are not specific\nto Kubernetes.\n\nWe call other cases *voluntary disruptions*.  These include both\nactions initiated by the application owner and those initiated by a Cluster\nAdministrator.  Typical application owner actions include:\n\n- deleting the deployment or other controller that manages the pod\n- updating a deployment's pod template causing a restart\n- directly deleting a pod (e.g. by accident)\n\nCluster administrator actions include:\n\n- [Draining a node](/docs/tasks/administer-cluster/safely-drain-node/) for repair or upgrade.\n- Draining a node from a cluster to scale the cluster down (learn about\n[Node Autoscaling](/docs/concepts/cluster-administration/node-autoscaling/)).\n- Removing a pod from a node to permit something else to fit on that node.\n\nThese actions might be taken directly by the cluster administrator, or by automation\nrun by the cluster administrator, or by your cluster hosting provider.\n\nAsk your cluster administrator or consult your cloud provider or distribution documentation\nto determine if any sources of voluntary disruptions are enabled for your cluster.\nIf none are enabled, you can skip creating Pod Disruption Budgets.\n\n{{< caution >}}\nNot all voluntary disruptions are constrained by Pod Disruption Budgets. For example,\ndeleting deployments or pods bypasses Pod Disruption Budgets.\n{{< /caution >}}\n\n",
    "parent_id": null,
    "source_file": "data/kubernetes/content/en/docs/concepts/workloads/pods/disruptions.md",
    "section_path": [
      "concepts",
      "workloads",
      "pods"
    ],
    "chunk_type": "parent",
    "strategy": "recursive"
  },
  {
    "id": "disruptions-p1",
    "content": "## Dealing with disruptions\n\nHere are some ways to mitigate involuntary disruptions:\n\n- Ensure your pod [requests the resources](/docs/tasks/configure-pod-container/assign-memory-resource) it needs.\n- Replicate your application if you need higher availability.  (Learn about running replicated\n  [stateless](/docs/tasks/run-application/run-stateless-application-deployment/)\n  and [stateful](/docs/tasks/run-application/run-replicated-stateful-application/) applications.)\n- For even higher availability when running replicated applications,\n  spread applications across racks (using\n  [anti-affinity](/docs/concepts/scheduling-eviction/assign-pod-node/#affinity-and-anti-affinity))\n  or across zones (if using a\n  [multi-zone cluster](/docs/setup/multiple-zones).)\n\nThe frequency of voluntary disruptions varies.  On a basic Kubernetes cluster, there are\nno automated voluntary disruptions (only user-triggered ones).  However, your cluster administrator or hosting provider\nmay run some additional services which cause voluntary disruptions. For example,\nrolling out node software updates can cause voluntary disruptions. Also, some implementations\nof cluster (node) autoscaling may cause voluntary disruptions to defragment and compact nodes.\nYour cluster administrator or hosting provider should have documented what level of voluntary\ndisruptions, if any, to expect. Certain configuration options, such as\n[using PriorityClasses](/docs/concepts/scheduling-eviction/pod-priority-preemption/)\nin your pod spec can also cause voluntary (and involuntary) disruptions.\n\n\n",
    "parent_id": null,
    "source_file": "data/kubernetes/content/en/docs/concepts/workloads/pods/disruptions.md",
    "section_path": [
      "concepts",
      "workloads",
      "pods"
    ],
    "chunk_type": "parent",
    "strategy": "recursive"
  },
  {
    "id": "disruptions-p2",
    "content": "## Pod disruption budgets\n\n{{< feature-state for_k8s_version=\"v1.21\" state=\"stable\" >}}\n\nKubernetes offers features to help you run highly available applications even when you\nintroduce frequent voluntary disruptions.\n\nAs an application owner, you can create a PodDisruptionBudget (PDB) for each application.\nA PDB limits the number of Pods of a replicated application that are down simultaneously from\nvoluntary disruptions. For example, a quorum-based application would\nlike to ensure that the number of replicas running is never brought below the\nnumber needed for a quorum. A web front end might want to\nensure that the number of replicas serving load never falls below a certain\npercentage of the total.\n\nCluster managers and hosting providers should use tools which\nrespect PodDisruptionBudgets by calling the [Eviction API](/docs/tasks/administer-cluster/safely-drain-node/#eviction-api)\ninstead of directly deleting pods or deployments.\n\nFor example, the `kubectl drain` subcommand lets you mark a node as going out of\nservice. When you run `kubectl drain`, the tool tries to evict all of the Pods on\nthe Node you're taking out of service. The eviction request that `kubectl` submits on\nyour behalf may be temporarily rejected, so the tool periodically retries all failed\nrequests until all Pods on the target node are terminated, or until a configurable timeout\nis reached.\n\nA PDB specifies the number of replicas that an application can tolerate having, relative to how\nmany it is intended to have.  For example, a Deployment which has a `.spec.replicas: 5` is\nsupposed to have 5 pods at any given time.  If its PDB allows for there to be 4 at a time,\nthen the Eviction API will allow voluntary disruption of one (but not two) pods at a time.\n\nThe group of pods that comprise the application is specified using a label selector, the same\nas the one used by the application's controller (deployment, stateful-set, etc).\n\nThe \"intended\" number of pods is computed from the `.spec.replicas` of the workload resource\nthat is managing those pods. The control plane discovers the owning workload resource by\nexamining the `.metadata.ownerReferences` of the Pod.\n\n[Involuntary disruptions](#voluntary-and-involuntary-disruptions) cannot be prevented by PDBs; however they\ndo count against the budget.\n\nPods which are deleted or unavailable due to a rolling upgrade to an application do count\nagainst the disruption budget, but workload resources (such as Deployment and StatefulSet)\nare not limited by PDBs when doing rolling upgrades. Instead, the handling of failures\nduring application updates is configured in the spec for the specific workload resource.\n\nIt is recommended to set `AlwaysAllow` [Unhealthy Pod Eviction Policy](/docs/tasks/run-application/configure-pdb/#unhealthy-pod-eviction-policy)\nto your PodDisruptionBudgets to support eviction of misbehaving applications during a node drain.\nThe default behavior is to wait for the application pods to become [healthy](/docs/tasks/run-application/configure-pdb/#healthiness-of-a-pod)\nbefore the drain can proceed.\n\nWhen a pod is evicted using the eviction API, it is gracefully\n[terminated](/docs/concepts/workloads/pods/pod-lifecycle/#pod-termination), honoring the\n`terminationGracePeriodSeconds` setting in its [PodSpec](/docs/reference/generated/kubernetes-api/{{< param \"version\" >}}/#podspec-v1-core).\n\n",
    "parent_id": null,
    "source_file": "data/kubernetes/content/en/docs/concepts/workloads/pods/disruptions.md",
    "section_path": [
      "concepts",
      "workloads",
      "pods"
    ],
    "chunk_type": "parent",
    "strategy": "recursive"
  },
  {
    "id": "disruptions-p3",
    "content": "## PodDisruptionBudget example {#pdb-example}\n\nConsider a cluster with 3 nodes, `node-1` through `node-3`.\nThe cluster is running several applications.  One of them has 3 replicas initially called\n`pod-a`, `pod-b`, and `pod-c`.  Another, unrelated pod without a PDB, called `pod-x`, is also shown.\nInitially, the pods are laid out as follows:\n\n|       node-1         |       node-2        |       node-3       |\n|:--------------------:|:-------------------:|:------------------:|\n| pod-a  *available*   | pod-b *available*   | pod-c *available*  |\n| pod-x  *available*   |                     |                    |\n\nAll 3 pods are part of a deployment, and they collectively have a PDB which requires\nthere be at least 2 of the 3 pods to be available at all times.\n\nFor example, assume the cluster administrator wants to reboot into a new kernel version to fix a bug in the kernel.\nThe cluster administrator first tries to drain `node-1` using the `kubectl drain` command.\nThat tool tries to evict `pod-a` and `pod-x`.  This succeeds immediately.\nBoth pods go into the `terminating` state at the same time.\nThis puts the cluster in this state:\n\n|   node-1 *draining*  |       node-2        |       node-3       |\n|:--------------------:|:-------------------:|:------------------:|\n| pod-a  *terminating* | pod-b *available*   | pod-c *available*  |\n| pod-x  *terminating* |                     |                    |\n\nThe deployment notices that one of the pods is terminating, so it creates a replacement\ncalled `pod-d`.  Since `node-1` is cordoned, it lands on another node.  Something has\nalso created `pod-y` as a replacement for `pod-x`.\n\n(Note: for a StatefulSet, `pod-a`, which would be called something like `pod-0`, would need\nto terminate completely before its replacement, which is also called `pod-0` but has a\ndifferent UID, could be created.  Otherwise, the example applies to a StatefulSet as well.)\n\nNow the cluster is in this state:\n\n|   node-1 *draining*  |       node-2        |       node-3       |\n|:--------------------:|:-------------------:|:------------------:|\n| pod-a  *terminating* | pod-b *available*   | pod-c *available*  |\n| pod-x  *terminating* | pod-d *starting*    | pod-y              |\n\nAt some point, the pods terminate, and the cluster looks like this:\n\n|    node-1 *drained*  |       node-2        |       node-3       |\n|:--------------------:|:-------------------:|:------------------:|\n|                      | pod-b *available*   | pod-c *available*  |\n|                      | pod-d *starting*    | pod-y              |\n\nAt this point, if an impatient cluster administrator tries to drain `node-2` or\n`node-3`, the drain command will block, because there are only 2 available\npods for the deployment, and its PDB requires at least 2.  After some time passes, `pod-d` becomes available.\n\nThe cluster state now looks like this:\n\n|    node-1 *drained*  |       node-2        |       node-3       |\n|:--------------------:|:-------------------:|:------------------:|\n|                      | pod-b *available*   | pod-c *available*  |\n|                      | pod-d *available*   | pod-y              |\n\nNow, the cluster administrator tries to drain `node-2`.\nThe drain command will try to evict the two pods in some order, say\n`pod-b` first and then `pod-d`.  It will succeed at evicting `pod-b`.\nBut, when it tries to evict `pod-d`, it will be refused because that would leave only\none pod available for the deployment.\n\nThe deployment creates a replacement for `pod-b` called `pod-e`.\nBecause there are not enough resources in the cluster to schedule\n`pod-e` the drain will again block.  The cluster may end up in this\nstate:\n\n|    node-1 *drained*  |       node-2        |       node-3       | *no node*          |\n|:--------------------:|:-------------------:|:------------------:|:------------------:|\n|                      | pod-b *terminating* | pod-c *available*  | pod-e *pending*    |\n|                      | pod-d *available*   | pod-y              |                    |\n\nAt this point, the cluster administrator needs to\nadd a node back to the cluster to proceed with the upgrade.\n\nYou can see how Kubernetes varies the rate at which disruptions\ncan happen, according to:\n\n- how many replicas an application needs\n- how long it takes to gracefully shutdown an instance\n- how long it takes a new instance to start up\n- the type of controller\n- the cluster's resource capacity\n\n",
    "parent_id": null,
    "source_file": "data/kubernetes/content/en/docs/concepts/workloads/pods/disruptions.md",
    "section_path": [
      "concepts",
      "workloads",
      "pods"
    ],
    "chunk_type": "parent",
    "strategy": "recursive"
  },
  {
    "id": "disruptions-p4",
    "content": "## Pod disruption conditions {#pod-disruption-conditions}\n\n{{< feature-state feature_gate_name=\"PodDisruptionConditions\" >}}\n\nA dedicated Pod `DisruptionTarget` [condition](/docs/concepts/workloads/pods/pod-lifecycle/#pod-conditions)\nis added to indicate\nthat the Pod is about to be deleted due to a {{<glossary_tooltip term_id=\"disruption\" text=\"disruption\">}}.\nThe `reason` field of the condition additionally\nindicates one of the following reasons for the Pod termination:\n\n`PreemptionByScheduler`\n: Pod is due to be {{<glossary_tooltip term_id=\"preemption\" text=\"preempted\">}} by a scheduler in order to accommodate a new Pod with a higher priority. For more information, see [Pod priority preemption](/docs/concepts/scheduling-eviction/pod-priority-preemption/).\n\n`DeletionByTaintManager`\n: Pod is due to be deleted by Taint Manager (which is part of the node lifecycle controller within `kube-controller-manager`) due to a `NoExecute` taint that the Pod does not tolerate; see {{<glossary_tooltip term_id=\"taint\" text=\"taint\">}}-based evictions.\n\n`EvictionByEvictionAPI`\n: Pod has been marked for {{<glossary_tooltip term_id=\"api-eviction\" text=\"eviction using the Kubernetes API\">}} .\n\n`DeletionByPodGC`\n: Pod, that is bound to a no longer existing Node, is due to be deleted by [Pod garbage collection](/docs/concepts/workloads/pods/pod-lifecycle/#pod-garbage-collection).\n\n`TerminationByKubelet`\n: Pod has been terminated by the kubelet, because of either {{<glossary_tooltip term_id=\"node-pressure-eviction\" text=\"node pressure eviction\">}},\n  the [graceful node shutdown](/docs/concepts/architecture/nodes/#graceful-node-shutdown),\n  or preemption for [system critical pods](/docs/tasks/administer-cluster/guaranteed-scheduling-critical-addon-pods/).\n\nIn all other disruption scenarios, like eviction due to exceeding\n[Pod container limits](/docs/concepts/configuration/manage-resources-containers/),\nPods don't receive the `DisruptionTarget` condition because the disruptions were\nprobably caused by the Pod and would reoccur on retry.\n\n{{< note >}}\nA Pod disruption might be interrupted. The control plane might re-attempt to\ncontinue the disruption of the same Pod, but it is not guaranteed. As a result,\nthe `DisruptionTarget` condition might be added to a Pod, but that Pod might then not actually be\ndeleted. In such a situation, after some time, the\nPod disruption condition will be cleared.\n{{< /note >}}\n\nAlong with cleaning up the pods, the Pod garbage collector (PodGC) will also mark them as failed if they are in a non-terminal\nphase (see also [Pod garbage collection](/docs/concepts/workloads/pods/pod-lifecycle/#pod-garbage-collection)).\n\nWhen using a Job (or CronJob), you may want to use these Pod disruption conditions as part of your Job's\n[Pod failure policy](/docs/concepts/workloads/controllers/job#pod-failure-policy).\n\n",
    "parent_id": null,
    "source_file": "data/kubernetes/content/en/docs/concepts/workloads/pods/disruptions.md",
    "section_path": [
      "concepts",
      "workloads",
      "pods"
    ],
    "chunk_type": "parent",
    "strategy": "recursive"
  },
  {
    "id": "disruptions-p5",
    "content": "## Separating Cluster Owner and Application Owner Roles\n\nOften, it is useful to think of the Cluster Manager\nand Application Owner as separate roles with limited knowledge\nof each other.   This separation of responsibilities\nmay make sense in these scenarios:\n\n- when there are many application teams sharing a Kubernetes cluster, and\n  there is natural specialization of roles\n- when third-party tools or services are used to automate cluster management\n\nPod Disruption Budgets support this separation of roles by providing an\ninterface between the roles.\n\nIf you do not have such a separation of responsibilities in your organization,\nyou may not need to use Pod Disruption Budgets.\n\n",
    "parent_id": null,
    "source_file": "data/kubernetes/content/en/docs/concepts/workloads/pods/disruptions.md",
    "section_path": [
      "concepts",
      "workloads",
      "pods"
    ],
    "chunk_type": "parent",
    "strategy": "recursive"
  },
  {
    "id": "disruptions-p6",
    "content": "## How to perform Disruptive Actions on your Cluster\n\nIf you are a Cluster Administrator, and you need to perform a disruptive action on all\nthe nodes in your cluster, such as a node or system software upgrade, here are some options:\n\n- Accept downtime during the upgrade.\n- Failover to another complete replica cluster.\n   -  No downtime, but may be costly both for the duplicated nodes\n     and for human effort to orchestrate the switchover.\n- Write disruption tolerant applications and use PDBs.\n   - No downtime.\n   - Minimal resource duplication.\n   - Allows more automation of cluster administration.\n   - Writing disruption-tolerant applications is tricky, but the work to tolerate voluntary\n     disruptions largely overlaps with work to support autoscaling and tolerating\n     involuntary disruptions.\n\n\n\n\n",
    "parent_id": null,
    "source_file": "data/kubernetes/content/en/docs/concepts/workloads/pods/disruptions.md",
    "section_path": [
      "concepts",
      "workloads",
      "pods"
    ],
    "chunk_type": "parent",
    "strategy": "recursive"
  },
  {
    "id": "disruptions-p7",
    "content": "## {{% heading \"whatsnext\" %}}\n\n\n* Follow steps to protect your application by [configuring a Pod Disruption Budget](/docs/tasks/run-application/configure-pdb/).\n\n* Learn more about [draining nodes](/docs/tasks/administer-cluster/safely-drain-node/)\n\n* Learn about [updating a deployment](/docs/concepts/workloads/controllers/deployment/#updating-a-deployment)\n  including steps to maintain its availability during the rollout.\n\n",
    "parent_id": null,
    "source_file": "data/kubernetes/content/en/docs/concepts/workloads/pods/disruptions.md",
    "section_path": [
      "concepts",
      "workloads",
      "pods"
    ],
    "chunk_type": "parent",
    "strategy": "recursive"
  },
  {
    "id": "downward-api-p0",
    "content": "## Available fields\n\nOnly some Kubernetes API fields are available through the downward API. This\nsection lists which fields you can make available.\n\nYou can pass information from available Pod-level fields using `fieldRef`.\nAt the API level, the `spec` for a Pod always defines at least one\n[Container](/docs/reference/kubernetes-api/workload-resources/pod-v1/#Container).\nYou can pass information from available Container-level fields using\n`resourceFieldRef`.\n\n### Information available via `fieldRef` {#downwardapi-fieldRef}\n\nFor some Pod-level fields, you can provide them to a container either as\nan environment variable or using a `downwardAPI` volume. The fields available\nvia either mechanism are:\n\n`metadata.name`\n: the pod's name\n\n`metadata.namespace`\n: the pod's {{< glossary_tooltip text=\"namespace\" term_id=\"namespace\" >}}\n\n`metadata.uid`\n: the pod's unique ID\n\n`metadata.annotations['<KEY>']`\n: the value of the pod's {{< glossary_tooltip text=\"annotation\" term_id=\"annotation\" >}} named `<KEY>` (for example, `metadata.annotations['myannotation']`)\n\n`metadata.labels['<KEY>']`\n: the text value of the pod's {{< glossary_tooltip text=\"label\" term_id=\"label\" >}} named `<KEY>` (for example, `metadata.labels['mylabel']`)\n\nThe following information is available through environment variables\n**but not as a downwardAPI volume fieldRef**:\n\n`spec.serviceAccountName`\n: the name of the pod's {{< glossary_tooltip text=\"service account\" term_id=\"service-account\" >}}\n\n`spec.nodeName`\n: the name of the {{< glossary_tooltip term_id=\"node\" text=\"node\">}} where the Pod is executing\n\n`status.hostIP`\n: the primary IP address of the node to which the Pod is assigned\n\n`status.hostIPs`\n: the IP addresses is a dual-stack version of `status.hostIP`, the first is always the same as `status.hostIP`.\n\n`status.podIP`\n: the pod's primary IP address (usually, its IPv4 address)\n\n`status.podIPs`\n: the IP addresses is a dual-stack version of `status.podIP`, the first is always the same as `status.podIP`\n\nThe following information is available through a `downwardAPI` volume \n`fieldRef`, **but not as environment variables**:\n\n`metadata.labels`\n: all of the pod's labels, formatted as `label-key=\"escaped-label-value\"` with one label per line\n\n`metadata.annotations`\n: all of the pod's annotations, formatted as `annotation-key=\"escaped-annotation-value\"` with one annotation per line  \n\n### Information available via `resourceFieldRef` {#downwardapi-resourceFieldRef}\n\nThese container-level fields allow you to provide information about\n[requests and limits](/docs/concepts/configuration/manage-resources-containers/#requests-and-limits)\nfor resources such as CPU and memory.\n\n{{< note >}}\n{{< feature-state feature_gate_name=\"InPlacePodVerticalScaling\" >}}\nContainer CPU and memory resources can be resized while the container is running.\nIf this happens, a downward API volume will be updated,\nbut environment variables will not be updated unless the container restarts.\nSee [Resize CPU and Memory Resources assigned to Containers](/docs/tasks/configure-pod-container/resize-container-resources/)\nfor more details.\n{{< /note >}}\n\n\n`resource: limits.cpu`\n: A container's CPU limit\n\n`resource: requests.cpu`\n: A container's CPU request\n\n`resource: limits.memory`\n: A container's memory limit\n\n`resource: requests.memory`\n: A container's memory request\n\n`resource: limits.hugepages-*`\n: A container's hugepages limit\n\n`resource: requests.hugepages-*`\n: A container's hugepages request\n\n`resource: limits.ephemeral-storage`\n: A container's ephemeral-storage limit\n\n`resource: requests.ephemeral-storage`\n: A container's ephemeral-storage request\n\n#### Fallback information for resource limits\n\nIf CPU and memory limits are not specified for a container, and you use the\ndownward API to try to expose that information, then the\nkubelet defaults to exposing the maximum allocatable value for CPU and memory\nbased on the [node allocatable](/docs/tasks/administer-cluster/reserve-compute-resources/#node-allocatable)\ncalculation.\n\n",
    "parent_id": null,
    "source_file": "data/kubernetes/content/en/docs/concepts/workloads/pods/downward-api.md",
    "section_path": [
      "concepts",
      "workloads",
      "pods"
    ],
    "chunk_type": "parent",
    "strategy": "recursive"
  },
  {
    "id": "downward-api-p1",
    "content": "## {{% heading \"whatsnext\" %}}\n\nYou can read about [`downwardAPI` volumes](/docs/concepts/storage/volumes/#downwardapi).\n\nYou can try using the downward API to expose container- or Pod-level information:\n* as [environment variables](/docs/tasks/inject-data-application/environment-variable-expose-pod-information/)\n* as [files in `downwardAPI` volume](/docs/tasks/inject-data-application/downward-api-volume-expose-pod-information/)\n",
    "parent_id": null,
    "source_file": "data/kubernetes/content/en/docs/concepts/workloads/pods/downward-api.md",
    "section_path": [
      "concepts",
      "workloads",
      "pods"
    ],
    "chunk_type": "parent",
    "strategy": "recursive"
  },
  {
    "id": "ephemeral-containers-p0",
    "content": "## Understanding ephemeral containers\n\n{{< glossary_tooltip text=\"Pods\" term_id=\"pod\" >}} are the fundamental building\nblock of Kubernetes applications. Since Pods are intended to be disposable and\nreplaceable, you cannot add a container to a Pod once it has been created.\nInstead, you usually delete and replace Pods in a controlled fashion using\n{{< glossary_tooltip text=\"deployments\" term_id=\"deployment\" >}}.\n\nSometimes it's necessary to inspect the state of an existing Pod, however, for\nexample to troubleshoot a hard-to-reproduce bug. In these cases you can run\nan ephemeral container in an existing Pod to inspect its state and run\narbitrary commands.\n\n### What is an ephemeral container?\n\nEphemeral containers differ from other containers in that they lack guarantees\nfor resources or execution, and they will never be automatically restarted, so\nthey are not appropriate for building applications.  Ephemeral containers are\ndescribed using the same `ContainerSpec` as regular containers, but many fields\nare incompatible and disallowed for ephemeral containers.\n\n- Ephemeral containers may not have ports, so fields such as `ports`,\n  `livenessProbe`, `readinessProbe` are disallowed.\n- Pod resource allocations are immutable, so setting `resources` is disallowed.\n- For a complete list of allowed fields, see the [EphemeralContainer reference\n  documentation](/docs/reference/generated/kubernetes-api/{{< param \"version\" >}}/#ephemeralcontainer-v1-core).\n\nEphemeral containers are created using a special `ephemeralcontainers` handler\nin the API rather than by adding them directly to `pod.spec`, so it's not\npossible to add an ephemeral container using `kubectl edit`.\n\nLike regular containers, you may not change or remove an ephemeral container\nafter you have added it to a Pod.\n\n{{< note >}}\nEphemeral containers are not supported by [static pods](/docs/tasks/configure-pod-container/static-pod/).\n{{< /note >}}\n\n",
    "parent_id": null,
    "source_file": "data/kubernetes/content/en/docs/concepts/workloads/pods/ephemeral-containers.md",
    "section_path": [
      "concepts",
      "workloads",
      "pods"
    ],
    "chunk_type": "parent",
    "strategy": "recursive"
  },
  {
    "id": "ephemeral-containers-p1",
    "content": "## Uses for ephemeral containers\n\nEphemeral containers are useful for interactive troubleshooting when `kubectl\nexec` is insufficient because a container has crashed or a container image\ndoesn't include debugging utilities.\n\nIn particular, [distroless images](https://github.com/GoogleContainerTools/distroless)\nenable you to deploy minimal container images that reduce attack surface\nand exposure to bugs and vulnerabilities. Since distroless images do not include a\nshell or any debugging utilities, it's difficult to troubleshoot distroless\nimages using `kubectl exec` alone.\n\nWhen using ephemeral containers, it's helpful to enable [process namespace\nsharing](/docs/tasks/configure-pod-container/share-process-namespace/) so\nyou can view processes in other containers.\n\n",
    "parent_id": null,
    "source_file": "data/kubernetes/content/en/docs/concepts/workloads/pods/ephemeral-containers.md",
    "section_path": [
      "concepts",
      "workloads",
      "pods"
    ],
    "chunk_type": "parent",
    "strategy": "recursive"
  },
  {
    "id": "ephemeral-containers-p2",
    "content": "## {{% heading \"whatsnext\" %}}\n\n* Learn how to [debug pods using ephemeral containers](/docs/tasks/debug/debug-application/debug-running-pod/#ephemeral-container).\n\n",
    "parent_id": null,
    "source_file": "data/kubernetes/content/en/docs/concepts/workloads/pods/ephemeral-containers.md",
    "section_path": [
      "concepts",
      "workloads",
      "pods"
    ],
    "chunk_type": "parent",
    "strategy": "recursive"
  },
  {
    "id": "init-containers-p0",
    "content": "## Understanding init containers\n\nA {{< glossary_tooltip text=\"Pod\" term_id=\"pod\" >}} can have multiple containers\nrunning apps within it, but it can also have one or more init containers, which are run\nbefore the app containers are started.\n\nInit containers are exactly like regular containers, except:\n\n* Init containers always run to completion.\n* Each init container must complete successfully before the next one starts.\n\nIf a Pod's init container fails, the kubelet repeatedly restarts that init container until it succeeds.\nHowever, if the Pod has a `restartPolicy` of Never, and an init container fails during startup of that Pod, Kubernetes treats the overall Pod as failed.\n\nTo specify an init container for a Pod, add the `initContainers` field into\nthe [Pod specification](/docs/reference/kubernetes-api/workload-resources/pod-v1/#PodSpec),\nas an array of `container` items (similar to the app `containers` field and its contents).\nSee [Container](/docs/reference/kubernetes-api/workload-resources/pod-v1/#Container) in the\nAPI reference for more details.\n\nThe status of the init containers is returned in `.status.initContainerStatuses`\nfield as an array of the container statuses (similar to the `.status.containerStatuses`\nfield).\n\n### Differences from regular containers\n\nInit containers support all the fields and features of app containers,\nincluding resource limits, [volumes](/docs/concepts/storage/volumes/), and security settings. However, the\nresource requests and limits for an init container are handled differently,\nas documented in [Resource sharing within containers](#resource-sharing-within-containers).\n\nRegular init containers (in other words: excluding sidecar containers) do not support the\n`lifecycle`, `livenessProbe`, `readinessProbe`, or `startupProbe` fields. Init containers\nmust run to completion before the Pod can be ready; sidecar containers continue running\nduring a Pod's lifetime, and _do_ support some probes. See [sidecar container](/docs/concepts/workloads/pods/sidecar-containers/)\nfor further details about sidecar containers.\n\nIf you specify multiple init containers for a Pod, kubelet runs each init\ncontainer sequentially. Each init container must succeed before the next can run.\nWhen all of the init containers have run to completion, kubelet initializes\nthe application containers for the Pod and runs them as usual.\n\n### Differences from sidecar containers\n\nInit containers run and complete their tasks before the main application container starts.\nUnlike [sidecar containers](/docs/concepts/workloads/pods/sidecar-containers),\ninit containers are not continuously running alongside the main containers.\n\nInit containers run to completion sequentially, and the main container does not start\nuntil all the init containers have successfully completed.\n\ninit containers do not support `lifecycle`, `livenessProbe`, `readinessProbe`, or\n`startupProbe` whereas sidecar containers support all these [probes](/docs/concepts/workloads/pods/pod-lifecycle/#types-of-probe) to control their lifecycle.\n\nInit containers share the same resources (CPU, memory, network) with the main application\ncontainers but do not interact directly with them. They can, however, use shared volumes\nfor data exchange.\n\n",
    "parent_id": null,
    "source_file": "data/kubernetes/content/en/docs/concepts/workloads/pods/init-containers.md",
    "section_path": [
      "concepts",
      "workloads",
      "pods"
    ],
    "chunk_type": "parent",
    "strategy": "recursive"
  },
  {
    "id": "init-containers-p1",
    "content": "## Using init containers\n\nBecause init containers have separate images from app containers, they\nhave some advantages for start-up related code:\n\n* Init containers can contain utilities or custom code for setup that are not present in an app\n  image. For example, there is no need to make an image `FROM` another image just to use a tool like\n  `sed`, `awk`, `python`, or `dig` during setup.\n* The application image builder and deployer roles can work independently without\n  the need to jointly build a single app image.\n* Init containers can run with a different view of the filesystem than app containers in the\n  same Pod. Consequently, they can be given access to\n  {{< glossary_tooltip text=\"Secrets\" term_id=\"secret\" >}} that app containers cannot access.\n* Because init containers run to completion before any app containers start, init containers offer\n  a mechanism to block or delay app container startup until a set of preconditions are met. Once\n  preconditions are met, all of the app containers in a Pod can start in parallel.\n* Init containers can securely run utilities or custom code that would otherwise make an app\n  container image less secure. By keeping unnecessary tools separate you can limit the attack\n  surface of your app container image.\n\n\n### Examples\nHere are some ideas for how to use init containers:\n\n* Wait for a {{< glossary_tooltip text=\"Service\" term_id=\"service\">}} to\n  be created, using a shell one-line command like:\n  ```shell\n  for i in {1..100}; do sleep 1; if nslookup myservice; then exit 0; fi; done; exit 1\n  ```\n\n* Register this Pod with a remote server from the downward API with a command like:\n  ```shell\n  curl -X POST http://$MANAGEMENT_SERVICE_HOST:$MANAGEMENT_SERVICE_PORT/register -d 'instance=$(<POD_NAME>)&ip=$(<POD_IP>)'\n  ```\n\n* Wait for some time before starting the app container with a command like\n  ```shell\n  sleep 60\n  ```\n\n* Clone a Git repository into a {{< glossary_tooltip text=\"Volume\" term_id=\"volume\" >}}\n\n* Place values into a configuration file and run a template tool to dynamically\n  generate a configuration file for the main app container. For example,\n  place the `POD_IP` value in a configuration and generate the main app\n  configuration file using Jinja.\n\n#### Init containers in use\n\nThis example defines a simple Pod that has two init containers.\nThe first waits for `myservice`, and the second waits for `mydb`. Once both\ninit containers complete, the Pod runs the app container from its `spec` section.\n\n```yaml\napiVersion: v1\nkind: Pod\nmetadata:\n  name: myapp-pod\n  labels:\n    app.kubernetes.io/name: MyApp\nspec:\n  containers:\n  - name: myapp-container\n    image: busybox:1.28\n    command: ['sh', '-c', 'echo The app is running! && sleep 3600']\n  initContainers:\n  - name: init-myservice\n    image: busybox:1.28\n    command: ['sh', '-c', \"until nslookup myservice.$(cat /var/run/secrets/kubernetes.io/serviceaccount/namespace).svc.cluster.local; do echo waiting for myservice; sleep 2; done\"]\n  - name: init-mydb\n    image: busybox:1.28\n    command: ['sh', '-c', \"until nslookup mydb.$(cat /var/run/secrets/kubernetes.io/serviceaccount/namespace).svc.cluster.local; do echo waiting for mydb; sleep 2; done\"]\n```\n\nYou can start this Pod by running:\n\n```shell\nkubectl apply -f myapp.yaml\n```\nThe output is similar to this:\n```\npod/myapp-pod created\n```\n\nAnd check on its status with:\n```shell\nkubectl get -f myapp.yaml\n```\nThe output is similar to this:\n```\nNAME        READY     STATUS     RESTARTS   AGE\nmyapp-pod   0/1       Init:0/2   0          6m\n```\n\nor for more details:\n```shell\nkubectl describe -f myapp.yaml\n```\nThe output is similar to this:\n```\nName:          myapp-pod\nNamespace:     default\n[...]\nLabels:        app.kubernetes.io/name=MyApp\nStatus:        Pending\n[...]\nInit Containers:\n  init-myservice:\n[...]\n    State:         Running\n[...]\n  init-mydb:\n[...]\n    State:         Waiting\n      Reason:      PodInitializing\n    Ready:         False\n[...]\nContainers:\n  myapp-container:\n[...]\n    State:         Waiting\n      Reason:      PodInitializing\n    Ready:         False\n[...]\nEvents:\n  FirstSeen    LastSeen    Count    From                      SubObjectPath                           Type          Reason        Message\n  ---------    --------    -----    ----                      -------------                           --------      ------        -------\n  16s          16s         1        {default-scheduler }                                              Normal        Scheduled     Successfully assigned myapp-pod to 172.17.4.201\n  16s          16s         1        {kubelet 172.17.4.201}    spec.initContainers{init-myservice}     Normal        Pulling       pulling image \"busybox\"\n  13s          13s         1        {kubelet 172.17.4.201}    spec.initContainers{init-myservice}     Normal        Pulled        Successfully pulled image \"busybox\"\n  13s          13s         1        {kubelet 172.17.4.201}    spec.initContainers{init-myservice}     Normal        Created       Created container init-myservice\n  13s          13s         1        {kubelet 172.17.4.201}    spec.initContainers{init-myservice}     Normal        Started       Started container init-myservice\n```\n\nTo see logs for the init containers in this Pod, run:\n```shell\nkubectl logs myapp-pod -c init-myservice # Inspect the first init container\nkubectl logs myapp-pod -c init-mydb      # Inspect the second init container\n```\n\nAt this point, those init containers will be waiting to discover {{< glossary_tooltip text=\"Services\" term_id=\"service\" >}} named\n`mydb` and `myservice`.\n\nHere's a configuration you can use to make those Services appear:\n\n```yaml\n---\napiVersion: v1\nkind: Service\nmetadata:\n  name: myservice\nspec:\n  ports:\n  - protocol: TCP\n    port: 80\n    targetPort: 9376\n---\napiVersion: v1\nkind: Service\nmetadata:\n  name: mydb\nspec:\n  ports:\n  - protocol: TCP\n    port: 80\n    targetPort: 9377\n```\n\nTo create the `mydb` and `myservice` services:\n\n```shell\nkubectl apply -f services.yaml\n```\nThe output is similar to this:\n```\nservice/myservice created\nservice/mydb created\n```\n\nYou'll then see that those init containers complete, and that the `myapp-pod`\nPod moves into the Running state:\n\n```shell\nkubectl get -f myapp.yaml\n```\nThe output is similar to this:\n```\nNAME        READY     STATUS    RESTARTS   AGE\nmyapp-pod   1/1       Running   0          9m\n```\n\nThis simple example should provide some inspiration for you to create your own\ninit containers. [What's next](#what-s-next) contains a link to a more detailed example.\n\n",
    "parent_id": null,
    "source_file": "data/kubernetes/content/en/docs/concepts/workloads/pods/init-containers.md",
    "section_path": [
      "concepts",
      "workloads",
      "pods"
    ],
    "chunk_type": "parent",
    "strategy": "recursive"
  },
  {
    "id": "init-containers-p2",
    "content": "## Detailed behavior\n\nDuring Pod startup, the kubelet delays running init containers until the networking\nand storage are ready. Then the kubelet runs the Pod's init containers in the order\nthey appear in the Pod's spec.\n\nEach init container must exit successfully before\nthe next container starts. If a container fails to start due to the runtime or\nexits with failure, it is retried according to the Pod `restartPolicy`. However,\nif the Pod `restartPolicy` is set to Always, the init containers use\n`restartPolicy` OnFailure.\n\nA Pod cannot be `Ready` until all init containers have succeeded. The ports on an\ninit container are not aggregated under a Service. A Pod that is initializing\nis in the `Pending` state but should have a condition `Initialized` set to false.\n\nIf the Pod [restarts](#pod-restart-reasons), or is restarted, all init containers\nmust execute again.\n\nChanges to the init container spec are limited to the container image field.\nDirectly altering the `image` field of  an init container does _not_ restart the\nPod or trigger its recreation. If the Pod has yet to start, that change may\nhave an effect on how the Pod boots up.\n\nFor a [pod template](/docs/concepts/workloads/pods/#pod-templates)\nyou can typically change any field for an init container; the impact of making\nthat change depends on where the pod template is used.\n\nBecause init containers can be restarted, retried, or re-executed, init container\ncode should be idempotent. In particular, code that writes into any `emptyDir` volume\nshould be prepared for the possibility that an output file already exists.\n\nInit containers have all of the fields of an app container. However, Kubernetes\nprohibits `readinessProbe` from being used because init containers cannot\ndefine readiness distinct from completion. This is enforced during validation.\n\nUse `activeDeadlineSeconds` on the Pod to prevent init containers from failing forever.\nThe active deadline includes init containers.\nHowever it is recommended to use `activeDeadlineSeconds` only if teams deploy their application\nas a Job, because `activeDeadlineSeconds` has an effect even after initContainer finished.\nThe Pod which is already running correctly would be killed by `activeDeadlineSeconds` if you set.\n\nThe name of each app and init container in a Pod must be unique; a\nvalidation error is thrown for any container sharing a name with another.\n\n### Resource sharing within containers\n\nGiven the order of execution for init, sidecar and app containers, the following rules\nfor resource usage apply:\n\n* The highest of any particular resource request or limit defined on all init\n  containers is the *effective init request/limit*. If any resource has no\n  resource limit specified this is considered as the highest limit.\n* The Pod's *effective request/limit* for a resource is the higher of:\n  * the sum of all app containers request/limit for a resource\n  * the effective init request/limit for a resource\n* Scheduling is done based on effective requests/limits, which means\n  init containers can reserve resources for initialization that are not used\n  during the life of the Pod.\n* The QoS (quality of service) tier of the Pod's *effective QoS tier* is the\n  QoS tier for init containers and app containers alike.\n\nQuota and limits are applied based on the effective Pod request and\nlimit.\n\n### Init containers and Linux cgroups {#cgroups}\n\nOn Linux, resource allocations for Pod level control groups (cgroups) are based on the effective Pod\nrequest and limit, the same as the scheduler.\n\n{{< comment >}}\nThis section also present under [sidecar containers](/docs/concepts/workloads/pods/sidecar-containers/) page.\nIf you're editing this section, change both places.\n{{< /comment >}}\n\n### Pod restart reasons\n\nA Pod can restart, causing re-execution of init containers, for the following\nreasons:\n\n* The Pod infrastructure container is restarted. This is uncommon and would\n  have to be done by someone with root access to nodes.\n* All containers in a Pod are terminated while `restartPolicy` is set to Always,\n  forcing a restart, and the init container completion record has been lost due\n  to {{< glossary_tooltip text=\"garbage collection\" term_id=\"garbage-collection\" >}}.\n\nThe Pod will not be restarted when the init container image is changed, or the\ninit container completion record has been lost due to garbage collection. This\napplies for Kubernetes v1.20 and later. If you are using an earlier version of\nKubernetes, consult the documentation for the version you are using.\n\n",
    "parent_id": null,
    "source_file": "data/kubernetes/content/en/docs/concepts/workloads/pods/init-containers.md",
    "section_path": [
      "concepts",
      "workloads",
      "pods"
    ],
    "chunk_type": "parent",
    "strategy": "recursive"
  },
  {
    "id": "init-containers-p3",
    "content": "## {{% heading \"whatsnext\" %}}\n\nLearn more about the following:\n* [Creating a Pod that has an init container](/docs/tasks/configure-pod-container/configure-pod-initialization/#create-a-pod-that-has-an-init-container).\n* [Debug init containers](/docs/tasks/debug/debug-application/debug-init-containers/).\n* Overview of [kubelet](/docs/reference/command-line-tools-reference/kubelet/) and [kubectl](/docs/reference/kubectl/).\n* [Types of probes](/docs/concepts/workloads/pods/pod-lifecycle/#types-of-probe): liveness, readiness, startup probe.\n* [Sidecar containers](/docs/concepts/workloads/pods/sidecar-containers).\n",
    "parent_id": null,
    "source_file": "data/kubernetes/content/en/docs/concepts/workloads/pods/init-containers.md",
    "section_path": [
      "concepts",
      "workloads",
      "pods"
    ],
    "chunk_type": "parent",
    "strategy": "recursive"
  },
  {
    "id": "pod-hostname-p0",
    "content": "## Default Pod hostname\n\nWhen a Pod is created, its hostname (as observed from within the Pod) \nis derived from the Pod's metadata.name value. \nBoth the hostname and its corresponding fully qualified domain name (FQDN) \nare set to the metadata.name value (from the Pod's perspective)\n\n```yaml\napiVersion: v1\nkind: Pod\nmetadata:\n  name: busybox-1\nspec:\n  containers:\n  - image: busybox:1.28\n    command:\n      - sleep\n      - \"3600\"\n    name: busybox\n```\nThe Pod created by this manifest will have its hostname and fully qualified domain name (FQDN) set to `busybox-1`.\n\n",
    "parent_id": null,
    "source_file": "data/kubernetes/content/en/docs/concepts/workloads/pods/pod-hostname.md",
    "section_path": [
      "concepts",
      "workloads",
      "pods"
    ],
    "chunk_type": "parent",
    "strategy": "recursive"
  },
  {
    "id": "pod-hostname-p1",
    "content": "## Hostname with pod's hostname and subdomain fields\nThe Pod spec includes an optional `hostname` field. \nWhen set, this value takes precedence over the Pod's `metadata.name` as the \nhostname (observed from within the Pod).\nFor example, a Pod with spec.hostname set to `my-host` will have its hostname set to `my-host`.\n\nThe Pod spec also includes an optional `subdomain` field, \nindicating the Pod belongs to a subdomain within its namespace. \nIf a Pod has `spec.hostname` set to \"foo\" and spec.subdomain set \nto \"bar\" in the namespace `my-namespace`, its hostname becomes `foo` and its \nfully qualified domain name (FQDN) becomes \n`foo.bar.my-namespace.svc.cluster-domain.example` (observed from within the Pod).\n\nWhen both hostname and subdomain are set, the cluster's DNS server will \ncreate A and/or AAAA records based on these fields. \nRefer to: [Pod's hostname and subdomain fields](/docs/concepts/services-networking/dns-pod-service/#pod-hostname-and-subdomain-field).\n\n",
    "parent_id": null,
    "source_file": "data/kubernetes/content/en/docs/concepts/workloads/pods/pod-hostname.md",
    "section_path": [
      "concepts",
      "workloads",
      "pods"
    ],
    "chunk_type": "parent",
    "strategy": "recursive"
  },
  {
    "id": "pod-hostname-p2",
    "content": "## Hostname with pod's setHostnameAsFQDN fields\n\n{{< feature-state for_k8s_version=\"v1.22\" state=\"stable\" >}}\n\nWhen a Pod is configured to have fully qualified domain name (FQDN), its\nhostname is the short hostname. For example, if you have a Pod with the fully\nqualified domain name `busybox-1.busybox-subdomain.my-namespace.svc.cluster-domain.example`,\nthen by default the `hostname` command inside that Pod returns `busybox-1` and the\n`hostname --fqdn` command returns the FQDN.\n\nWhen both `setHostnameAsFQDN: true` and the subdomain field is set in the Pod spec,\nthe kubelet writes the Pod's FQDN\ninto the hostname for that Pod's namespace. In this case, both `hostname` and `hostname --fqdn`\nreturn the Pod's FQDN.\n\nThe Pod's FQDN is constructed in the same manner as previously defined.\nIt is composed of the Pod's `spec.hostname` (if specified) or `metadata.name` field,\nthe `spec.subdomain`, the `namespace` name, and the cluster domain suffix.\n\n{{< note >}}\nIn Linux, the hostname field of the kernel (the `nodename` field of `struct utsname`) is limited to 64 characters.\n\nIf a Pod enables this feature and its FQDN is longer than 64 character, it will fail to start.\nThe Pod will remain in `Pending` status (`ContainerCreating` as seen by `kubectl`) generating\nerror events, such as \"Failed to construct FQDN from Pod hostname and cluster domain\".\n\nThis means that when using this field, \nyou must ensure the combined length of the Pod's `metadata.name` (or `spec.hostname`) \nand `spec.subdomain` fields results in an FQDN that does not exceed 64 characters.\n{{< /note >}}\n\n",
    "parent_id": null,
    "source_file": "data/kubernetes/content/en/docs/concepts/workloads/pods/pod-hostname.md",
    "section_path": [
      "concepts",
      "workloads",
      "pods"
    ],
    "chunk_type": "parent",
    "strategy": "recursive"
  },
  {
    "id": "pod-hostname-p3",
    "content": "## Hostname with pod's hostnameOverride\n{{< feature-state feature_gate_name=\"HostnameOverride\" >}}\n\nSetting a value for `hostnameOverride` in the Pod spec causes the kubelet \nto unconditionally set both the Pod's hostname and fully qualified domain name (FQDN)\nto the `hostnameOverride` value. \n\nThe `hostnameOverride` field has a length limitation of 64 characters \nand must adhere to the DNS subdomain names standard defined in [RFC 1123](https://datatracker.ietf.org/doc/html/rfc1123).\n\nExample:\n```yaml\napiVersion: v1\nkind: Pod\nmetadata:\n  name: busybox-2-busybox-example-domain\nspec:\n  hostnameOverride: busybox-2.busybox.example.domain\n  containers:\n  - image: busybox:1.28\n    command:\n      - sleep\n      - \"3600\"\n    name: busybox\n```\n{{< note >}}\nThis only affects the hostname within the Pod; it does not affect the Pod's A or AAAA records in the cluster DNS server.\n{{< /note >}}\n\nIf `hostnameOverride` is set alongside `hostname` and `subdomain` fields:\n* The hostname inside the Pod is overridden to the `hostnameOverride` value.\n  \n* The Pod's A and/or AAAA records in the cluster DNS server are still generated based on the `hostname` and `subdomain` fields.\n\nNote: If `hostnameOverride` is set, you cannot simultaneously set the `hostNetwork` and `setHostnameAsFQDN` fields.\nThe API server will explicitly reject any create request attempting this combination.\n\nFor details on behavior when `hostnameOverride` is set in combination with \nother fields (hostname, subdomain, setHostnameAsFQDN, hostNetwork), \nsee the table in the [KEP-4762 design details](https://github.com/kubernetes/enhancements/blob/master/keps/sig-network/4762-allow-arbitrary-fqdn-as-pod-hostname/README.md#design-details ).",
    "parent_id": null,
    "source_file": "data/kubernetes/content/en/docs/concepts/workloads/pods/pod-hostname.md",
    "section_path": [
      "concepts",
      "workloads",
      "pods"
    ],
    "chunk_type": "parent",
    "strategy": "recursive"
  },
  {
    "id": "pod-lifecycle-p0",
    "content": "## Pod lifetime\n\nWhilst a Pod is running, the kubelet is able to restart containers to handle some\nkind of faults. Within a Pod, Kubernetes tracks different container\n[states](#container-states) and determines what action to take to make the Pod\nhealthy again.\n\nIn the Kubernetes API, Pods have both a specification and an actual status. The\nstatus for a Pod object consists of a set of [Pod conditions](#pod-conditions).\nYou can also inject [custom readiness information](#pod-readiness-gate) into the\ncondition data for a Pod, if that is useful to your application.\n\nPods are only [scheduled](/docs/concepts/scheduling-eviction/) once in their lifetime;\nassigning a Pod to a specific node is called _binding_, and the process of selecting\nwhich node to use is called _scheduling_.\nOnce a Pod has been scheduled and is bound to a node, Kubernetes tries\nto run that Pod on the node. The Pod runs on that node until it stops, or until the Pod\nis [terminated](#pod-termination); if Kubernetes isn't able to start the Pod on the selected\nnode (for example, if the node crashes before the Pod starts), then that particular Pod\nnever starts.\n\nYou can use [Pod Scheduling Readiness](/docs/concepts/scheduling-eviction/pod-scheduling-readiness/)\nto delay scheduling for a Pod until all its _scheduling gates_ are removed. For example,\nyou might want to define a set of Pods but only trigger scheduling once all the Pods\nhave been created.\n\n### Pods and fault recovery {#pod-fault-recovery}\n\nIf one of the containers in the Pod fails, then Kubernetes may try to restart that\nspecific container.\nRead [How Pods handle problems with containers](#container-restarts) to learn more.\n\nPods can however fail in a way that the cluster cannot recover from, and in that case\nKubernetes does not attempt to heal the Pod further; instead, Kubernetes deletes the\nPod and relies on other components to provide automatic healing.\n\nIf a Pod is scheduled to a {{< glossary_tooltip text=\"node\" term_id=\"node\" >}} and that\nnode then fails, the Pod is treated as unhealthy and Kubernetes eventually deletes the Pod.\nA Pod won't survive an {{< glossary_tooltip text=\"eviction\" term_id=\"eviction\" >}} due to\na lack of resources or Node maintenance.\n\nKubernetes uses a higher-level abstraction, called a\n{{< glossary_tooltip term_id=\"controller\" text=\"controller\" >}}, that handles the work of\nmanaging the relatively disposable Pod instances.\n\nA given Pod (as defined by a UID) is never \"rescheduled\" to a different node; instead,\nthat Pod can be replaced by a new, near-identical Pod. If you make a replacement Pod, it can\neven have same name (as in `.metadata.name`) that the old Pod had, but the replacement\nwould have a different `.metadata.uid` from the old Pod.\n\nKubernetes does not guarantee that a replacement for an existing Pod would be scheduled to\nthe same node as the old Pod that was being replaced.\n\n### Associated lifetimes\n\nWhen something is said to have the same lifetime as a Pod, such as a\n{{< glossary_tooltip term_id=\"volume\" text=\"volume\" >}},\nthat means that the thing exists as long as that specific Pod (with that exact UID)\nexists. If that Pod is deleted for any reason, and even if an identical replacement\nis created, the related thing (a volume, in this example) is also destroyed and\ncreated anew.\n\n{{< figure src=\"/images/docs/pod.svg\" title=\"Figure 1.\" class=\"diagram-medium\" caption=\"A multi-container Pod that contains a file puller [sidecar](/docs/concepts/workloads/pods/sidecar-containers/) and a web server. The Pod uses an [ephemeral `emptyDir` volume](/docs/concepts/storage/volumes/#emptydir) for shared storage between the containers.\" >}}\n\n",
    "parent_id": null,
    "source_file": "data/kubernetes/content/en/docs/concepts/workloads/pods/pod-lifecycle.md",
    "section_path": [
      "concepts",
      "workloads",
      "pods"
    ],
    "chunk_type": "parent",
    "strategy": "recursive"
  },
  {
    "id": "pod-lifecycle-p1",
    "content": "## Pod phase\n\nA Pod's `status` field is a\n[PodStatus](/docs/reference/generated/kubernetes-api/{{< param \"version\" >}}/#podstatus-v1-core)\nobject, which has a `phase` field.\n\nThe phase of a Pod is a simple, high-level summary of where the Pod is in its\nlifecycle. The phase is not intended to be a comprehensive rollup of observations\nof container or Pod state, nor is it intended to be a comprehensive state machine.\n\nThe number and meanings of Pod phase values are tightly guarded.\nOther than what is documented here, nothing should be assumed about Pods that\nhave a given `phase` value.\n\nHere are the possible values for `phase`:\n\nValue       | Description\n:-----------|:-----------\n`Pending`   | The Pod has been accepted by the Kubernetes cluster, but one or more of the containers has not been set up and made ready to run. This includes time a Pod spends waiting to be scheduled as well as the time spent downloading container images over the network.\n`Running`   | The Pod has been bound to a node, and all of the containers have been created. At least one container is still running, or is in the process of starting or restarting.\n`Succeeded` | All containers in the Pod have terminated in success, and will not be restarted.\n`Failed`    | All containers in the Pod have terminated, and at least one container has terminated in failure. That is, the container either exited with non-zero status or was terminated by the system, and is not set for automatic restarting.\n`Unknown`   | For some reason the state of the Pod could not be obtained. This phase typically occurs due to an error in communicating with the node where the Pod should be running.\n\n{{< note >}}\n\nWhen a pod is failing to start repeatedly, `CrashLoopBackOff` may appear in the `Status` field of some kubectl commands.\nSimilarly, when a pod is being deleted, `Terminating` may appear in the `Status` field of some kubectl commands.\n\nMake sure not to confuse _Status_, a kubectl display field for user intuition, with the pod's `phase`.\nPod phase is an explicit part of the Kubernetes data model and of the\n[Pod API](/docs/reference/kubernetes-api/workload-resources/pod-v1/). \n\n```\n  NAMESPACE               NAME               READY   STATUS             RESTARTS   AGE\n  alessandras-namespace   alessandras-pod    0/1     CrashLoopBackOff   200        2d9h\n```\n\nA Pod is granted a term to terminate gracefully, which defaults to 30 seconds.\nYou can use the flag `--force` to [terminate a Pod by force](/docs/concepts/workloads/pods/pod-lifecycle/#pod-termination-forced).\n{{< /note >}}\n\nSince Kubernetes 1.27, the kubelet transitions deleted Pods, except for\n[static Pods](/docs/tasks/configure-pod-container/static-pod/) and\n[force-deleted Pods](/docs/concepts/workloads/pods/pod-lifecycle/#pod-termination-forced)\nwithout a finalizer, to a terminal phase (`Failed` or `Succeeded` depending on\nthe exit statuses of the pod containers) before their deletion from the API server.\n\nIf a node dies or is disconnected from the rest of the cluster, Kubernetes\napplies a policy for setting the `phase` of all Pods on the lost node to Failed.\n\n",
    "parent_id": null,
    "source_file": "data/kubernetes/content/en/docs/concepts/workloads/pods/pod-lifecycle.md",
    "section_path": [
      "concepts",
      "workloads",
      "pods"
    ],
    "chunk_type": "parent",
    "strategy": "recursive"
  },
  {
    "id": "pod-lifecycle-p2",
    "content": "## Container states\n\nAs well as the [phase](#pod-phase) of the Pod overall, Kubernetes tracks the state of\neach container inside a Pod. You can use\n[container lifecycle hooks](/docs/concepts/containers/container-lifecycle-hooks/) to\ntrigger events to run at certain points in a container's lifecycle.\n\nOnce the {{< glossary_tooltip text=\"scheduler\" term_id=\"kube-scheduler\" >}}\nassigns a Pod to a Node, the kubelet starts creating containers for that Pod\nusing a {{< glossary_tooltip text=\"container runtime\" term_id=\"container-runtime\" >}}.\nThere are three possible container states: `Waiting`, `Running`, and `Terminated`.\n\nTo check the state of a Pod's containers, you can use\n`kubectl describe pod <name-of-pod>`. The output shows the state for each container\nwithin that Pod.\n\nEach state has a specific meaning:\n\n### `Waiting` {#container-state-waiting}\n\nIf a container is not in either the `Running` or `Terminated` state, it is `Waiting`.\nA container in the `Waiting` state is still running the operations it requires in\norder to complete start up: for example, pulling the container image from a container\nimage registry, or applying {{< glossary_tooltip text=\"Secret\" term_id=\"secret\" >}}\ndata.\nWhen you use `kubectl` to query a Pod with a container that is `Waiting`, you also see\na Reason field to summarize why the container is in that state.\n\n### `Running` {#container-state-running}\n\nThe `Running` status indicates that a container is executing without issues. If there\nwas a `postStart` hook configured, it has already executed and finished. When you use\n`kubectl` to query a Pod with a container that is `Running`, you also see information\nabout when the container entered the `Running` state.\n\n### `Terminated` {#container-state-terminated}\n\nA container in the `Terminated` state began execution and then either ran to\ncompletion or failed for some reason. When you use `kubectl` to query a Pod with\na container that is `Terminated`, you see a reason, an exit code, and the start and\nfinish time for that container's period of execution.\n\nIf a container has a `preStop` hook configured, this hook runs before the container enters\nthe `Terminated` state.\n\n",
    "parent_id": null,
    "source_file": "data/kubernetes/content/en/docs/concepts/workloads/pods/pod-lifecycle.md",
    "section_path": [
      "concepts",
      "workloads",
      "pods"
    ],
    "chunk_type": "parent",
    "strategy": "recursive"
  },
  {
    "id": "pod-lifecycle-p3",
    "content": "## How Pods handle problems with containers {#container-restarts}\n\nKubernetes manages container failures within Pods using a [`restartPolicy`](#restart-policy) defined\nin the Pod `spec`. This policy determines how Kubernetes reacts to containers exiting due to errors\nor other reasons, which falls in the following sequence:\n\n1. **Initial crash**: Kubernetes attempts an immediate restart based on the Pod `restartPolicy`.\n1. **Repeated crashes**: After the initial crash Kubernetes applies an exponential\n   backoff delay for subsequent restarts, described in [`restartPolicy`](#restart-policy).\n   This prevents rapid, repeated restart attempts from overloading the system.\n1. **CrashLoopBackOff state**: This indicates that the backoff delay mechanism is currently\n   in effect for a given container that is in a crash loop, failing and restarting repeatedly.\n1. **Backoff reset**: If a container runs successfully for a certain duration\n   (e.g., 10 minutes), Kubernetes resets the backoff delay, treating any new crash\n   as the first one.\n\nIn practice, a `CrashLoopBackOff` is a condition or event that might be seen as output\nfrom the `kubectl` command, while describing or listing Pods, when a container in the Pod\nfails to start properly and then continually tries and fails in a loop.\n\nIn other words, when a container enters the crash loop, Kubernetes applies the\nexponential backoff delay mentioned in the [Container restart policy](#restart-policy).\nThis mechanism prevents a faulty container from overwhelming the system with continuous\nfailed start attempts.\n\nThe `CrashLoopBackOff` can be caused by issues like the following:\n\n* Application errors that cause the container to exit.\n* Configuration errors, such as incorrect environment variables or missing\n  configuration files.\n* Resource constraints, where the container might not have enough memory or CPU\n  to start properly.\n* Health checks failing if the application doesn't start serving within the\n  expected time.\n* Container liveness probes or startup probes returning a `Failure` result\n  as mentioned in the [probes section](#container-probes).\n\nTo investigate the root cause of a `CrashLoopBackOff` issue, a user can:\n\n1. **Check logs**: Use `kubectl logs <name-of-pod>` to check the logs of the container.\n   This is often the most direct way to diagnose the issue causing the crashes.\n1. **Inspect events**: Use `kubectl describe pod <name-of-pod>` to see events\n   for the Pod, which can provide hints about configuration or resource issues.\n1. **Review configuration**: Ensure that the Pod configuration, including\n   environment variables and mounted volumes, is correct and that all required\n   external resources are available.\n1. **Check resource limits**: Make sure that the container has enough CPU\n   and memory allocated. Sometimes, increasing the resources in the Pod definition\n   can resolve the issue.\n1. **Debug application**: There might exist bugs or misconfigurations in the\n   application code. Running this container image locally or in a development\n   environment can help diagnose application specific issues.\n\n### Container restarts {#restart-policy}\n\nWhen a container in your Pod stops, or experiences failure, Kubernetes can restart it.\nA restart isn't always appropriate; for example,\n{{< glossary_tooltip text=\"init containers\" term_id=\"init-container\" >}} run only once (if successful),\nduring Pod startup.\nYou can configure restarts as a policy that applies to all Pods, or using container-level configuration (for example: when you define a\n{{< glossary_tooltip text=\"sidecar container\" term_id=\"sidecar-container\" >}}) or define container-level override.\n\n#### Container restarts and resilience {#container-restart-resilience}\n\nThe Kubernetes project recommends following cloud-native principles, including resilient\ndesign that accounts for unannounced or arbitrary restarts. You can achieve this either\nby failing the Pod and relying on automatic\n[replacement](/docs/concepts/workloads/controllers/), or you can design for container-level resilience.\nEither approach helps to ensure that your overall workload remains available despite\npartial failure.\n\n#### Pod-level container restart policy\n\nThe `spec` of a Pod has a `restartPolicy` field with possible values Always, OnFailure,\nand Never. The default value is Always.\n\nThe `restartPolicy` for a Pod applies to {{< glossary_tooltip text=\"app containers\" term_id=\"app-container\" >}}\nin the Pod and to regular [init containers](/docs/concepts/workloads/pods/init-containers/).\n[Sidecar containers](/docs/concepts/workloads/pods/sidecar-containers/)\nignore the Pod-level `restartPolicy` field: in Kubernetes, a sidecar is defined as an\nentry inside `initContainers` that has its container-level `restartPolicy` set to `Always`.\nFor init containers that exit with an error, the kubelet restarts the init container if\nthe Pod level `restartPolicy` is either `OnFailure` or `Always`:\n\n* `Always`: Automatically restarts the container after any termination.\n* `OnFailure`: Only restarts the container if it exits with an error (non-zero exit status).\n* `Never`: Does not automatically restart the terminated container.\n\nWhen the kubelet is handling container restarts according to the configured restart\npolicy, that only applies to restarts that make replacement containers inside the\nsame Pod and running on the same node. After containers in a Pod exit, the kubelet\nrestarts them with an exponential backoff delay (10s, 20s, 40s, …), that is capped at\n300 seconds (5 minutes). Once a container has executed for 10 minutes without any\nproblems, the kubelet resets the restart backoff timer for that container.\n[Sidecar containers and Pod lifecycle](/docs/concepts/workloads/pods/sidecar-containers/#sidecar-containers-and-pod-lifecycle)\nexplains the behaviour of `init containers` when specify `restartPolicy` field on it.\n\n#### Individual container restart policy and rules {#container-restart-rules}\n\n{{< feature-state feature_gate_name=\"ContainerRestartRules\" >}}\n\nIf your cluster has the feature gate `ContainerRestartRules` enabled, you can specify\n`restartPolicy` and `restartPolicyRules` on _individual containers_ to override the Pod\nrestart policy. Container restart policy and rules applies to {{< glossary_tooltip text=\"app containers\" term_id=\"app-container\" >}}\nin the Pod and to regular [init containers](/docs/concepts/workloads/pods/init-containers/).\n\nA Kubernetes-native [sidecar container](/docs/concepts/workloads/pods/sidecar-containers/)\nhas its container-level `restartPolicy` set to `Always`.\n\nThe container restarts will follow the same exponential backoff as pod restart policy described above. \nSupported container restart policies:\n\n* `Always`: Automatically restarts the container after any termination.\n* `OnFailure`: Only restarts the container if it exits with an error (non-zero exit status).\n* `Never`: Does not automatically restart the terminated container.\n\nAdditionally, _individual containers_ can specify `restartPolicyRules`. If the `restartPolicyRules`\nfield is specified, then container `restartPolicy` **must** also be specified. The `restartPolicyRules`\ndefine a list of rules to apply on container exit. Each rule will consist of a condition\nand an action. The supported condition is `exitCodes`, which compares the exit code of the container\nwith a list of given values. The supported action is `Restart`, which means the container will be\nrestarted. The rules will be evaluated in order. On the first match, the action will be applied.\nIf none of the rules’ conditions matched, Kubernetes fallback to container’s configured\n`restartPolicy`.\n\nFor example, a Pod with OnFailure restart policy that have a `try-once` container. This allows\nPod to only restart certain containers:\n\n```yaml\napiVersion: v1\nkind: Pod\nmetadata:\n  name: on-failure-pod\nspec:\n  restartPolicy: OnFailure\n  containers:\n  - name: try-once-container    # This container will run only once because the restartPolicy is Never.\n    image: registry.k8s.io/busybox:1.27.2\n    command: ['sh', '-c', 'echo \"Only running once\" && sleep 10 && exit 1']\n    restartPolicy: Never     \n  - name: on-failure-container  # This container will be restarted on failure.\n    image: registry.k8s.io/busybox:1.27.2\n    command: ['sh', '-c', 'echo \"Keep restarting\" && sleep 1800 && exit 1']\n```\n\nA Pod with `Always` restart policy with an init container that only execute once. If the init\ncontainer fails, the Pod fails. This allows the Pod to fail if the initialization failed,\nbut also keep running once the initialization succeeds:\n\n```yaml\napiVersion: v1\nkind: Pod\nmetadata:\n  name: fail-pod-if-init-fails\nspec:\n  restartPolicy: Always\n  initContainers:\n  - name: init-once      # This init container will only try once. If it fails, the pod will fail.\n    image: registry.k8s.io/busybox:1.27.2\n    command: ['sh', '-c', 'echo \"Failing initialization\" && sleep 10 && exit 1']\n    restartPolicy: Never\n  containers:\n  - name: main-container # This container will always be restarted once initialization succeeds.\n    image: registry.k8s.io/busybox:1.27.2\n    command: ['sh', '-c', 'sleep 1800 && exit 0']\n```\n\nA Pod with Never restart policy with a container that ignores and restarts on specific exit codes.\nThis is useful to differentiate between restartable errors and non-restartable errors:\n\n```yaml\napiVersion: v1\nkind: Pod\nmetadata:\n  name: restart-on-exit-codes\nspec:\n  restartPolicy: Never\n  containers:\n  - name: restart-on-exit-codes\n    image: registry.k8s.io/busybox:1.27.2\n    command: ['sh', '-c', 'sleep 60 && exit 0']\n    restartPolicy: Never     # Container restart policy must be specified if rules are specified\n    restartPolicyRules:      # Only restart the container if it exits with code 42\n    - action: Restart\n      exitCodes:\n        operator: In\n        values: [42]\n```\n\nRestart rules can be used for many more advanced lifecycle management scenarios. Note, restart rules\nare affected by the same inconsistencies as the regular restart policy. The kubelet restarts, container\nruntime garbage collection, intermitted connectivity issues with the control plane may cause the state\nloss and containers may be re-run even when you expect a container not to be restarted.\n\n#### Restart All Containers {#restart-all-containers}\n\n{{< feature-state feature_gate_name=\"RestartAllContainersOnContainerExits\" >}}\n\nIf your cluster has the feature gate `RestartAllContainersOnContainerExits` enabled, you can specify\n`RestartAllContainers` as an action in `restartPolicyRules` at container level. When a container's exit\nmatches a rule with this action, the entire Pod is terminated and restarted in-place.\n\nThis \"in-place\" restart offers a more efficient way to reset a Pod's state compared to full deletion\nand recreation. This is especially valuable for workloads where rescheduling is costly, such as\nbatch jobs or AI/ML training tasks.\n\n##### How in-place Pod restarts work\n\nWhen a `RestartAllContainers` action is triggered, the kubelet performs the following steps:\n\n1. **Fast Termination**: All running containers in the Pod are terminated.\n   The configured `terminationGracePeriodSeconds` is not respected, and any configured `preStop` hooks\n   are not executed. This ensures a swift shutdown.\n1. **Preservation of Pod Resources**: The Pod's essential resources are preserved:\n\n   * Pod UID, IP address, and network namespace\n   * Pod sandbox and any attached devices\n   * All volumes, including `emptyDir` and mounted volumes\n\n1. **Pod Status Update**: The Pod's status is updated with a `PodRestartInPlace` condition set to `True`.\n   This makes the restart process observable.\n1. **Full Restart Sequence**: Once all containers are terminated, the `PodRestartInPlace` condition\n   is set to `False`, and the Pod begins the standard startup process:\n\n   * **Init containers are re-run** in order.\n   * Sidecar and regular containers are started.\n\nA key aspect of this feature is that **all** containers are restarted, including those that\npreviously completed successfully or failed. The `RestartAllContainers` action overrides\nany configured container-level or Pod-level `restartPolicy`.\n\nThis mechanism is useful in scenarios where a clean slate for all containers is necessary, such as:\n\n- When an `init` container sets up an environment that can become corrupted, this feature ensures\n  the setup process is re-executed.\n- A sidecar container can monitor the health of a main application and trigger a full Pod restart\n  if the application enters an unrecoverable state.\n\nConsider a workload where a watcher sidecar is responsible for restarting the main application\nfrom a known-good state if it encounters an error. The watcher can exit with a specific code\nto trigger a full, in-place restart of the worker Pod.\n\n{{% code_sample file=\"pods/restart-policy/restart-all-containers.yaml\" %}}\n\nIn this example:\n\n- The Pod's overall `restartPolicy` is `Never`.\n- The `watcher-sidecar` runs a command and then exits with code `88`.\n- The exit code matches the rule, triggering the `RestartAllContainers` action.\n- The entire Pod, including the `setup-environment` init container and the `main-application` container,\n  is then restarted in-place. The pod keeps its UID, sandbox, IP, and volumes.\n\n### Reduced container restart delay\n\n{{< feature-state\nfeature_gate_name=\"ReduceDefaultCrashLoopBackOffDecay\" >}}\n\nWith the alpha feature gate `ReduceDefaultCrashLoopBackOffDecay` enabled,\ncontainer start retries across your cluster will be reduced to begin at 1s\n(instead of 10s) and increase exponentially by 2x each restart until a maximum\ndelay of 60s (instead of 300s which is 5 minutes).\n\nIf you use this feature along with the alpha feature\n`KubeletCrashLoopBackOffMax` (described below), individual nodes may have\ndifferent maximum delays.\n\n### Configurable container restart delay\n\n{{< feature-state feature_gate_name=\"KubeletCrashLoopBackOffMax\" >}}\n\nWith the feature gate `KubeletCrashLoopBackOffMax` enabled, you can\nreconfigure the maximum delay between container start retries from the default\nof 300s (5 minutes). This configuration is set per node using kubelet\nconfiguration. In your [kubelet configuration](/docs/tasks/administer-cluster/kubelet-config-file/),\nunder `crashLoopBackOff` set the `maxContainerRestartPeriod` field between `\"1s\"` and\n`\"300s\"`. As described above in [Container restart policy](#restart-policy),\ndelays on that node will still start at 10s and increase exponentially by 2x\neach restart, but will now be capped at your configured maximum. If the\n`maxContainerRestartPeriod` you configure is less than the default initial value\nof 10s, the initial delay will instead be set to the configured maximum.\n\nSee the following kubelet configuration examples:\n\n```yaml\n# container restart delays will start at 10s, increasing\n# 2x each time they are restarted, to a maximum of 100s\nkind: KubeletConfiguration\ncrashLoopBackOff:\n    maxContainerRestartPeriod: \"100s\"\n```\n\n```yaml\n# delays between container restarts will always be 2s\nkind: KubeletConfiguration\ncrashLoopBackOff:\n    maxContainerRestartPeriod: \"2s\"\n```\n\nIf you use this feature along with the alpha feature\n`ReduceDefaultCrashLoopBackOffDecay` (described above), your cluster defaults\nfor initial backoff and maximum backoff will no longer be 10s and 300s, but 1s\nand 60s. Per node configuration takes precedence over the defaults set by\n`ReduceDefaultCrashLoopBackOffDecay`, even if this would result in a node having\na longer maximum backoff than other nodes in the cluster.\n\n",
    "parent_id": null,
    "source_file": "data/kubernetes/content/en/docs/concepts/workloads/pods/pod-lifecycle.md",
    "section_path": [
      "concepts",
      "workloads",
      "pods"
    ],
    "chunk_type": "parent",
    "strategy": "recursive"
  },
  {
    "id": "pod-lifecycle-p4",
    "content": "## Pod conditions\n\nA Pod has a PodStatus, which has an array of\n[PodConditions](/docs/reference/generated/kubernetes-api/{{< param \"version\" >}}/#podcondition-v1-core)\nthrough which the Pod has or has not passed. The kubelet manages the following\nPodConditions:\n\n* `PodScheduled`: the Pod has been scheduled to a node.\n* `PodReadyToStartContainers`: (beta feature; enabled by [default](#pod-has-network)) the\n  Pod sandbox has been successfully created and networking configured.\n* `ContainersReady`: all containers in the Pod are ready.\n* `Initialized`: all [init containers](/docs/concepts/workloads/pods/init-containers/)\n  have completed successfully.\n* `Ready`: the Pod is able to serve requests and should be added to the load\n  balancing pools of all matching Services.\n* `DisruptionTarget`: the pod is about to be terminated due to a disruption (such as preemption, eviction or garbage-collection).\n* `PodResizePending`: a pod resize was requested but cannot be applied. See [Pod resize status](/docs/tasks/configure-pod-container/resize-container-resources#pod-resize-status).\n* `PodResizeInProgress`: the pod is in the process of resizing. See\n  [Pod resize status](/docs/tasks/configure-pod-container/resize-container-resources#pod-resize-status).\n\nField name           | Description\n:--------------------|:-----------\n`type`               | Name of this Pod condition.\n`status`             | Indicates whether that condition is applicable, with possible values \"`True`\", \"`False`\", or \"`Unknown`\".\n`lastProbeTime`      | Timestamp of when the Pod condition was last probed.\n`lastTransitionTime` | Timestamp for when the Pod last transitioned from one status to another.\n`reason`             | Machine-readable, UpperCamelCase text indicating the reason for the condition's last transition.\n`message`            | Human-readable message indicating details about the last status transition.\n\n### Pod readiness {#pod-readiness-gate}\n\n{{< feature-state for_k8s_version=\"v1.14\" state=\"stable\" >}}\n\nYour application can inject extra feedback or signals into PodStatus:\n_Pod readiness_. To use this, set `readinessGates` in the Pod's `spec` to\nspecify a list of additional conditions that the kubelet evaluates for Pod readiness.\n\nReadiness gates are determined by the current state of `status.condition`\nfields for the Pod. If Kubernetes cannot find such a condition in the\n`status.conditions` field of a Pod, the status of the condition\nis defaulted to \"`False`\".\n\nHere is an example:\n\n```yaml\nkind: Pod\n...\nspec:\n  readinessGates:\n    - conditionType: \"www.example.com/feature-1\"\nstatus:\n  conditions:\n    - type: Ready                              # a built-in PodCondition\n      status: \"False\"\n      lastProbeTime: null\n      lastTransitionTime: 2018-01-01T00:00:00Z\n    - type: \"www.example.com/feature-1\"        # an extra PodCondition\n      status: \"False\"\n      lastProbeTime: null\n      lastTransitionTime: 2018-01-01T00:00:00Z\n  containerStatuses:\n    - containerID: docker://abcd...\n      ready: true\n...\n```\n\nThe Pod conditions you add must have names that meet the Kubernetes\n[label key format](/docs/concepts/overview/working-with-objects/labels/#syntax-and-character-set).\n\n### Status for Pod readiness {#pod-readiness-status}\n\nThe `kubectl patch` command does not support patching object status.\nTo set these `status.conditions` for the Pod, applications and\n{{< glossary_tooltip term_id=\"operator-pattern\" text=\"operators\">}} should use\nthe `PATCH` action.\nYou can use a [Kubernetes client library](/docs/reference/using-api/client-libraries/) to\nwrite code that sets custom Pod conditions for Pod readiness.\n\nFor a Pod that uses custom conditions, that Pod is evaluated to be ready **only**\nwhen both the following statements apply:\n\n* All containers in the Pod are ready.\n* All conditions specified in `readinessGates` are `True`.\n\nWhen a Pod's containers are Ready but at least one custom condition is missing or\n`False`, the kubelet sets the Pod's [condition](#pod-conditions) to `ContainersReady`.\n\n### Pod network readiness {#pod-has-network}\n\n{{< feature-state for_k8s_version=\"v1.29\" state=\"beta\" >}}\n\n{{< note >}}\nDuring its early development, this condition was named `PodHasNetwork`.\n{{< /note >}}\n\nAfter a Pod gets scheduled on a node, it needs to be admitted by the kubelet and\nto have any required storage volumes mounted. Once these phases are complete,\nthe kubelet works with\na container runtime (using {{< glossary_tooltip term_id=\"cri\" >}}) to set up a\nruntime sandbox and configure networking for the Pod. If the\n`PodReadyToStartContainersCondition`\n[feature gate](/docs/reference/command-line-tools-reference/feature-gates/) is enabled\n(it is enabled by default for Kubernetes {{< skew currentVersion >}}), the\n`PodReadyToStartContainers` condition will be added to the `status.conditions` field of a Pod.\n\nThe `PodReadyToStartContainers` condition is set to `False` by the kubelet when it detects a\nPod does not have a runtime sandbox with networking configured. This occurs in\nthe following scenarios:\n\n- Early in the lifecycle of the Pod, when the kubelet has not yet begun to set up a sandbox for\n  the Pod using the container runtime.\n- Later in the lifecycle of the Pod, when the Pod sandbox has been destroyed due to either:\n  - the node rebooting, without the Pod getting evicted\n  - for container runtimes that use virtual machines for isolation, the Pod\n    sandbox virtual machine rebooting, which then requires creating a new sandbox and\n    fresh container network configuration.\n\nThe `PodReadyToStartContainers` condition is set to `True` by the kubelet after the\nsuccessful completion of sandbox creation and network configuration for the Pod\nby the runtime plugin. The kubelet can start pulling container images and create\ncontainers after `PodReadyToStartContainers` condition has been set to `True`.\n\nFor a Pod with init containers, the kubelet sets the `Initialized` condition to\n`True` after the init containers have successfully completed (which happens\nafter successful sandbox creation and network configuration by the runtime\nplugin). For a Pod without init containers, the kubelet sets the `Initialized`\ncondition to `True` before sandbox creation and network configuration starts.\n\n",
    "parent_id": null,
    "source_file": "data/kubernetes/content/en/docs/concepts/workloads/pods/pod-lifecycle.md",
    "section_path": [
      "concepts",
      "workloads",
      "pods"
    ],
    "chunk_type": "parent",
    "strategy": "recursive"
  },
  {
    "id": "pod-lifecycle-p5",
    "content": "## Resizing Pods {#pod-resize}\n\n{{< feature-state feature_gate_name=\"InPlacePodVerticalScaling\" >}}\n\nKubernetes supports changing the CPU and memory resources allocated to Pods\nafter they are created. (For other infrastructure resources, you would need to\nuse different techniques specific to those resources.) There are two main\napproaches to resizing CPU and memory:\n\n### In-place Pod resize {#pod-resize-inplace}\n\nYou can resize a Pod's container-level CPU and memory resources without recreating the Pod.\nThis is also called _in-place Pod vertical scaling_. This allows you to adjust resource\nallocation for running containers while potentially avoiding application disruption.\n\nTo perform an in-place resize, you update the Pod's desired state using the `/resize`\nsubresource. The kubelet then attempts to apply the new resource values to the running\ncontainers. The Pod {{< glossary_tooltip text=\"conditions\" term_id=\"condition\" >}}\n`PodResizePending` and `PodResizeInProgress` (described in [Pod conditions](#pod-conditions))\nindicate the status of the resize operation. For more details about resize status, see\n[Container Resize Status](/docs/tasks/configure-pod-container/resize-container-resources/#container-resize-status).\n\nKey considerations for in-place resize:\n- Only CPU and memory resources can be resized in-place.\n- The Pod's [Quality of Service (QoS) class](/docs/concepts/workloads/pods/pod-qos/)\n  is determined at creation and cannot be changed by resizing.\n- You can configure whether a container restart is required for the resize using\n  `resizePolicy` in the container specification.\n\nFor detailed instructions on performing in-place resize, see\n[Resize CPU and Memory Resources assigned to Containers](/docs/tasks/configure-pod-container/resize-container-resources/).\n\n### Resizing by launching replacement Pods\n\nThe more cloud native approach to changing a Pod's resources is through the\nworkload resource that manages it (such as a Deployment or StatefulSet).\nWhen you update the resource specifications in the Pod template,\nthe workload's controller creates new Pods with the updated resources and terminates\nthe old Pods according to its update strategy.\n\nThis approach:\n- Works with any Kubernetes version.\n- Can change any Pod specification, not just resources.\n- Results in Pod replacement, so you should design your workload to handle\n  [planned disruptions](/docs/concepts/workloads/pods/disruptions/). Consider using a\n  [PodDisruptionBudget](/docs/tasks/run-application/configure-pdb/) to control availability.\n- Requires that your Pods are managed by a workload resource.\n\nYou can also use a\n[VerticalPodAutoscaler](/docs/concepts/workloads/autoscaling/vertical-pod-autoscale/)\nto automatically manage Pod resource recommendations and updates.\n\n",
    "parent_id": null,
    "source_file": "data/kubernetes/content/en/docs/concepts/workloads/pods/pod-lifecycle.md",
    "section_path": [
      "concepts",
      "workloads",
      "pods"
    ],
    "chunk_type": "parent",
    "strategy": "recursive"
  },
  {
    "id": "pod-lifecycle-p6",
    "content": "## Container probes\n\nA _probe_ is a diagnostic performed periodically by the [kubelet](/docs/reference/command-line-tools-reference/kubelet/)\non a container. To perform a diagnostic, the kubelet either executes code within the container,\nor makes a network request.\n\n### Check mechanisms {#probe-check-methods}\n\nThere are four different ways to check a container using a probe.\nEach probe must define exactly one of these four mechanisms:\n\n`exec`\n: Executes a specified command inside the container. The diagnostic\n  is considered successful if the command exits with a status code of 0.\n\n`grpc`\n: Performs a remote procedure call using [gRPC](https://grpc.io/).\n  The target should implement\n  [gRPC health checks](https://grpc.io/grpc/core/md_doc_health-checking.html).\n  The diagnostic is considered successful if the `status`\n  of the response is `SERVING`.  \n\n`httpGet`\n: Performs an HTTP `GET` request against the Pod's IP\n  address on a specified port and path. The diagnostic is\n  considered successful if the response has a status code\n  greater than or equal to 200 and less than 400.\n\n`tcpSocket`\n: Performs a TCP check against the Pod's IP address on\n  a specified port. The diagnostic is considered successful if\n  the port is open. If the remote system (the container) closes\n  the connection immediately after it opens, this counts as healthy.\n\n{{< caution >}}\nUnlike the other mechanisms, `exec` probe's implementation involves\nthe creation/forking of multiple processes each time when executed.\nAs a result, in case of the clusters having higher pod densities,\nlower intervals of `initialDelaySeconds`, `periodSeconds`,\nconfiguring any probe with exec mechanism might introduce an overhead on the cpu usage of the node.\nIn such scenarios, consider using the alternative probe mechanisms to avoid the overhead.\n{{< /caution >}}\n\n### Probe outcome\n\nEach probe has one of three results:\n\n`Success`\n: The container passed the diagnostic.\n\n`Failure`\n: The container failed the diagnostic.\n\n`Unknown`\n: The diagnostic failed (no action should be taken, and the kubelet\n  will make further checks).\n\n### Types of probe\n\nThe kubelet can optionally perform and react to three kinds of probes on running\ncontainers:\n\n`livenessProbe`\n: Indicates whether the container is running. If\n  the liveness probe fails, the kubelet kills the container, and the container\n  is subjected to its [restart policy](#restart-policy). If a container does not\n  provide a liveness probe, the default state is `Success`.\n\n`readinessProbe`\n: Indicates whether the container is ready to respond to requests.\n  If the readiness probe fails, the {{< glossary_tooltip term_id=\"endpoint-slice\" text=\"EndpointSlice\" >}}\n  controller removes the Pod's IP address from the EndpointSlices of all Services that match the Pod.\n  The default state of readiness before the initial delay is `Failure`. If a container does\n  not provide a readiness probe, the default state is `Success`.\n\n`startupProbe`\n: Indicates whether the application within the container is started.\n  All other probes are disabled if a startup probe is provided, until it succeeds.\n  If the startup probe fails, the kubelet kills the container, and the container\n  is subjected to its [restart policy](#restart-policy). If a container does not\n  provide a startup probe, the default state is `Success`.\n\nFor more information about how to set up a liveness, readiness, or startup probe,\nsee [Configure Liveness, Readiness and Startup Probes](/docs/tasks/configure-pod-container/configure-liveness-readiness-startup-probes/).\n\n#### When should you use a liveness probe?\n\nIf the process in your container is able to crash on its own whenever it\nencounters an issue or becomes unhealthy, you do not necessarily need a liveness\nprobe; the kubelet will automatically perform the correct action in accordance\nwith the Pod's `restartPolicy`.\n\nIf you'd like your container to be killed and restarted if a probe fails, then\nspecify a liveness probe, and specify a `restartPolicy` of Always or OnFailure.\n\n#### When should you use a readiness probe?\n\nIf you'd like to start sending traffic to a Pod only when a probe succeeds,\nspecify a readiness probe. In this case, the readiness probe might be the same\nas the liveness probe, but the existence of the readiness probe in the spec means\nthat the Pod will start without receiving any traffic and only start receiving\ntraffic after the probe starts succeeding.\n\nIf you want your container to be able to take itself down for maintenance, you\ncan specify a readiness probe that checks an endpoint specific to readiness that\nis different from the liveness probe.\n\nIf your app has a strict dependency on back-end services, you can implement both\na liveness and a readiness probe. The liveness probe passes when the app itself\nis healthy, but the readiness probe additionally checks that each required\nback-end service is available. This helps you avoid directing traffic to Pods\nthat can only respond with error messages.\n\nIf your container needs to work on loading large data, configuration files, or\nmigrations during startup, you can use a\n[startup probe](#when-should-you-use-a-startup-probe). However, if you want to\ndetect the difference between an app that has failed and an app that is still\nprocessing its startup data, you might prefer a readiness probe.\n\n{{< note >}}\nIf you want to be able to drain requests when the Pod is deleted, you do not\nnecessarily need a readiness probe; when the Pod is deleted, the corresponding endpoint\nin the `EndpointSlice` will update its [conditions](/docs/concepts/services-networking/endpoint-slices/#conditions):\nthe endpoint `ready` condition will be set to `false`, so load balancers\nwill not use the Pod for regular traffic. See [Pod termination](#pod-termination)\nfor more information about how the kubelet handles Pod deletion.\n{{< /note >}}\n\n#### When should you use a startup probe?\n\nStartup probes are useful for Pods that have containers that take a long time to\ncome into service. Rather than set a long liveness interval, you can configure\na separate configuration for probing the container as it starts up, allowing\na time longer than the liveness interval would allow.\n\n<!-- ensure front matter contains math: true -->\nIf your container usually starts in more than\n\\\\( initialDelaySeconds + failureThreshold \\times  periodSeconds \\\\), you should specify a\nstartup probe that checks the same endpoint as the liveness probe. The default for\n`periodSeconds` is 10s. You should then set its `failureThreshold` high enough to\nallow the container to start, without changing the default values of the liveness\nprobe. This helps to protect against deadlocks.\n\n",
    "parent_id": null,
    "source_file": "data/kubernetes/content/en/docs/concepts/workloads/pods/pod-lifecycle.md",
    "section_path": [
      "concepts",
      "workloads",
      "pods"
    ],
    "chunk_type": "parent",
    "strategy": "recursive"
  },
  {
    "id": "pod-lifecycle-p7",
    "content": "## Termination of Pods {#pod-termination}\n\nBecause Pods represent processes running on nodes in the cluster, it is important to\nallow those processes to gracefully terminate when they are no longer needed (rather\nthan being abruptly stopped with a `KILL` signal and having no chance to clean up).\n\nThe design aim is for you to be able to request deletion and know when processes\nterminate, but also be able to ensure that deletes eventually complete.\nWhen you request deletion of a Pod, the cluster records and tracks the intended grace period\nbefore the Pod is allowed to be forcefully killed. With that forceful shutdown tracking in\nplace, the {{< glossary_tooltip text=\"kubelet\" term_id=\"kubelet\" >}} attempts graceful\nshutdown.\n\nTypically, with this graceful termination of the pod, kubelet makes requests to the container runtime\nto attempt to stop the containers in the pod by first sending a TERM (aka. SIGTERM) signal,\nwith a grace period timeout, to the main process in each container.\nThe requests to stop the containers are processed by the container runtime asynchronously.\nThere is no guarantee to the order of processing for these requests.\nMany container runtimes respect the `STOPSIGNAL` value defined in the container image and,\nif different, send the container image configured STOPSIGNAL instead of TERM.\nOnce the grace period has expired, the KILL signal is sent to any remaining\nprocesses, and the Pod is then deleted from the\n{{< glossary_tooltip text=\"API Server\" term_id=\"kube-apiserver\" >}}. If the kubelet or the\ncontainer runtime's management service is restarted while waiting for processes to terminate, the\ncluster retries from the start including the full original grace period.\n\n### Stop Signals {#pod-termination-stop-signals}\n\nThe stop signal used to kill the container can be defined in the container image with the `STOPSIGNAL` instruction.\nIf no stop signal is defined in the image, the default signal of the container runtime\n(SIGTERM for both containerd and CRI-O) would be used to kill the container.\n\n### Defining custom stop signals\n\n{{< feature-state feature_gate_name=\"ContainerStopSignals\" >}}\n\nIf the `ContainerStopSignals` feature gate is enabled, you can configure a custom stop signal\nfor your containers from the container Lifecycle. We require the Pod's `spec.os.name` field\nto be present as a requirement for defining stop signals in the container lifecycle.\nThe list of signals that are valid depends on the OS the Pod is scheduled to.\nFor Pods scheduled to Windows nodes, we only support SIGTERM and SIGKILL as valid signals.\n\nHere is an example Pod spec defining a custom stop signal:\n\n```yaml\nspec:\n  os:\n    name: linux\n  containers:\n    - name: my-container\n      image: container-image:latest\n      lifecycle:\n        stopSignal: SIGUSR1\n```\n\nIf a stop signal is defined in the lifecycle, this will override the signal defined in the container image.\nIf no stop signal is defined in the container spec, the container would fall back to the default behavior.\n\n### Pod Termination Flow {#pod-termination-flow}\n\nPod termination flow, illustrated with an example:\n\n1. You use the `kubectl` tool to manually delete a specific Pod, with the default grace period\n   (30 seconds).\n\n1. The Pod in the API server is updated with the time beyond which the Pod is considered \"dead\"\n   along with the grace period.\n   If you use `kubectl describe` to check the Pod you're deleting, that Pod shows up as \"Terminating\".\n   On the node where the Pod is running: as soon as the kubelet sees that a Pod has been marked\n   as terminating (a graceful shutdown duration has been set), the kubelet begins the local Pod\n   shutdown process.\n\n   1. If one of the Pod's containers has defined a `preStop`\n      [hook](/docs/concepts/containers/container-lifecycle-hooks) and the `terminationGracePeriodSeconds`\n      in the Pod spec is not set to 0, the kubelet runs that hook inside of the container.\n      The default `terminationGracePeriodSeconds` setting is 30 seconds.\n\n      If the `preStop` hook is still running after the grace period expires, the kubelet requests\n      a small, one-off grace period extension of 2 seconds.\n\n   {{% note %}}\n   If the `preStop` hook needs longer to complete than the default grace period allows,\n   you must modify `terminationGracePeriodSeconds` to suit this.\n   {{% /note %}}\n\n   1. The kubelet triggers the container runtime to send a TERM signal to process 1 inside each\n      container.\n\n      There is [special ordering](#termination-with-sidecars) if the Pod has any\n      {{< glossary_tooltip text=\"sidecar containers\" term_id=\"sidecar-container\" >}} defined.\n      Otherwise, the containers in the Pod receive the TERM signal at different times and in\n      an arbitrary order. If the order of shutdowns matters, consider using a `preStop` hook\n      to synchronize (or switch to using sidecar containers).\n\n1. At the same time as the kubelet is starting graceful shutdown of the Pod, the control plane\n   evaluates whether to remove that shutting-down Pod from EndpointSlice objects,\n   where those objects represent a {{< glossary_tooltip term_id=\"service\" text=\"Service\" >}}\n   with a configured {{< glossary_tooltip text=\"selector\" term_id=\"selector\" >}}.\n   {{< glossary_tooltip text=\"ReplicaSets\" term_id=\"replica-set\" >}} and other workload resources\n   no longer treat the shutting-down Pod as a valid, in-service replica.\n\n   Pods that shut down slowly should not continue to serve regular traffic and should start\n   terminating and finish processing open connections.  Some applications need to go beyond\n   finishing open connections and need more graceful termination, for example, session draining\n   and completion.\n\n   Any endpoints that represent the terminating Pods are not immediately removed from\n   EndpointSlices, and a status indicating [terminating state](/docs/concepts/services-networking/endpoint-slices/#conditions)\n   is exposed from the EndpointSlice API.\n   Terminating endpoints always have their `ready` status as `false` (for backward compatibility\n   with versions before 1.26), so load balancers will not use it for regular traffic.\n\n   If traffic draining on terminating Pod is needed, the actual readiness can be checked as a\n   condition `serving`.  You can find more details on how to implement connections draining in the\n   tutorial [Pods And Endpoints Termination Flow](/docs/tutorials/services/pods-and-endpoint-termination-flow/)\n\n   <a id=\"pod-termination-beyond-grace-period\" />\n\n1. The kubelet ensures the Pod is shut down and terminated\n\n   1. When the grace period expires, if there is still any container running in the Pod, the\n      kubelet triggers forcible shutdown.\n      The container runtime sends `SIGKILL` to any processes still running in any container in the Pod.\n      The kubelet also cleans up a hidden `pause` container if that container runtime uses one.\n   1. The kubelet transitions the Pod into a terminal phase (`Failed` or `Succeeded` depending on\n      the end state of its containers).\n   1. The kubelet triggers forcible removal of the Pod object from the API server, by setting grace period\n      to 0 (immediate deletion).\n   1. The API server deletes the Pod's API object, which is then no longer visible from any client.\n\n### Forced Pod termination {#pod-termination-forced}\n\n{{< caution >}}\nForced deletions can be potentially disruptive for some workloads and their Pods.\n{{< /caution >}}\n\nBy default, all deletes are graceful within 30 seconds. The `kubectl delete` command supports\nthe `--grace-period=<seconds>` option which allows you to override the default and specify your\nown value.\n\nSetting the grace period to `0` forcibly and immediately deletes the Pod from the API\nserver. If the Pod was still running on a node, that forcible deletion triggers the kubelet to\nbegin immediate cleanup.\n\nUsing kubectl, You must specify an additional flag `--force` along with `--grace-period=0`\nin order to perform force deletions.\n\nWhen a force deletion is performed, the API server does not wait for confirmation\nfrom the kubelet that the Pod has been terminated on the node it was running on. It\nremoves the Pod in the API immediately so a new Pod can be created with the same\nname. On the node, Pods that are set to terminate immediately will still be given\na small grace period before being force killed.\n\n{{< caution >}}\nImmediate deletion does not wait for confirmation that the running resource has been terminated.\nThe resource may continue to run on the cluster indefinitely.\n{{< /caution >}}\n\nIf you need to force-delete Pods that are part of a StatefulSet, refer to the task\ndocumentation for\n[deleting Pods from a StatefulSet](/docs/tasks/run-application/force-delete-stateful-set-pod/).\n\n### Pod shutdown and sidecar containers {#termination-with-sidecars}\n\nIf your Pod includes one or more\n[sidecar containers](/docs/concepts/workloads/pods/sidecar-containers/)\n(init containers with an `Always` restart policy), the kubelet will delay sending\nthe TERM signal to these sidecar containers until the last main container has fully terminated.\nThe sidecar containers will be terminated in the reverse order they are defined in the Pod spec.\nThis ensures that sidecar containers continue serving the other containers in the Pod until they\nare no longer needed.\n\nThis means that slow termination of a main container will also delay the termination of the sidecar containers.\nIf the grace period expires before the termination process is complete, the Pod may enter [forced termination](#pod-termination-beyond-grace-period).\nIn this case, all remaining containers in the Pod will be terminated simultaneously with a short grace period.\n\nSimilarly, if the Pod has a `preStop` hook that exceeds the termination grace period, emergency termination may occur.\nIn general, if you have used `preStop` hooks to control the termination order without sidecar containers, you can now\nremove them and allow the kubelet to manage sidecar termination automatically.\n\n### Garbage collection of Pods {#pod-garbage-collection}\n\nFor failed Pods, the API objects remain in the cluster's API until a human or\n{{< glossary_tooltip term_id=\"controller\" text=\"controller\" >}} process\nexplicitly removes them.\n\nThe Pod garbage collector (PodGC), which is a controller in the control plane, cleans up\nterminated Pods (with a phase of `Succeeded` or `Failed`), when the number of Pods exceeds the\nconfigured threshold (determined by `terminated-pod-gc-threshold` in the kube-controller-manager).\nThis avoids a resource leak as Pods are created and terminated over time.\n\nAdditionally, PodGC cleans up any Pods which satisfy any of the following conditions:\n\n1. are orphan Pods - bound to a node which no longer exists,\n1. are unscheduled terminating Pods,\n1. are terminating Pods, bound to a non-ready node tainted with\n   [`node.kubernetes.io/out-of-service`](/docs/reference/labels-annotations-taints/#node-kubernetes-io-out-of-service).\n\nAlong with cleaning up the Pods, PodGC will also mark them as failed if they are in a non-terminal\nphase. Also, PodGC adds a Pod disruption condition when cleaning up an orphan Pod.\nSee [Pod disruption conditions](/docs/concepts/workloads/pods/disruptions#pod-disruption-conditions)\nfor more details.\n\n",
    "parent_id": null,
    "source_file": "data/kubernetes/content/en/docs/concepts/workloads/pods/pod-lifecycle.md",
    "section_path": [
      "concepts",
      "workloads",
      "pods"
    ],
    "chunk_type": "parent",
    "strategy": "recursive"
  },
  {
    "id": "pod-lifecycle-p8",
    "content": "## Pod behavior during kubelet restarts {#kubelet-restarts}\n\nIf you restart the kubelet, Pods (and their containers) continue to run\neven during the restart.\nWhen there are running Pods on a node, stopping or restarting the kubelet\non that node does **not** cause the kubelet to stop all local Pods\nbefore the kubelet itself stops.\nTo stop the Pods on a node, you can use `kubectl drain`.\n\n### Detection of kubelet restarts\n\n{{< feature-state feature_gate_name=\"ChangeContainerStatusOnKubeletRestart\" >}}\n\nWhen the kubelet starts, it checks to see if there is already a Node with bound Pods.\nIf the Node's [`Ready` condition](/docs/reference/node/node-status/#condition) remains unchanged,\nin other words the condition has not transitioned from true to false, Kubernetes detects this a _kubelet restart_.\n(It's possible to restart the kubelet in other ways, for example to fix a node bug,\nbut in these cases, Kubernetes picks the safe option and treats this as if you\nstopped the kubelet and then later started it).\n\nWhen the kubelet restarts, the container statuses are managed differently based on the feature gate setting:\n\n* By default, the kubelet does not change container statuses after a restart.\n  Containers that were in set to `ready: true` state remain remain ready.\n\n  If you stop the kubelet long enough for it to fail a series of\n  [node heartbeat](/docs/concepts/architecture/leases/#node-heart-beats) checks,\n  and then you wait before you start the kubelet again, Kubernetes may begin to evict Pods from that Node.\n  However, even though Pod evictions begin to happen, Kubernetes does not mark the\n  individual containers in those Pods as `ready: false`. The Pod-level eviction\n  happens after the control plane taints the node as `node.kubernetes.io/not-ready` (due to the failed heartbeats).\n\n* In Kubernetes {{< skew currentVersion >}} you can opt in to a legacy behavior where the kubelet always modify\n  the containers `ready` value, after a kubelet restart, to be false.\n\n  This legacy behavior was the default for a long time, but caused issue for people using Kubernetes,\n  especially in large scale deployments. Althought the feature gate allows reverting to this legacy\n  behavior temporarily, the Kubernetes project recommends that you file a bug report if you encounter problems.\n  The `ChangeContainerStatusOnKubeletRestart`\n  [feature gate](/docs/reference/command-line-tools-reference/feature-gates/#ChangeContainerStatusOnKubeletRestart)\n  will be removed in the future.\n\n",
    "parent_id": null,
    "source_file": "data/kubernetes/content/en/docs/concepts/workloads/pods/pod-lifecycle.md",
    "section_path": [
      "concepts",
      "workloads",
      "pods"
    ],
    "chunk_type": "parent",
    "strategy": "recursive"
  },
  {
    "id": "pod-lifecycle-p9",
    "content": "## {{% heading \"whatsnext\" %}}\n\n* Get hands-on experience\n  [attaching handlers to container lifecycle events](/docs/tasks/configure-pod-container/attach-handler-lifecycle-event/).\n\n* Get hands-on experience\n  [configuring Liveness, Readiness and Startup Probes](/docs/tasks/configure-pod-container/configure-liveness-readiness-startup-probes/).\n\n* Learn more about [container lifecycle hooks](/docs/concepts/containers/container-lifecycle-hooks/).\n\n* Learn more about [sidecar containers](/docs/concepts/workloads/pods/sidecar-containers/).\n\n* For detailed information about Pod and container status in the API, see\n  the API reference documentation covering\n  [`status`](/docs/reference/kubernetes-api/workload-resources/pod-v1/#PodStatus) for Pod.\n",
    "parent_id": null,
    "source_file": "data/kubernetes/content/en/docs/concepts/workloads/pods/pod-lifecycle.md",
    "section_path": [
      "concepts",
      "workloads",
      "pods"
    ],
    "chunk_type": "parent",
    "strategy": "recursive"
  },
  {
    "id": "pod-qos-p0",
    "content": "## Quality of Service classes\n\nKubernetes classifies the Pods that you run and allocates each Pod into a specific\n_quality of service (QoS) class_. Kubernetes uses that classification to influence how different\npods are handled. Kubernetes does this classification based on the\n[resource requests](/docs/concepts/configuration/manage-resources-containers/)\nof the {{< glossary_tooltip text=\"Containers\" term_id=\"container\" >}} in that Pod, along with\nhow those requests relate to resource limits.\nThis is known as {{< glossary_tooltip text=\"Quality of Service\" term_id=\"qos-class\" >}}\n(QoS) class. Kubernetes assigns every Pod a QoS class based on the resource requests\nand limits of its component Containers. QoS classes are used by Kubernetes to decide\nwhich Pods to evict from a Node experiencing\n[Node Pressure](/docs/concepts/scheduling-eviction/node-pressure-eviction/). The possible\nQoS classes are `Guaranteed`, `Burstable`, and `BestEffort`. When a Node runs out of resources,\nKubernetes will first evict `BestEffort` Pods running on that Node, followed by `Burstable` and\nfinally `Guaranteed` Pods. When this eviction is due to resource pressure, only Pods exceeding\nresource requests are candidates for eviction.\n\n### Guaranteed\n\nPods that are `Guaranteed` have the strictest resource limits and are least likely\nto face eviction. They are guaranteed not to be killed until they exceed their limits\nor there are no lower-priority Pods that can be preempted from the Node. They may\nnot acquire resources beyond their specified limits. These Pods can also make\nuse of exclusive CPUs using the\n[`static`](/docs/tasks/administer-cluster/cpu-management-policies/#static-policy-configuration) CPU management policy.\n\n#### Criteria\n\nFor a Pod to be given a QoS class of `Guaranteed`:\n\n* Every Container in the Pod must have a memory limit and a memory request.\n* For every Container in the Pod, the memory limit must equal the memory request.\n* Every Container in the Pod must have a CPU limit and a CPU request.\n* For every Container in the Pod, the CPU limit must equal the CPU request.\n\nIf instead the Pod uses [Pod-level resources](/docs/concepts/configuration/manage-resources-containers/#pod-level-resource-specification):\n\n{{< feature-state feature_gate_name=\"PodLevelResources\" >}}\n\n* The Pod must have a Pod-level memory limit and memory request, and their values must be equal.\n* The Pod must have a Pod-level CPU limit and CPU request, and their values must be equal.\n\n### Burstable\n\nPods that are `Burstable` have some lower-bound resource guarantees based on the request, but\ndo not require a specific limit. If a limit is not specified, it defaults to a\nlimit equivalent to the capacity of the Node, which allows the Pods to flexibly increase\ntheir resources if resources are available. In the event of Pod eviction due to Node\nresource pressure, these Pods are evicted only after all `BestEffort` Pods are evicted.\nBecause a `Burstable` Pod can include a Container that has no resource limits or requests, a Pod\nthat is `Burstable` can try to use any amount of node resources.\n\n#### Criteria\n\nA Pod is given a QoS class of `Burstable` if:\n\n* The Pod does not meet the criteria for QoS class `Guaranteed`.\n* At least one Container in the Pod has a memory or CPU request or limit,\n  or the Pod has a Pod-level memory or CPU request or limit.\n\n### BestEffort\n\nPods in the `BestEffort` QoS class can use node resources that aren't specifically assigned\nto Pods in other QoS classes. For example, if you have a node with 16 CPU cores available to the\nkubelet, and you assign 4 CPU cores to a `Guaranteed` Pod, then a Pod in the `BestEffort`\nQoS class can try to use any amount of the remaining 12 CPU cores.\n\nThe kubelet prefers to evict `BestEffort` Pods if the node comes under resource pressure.\n\n#### Criteria\n\nA Pod has a QoS class of `BestEffort` if it doesn't meet the criteria for either `Guaranteed`\nor `Burstable`. In other words, a Pod is `BestEffort` only if none of the Containers in the Pod have a\nmemory limit or a memory request, and none of the Containers in the Pod have a\nCPU limit or a CPU request, and the Pod does not have any Pod-level memory or CPU limits or requests.\nContainers in a Pod can request other resources (not CPU or memory) and still be classified as\n`BestEffort`.\n\n",
    "parent_id": null,
    "source_file": "data/kubernetes/content/en/docs/concepts/workloads/pods/pod-qos.md",
    "section_path": [
      "concepts",
      "workloads",
      "pods"
    ],
    "chunk_type": "parent",
    "strategy": "recursive"
  },
  {
    "id": "pod-qos-p1",
    "content": "## Memory QoS with cgroup v2\n\n{{< feature-state feature_gate_name=\"MemoryQoS\" >}}\n\nMemory QoS uses the memory controller of cgroup v2 to guarantee memory resources in Kubernetes.\nMemory requests and limits of containers in pod are used to set specific interfaces `memory.min`\nand `memory.high` provided by the memory controller. When `memory.min` is set to memory requests,\nmemory resources are reserved and never reclaimed by the kernel; this is how Memory QoS ensures\nmemory availability for Kubernetes pods. And if memory limits are set in the container,\nthis means that the system needs to limit container memory usage; Memory QoS uses `memory.high`\nto throttle workload approaching its memory limit, ensuring that the system is not overwhelmed\nby instantaneous memory allocation.\n\nMemory QoS relies on QoS class to determine which settings to apply; however, these are different\nmechanisms that both provide controls over quality of service.\n\n",
    "parent_id": null,
    "source_file": "data/kubernetes/content/en/docs/concepts/workloads/pods/pod-qos.md",
    "section_path": [
      "concepts",
      "workloads",
      "pods"
    ],
    "chunk_type": "parent",
    "strategy": "recursive"
  },
  {
    "id": "pod-qos-p2",
    "content": "## Some behavior is independent of QoS class {#class-independent-behavior}\n\nCertain behavior is independent of the QoS class assigned by Kubernetes. For example:\n\n* Any Container exceeding a resource limit will be killed and restarted by the kubelet without\n  affecting other Containers in that Pod.\n\n* If a Container exceeds its resource request and the node it runs on faces\n  resource pressure, the Pod it is in becomes a candidate for [eviction](/docs/concepts/scheduling-eviction/node-pressure-eviction/).\n  If this occurs, all Containers in the Pod will be terminated. Kubernetes may create a\n  replacement Pod, usually on a different node.\n\n* The resource request of a Pod is equal to the sum of the resource requests of\n  its component Containers, and the resource limit of a Pod is equal to the sum of\n  the resource limits of its component Containers.\n\n* The kube-scheduler does not consider QoS class when selecting which Pods to\n  [preempt](/docs/concepts/scheduling-eviction/pod-priority-preemption/#preemption).\n  Preemption can occur when a cluster does not have enough resources to run all the Pods\n  you defined.\n\n* The QoS class is determined when the Pod is created and remains unchanged for the\n  lifetime of the Pod. If you later attempt an\n  [in-place resize](/docs/concepts/workloads/pods/pod-lifecycle/#pod-resize)\n  that would result in a different QoS class, the resize is rejected by admission.\n\n",
    "parent_id": null,
    "source_file": "data/kubernetes/content/en/docs/concepts/workloads/pods/pod-qos.md",
    "section_path": [
      "concepts",
      "workloads",
      "pods"
    ],
    "chunk_type": "parent",
    "strategy": "recursive"
  },
  {
    "id": "pod-qos-p3",
    "content": "## {{% heading \"whatsnext\" %}}\n\n* Learn about [resource management for Pods and Containers](/docs/concepts/configuration/manage-resources-containers/).\n* Learn about [Node-pressure eviction](/docs/concepts/scheduling-eviction/node-pressure-eviction/).\n* Learn about [Pod priority and preemption](/docs/concepts/scheduling-eviction/pod-priority-preemption/).\n* Learn about [Pod disruptions](/docs/concepts/workloads/pods/disruptions/).\n* Learn how to [assign memory resources to containers and pods](/docs/tasks/configure-pod-container/assign-memory-resource/).\n* Learn how to [assign CPU resources to containers and pods](/docs/tasks/configure-pod-container/assign-cpu-resource/).\n* Learn how to [configure Quality of Service for Pods](/docs/tasks/configure-pod-container/quality-service-pod/).\n",
    "parent_id": null,
    "source_file": "data/kubernetes/content/en/docs/concepts/workloads/pods/pod-qos.md",
    "section_path": [
      "concepts",
      "workloads",
      "pods"
    ],
    "chunk_type": "parent",
    "strategy": "recursive"
  },
  {
    "id": "sidecar-containers-p0",
    "content": "## Sidecar containers in Kubernetes {#pod-sidecar-containers}\n\nKubernetes implements sidecar containers as a special case of\n[init containers](/docs/concepts/workloads/pods/init-containers/); sidecar containers remain\nrunning after Pod startup. This document uses the term _regular init containers_ to clearly\nrefer to containers that only run during Pod startup.\n\nProvided that your cluster has the `SidecarContainers`\n[feature gate](/docs/reference/command-line-tools-reference/feature-gates/) enabled\n(the feature is active by default since Kubernetes v1.29), you can specify a `restartPolicy`\nfor containers listed in a Pod's `initContainers` field.\nThese restartable _sidecar_ containers are independent from other init containers and from\nthe main application container(s) within the same pod.\nThese can be started, stopped, or restarted without affecting the main application container\nand other init containers.\n\nYou can also run a Pod with multiple containers that are not marked as init or sidecar\ncontainers. This is appropriate if the containers within the Pod are required for the\nPod to work overall, but you don't need to control which containers start or stop first.\nYou could also do this if you need to support older versions of Kubernetes that don't\nsupport a container-level `restartPolicy` field.\n\n### Example application {#sidecar-example}\n\nHere's an example of a Deployment with two containers, one of which is a sidecar:\n\n{{% code_sample language=\"yaml\" file=\"application/deployment-sidecar.yaml\" %}}\n\n",
    "parent_id": null,
    "source_file": "data/kubernetes/content/en/docs/concepts/workloads/pods/sidecar-containers.md",
    "section_path": [
      "concepts",
      "workloads",
      "pods"
    ],
    "chunk_type": "parent",
    "strategy": "recursive"
  },
  {
    "id": "sidecar-containers-p1",
    "content": "## Sidecar containers and Pod lifecycle\n\nIf an init container is created with its `restartPolicy` set to `Always`, it will\nstart and remain running during the entire life of the Pod. This can be helpful for\nrunning supporting services separated from the main application containers.\n\nIf a `readinessProbe` is specified for this init container, its result will be used\nto determine the `ready` state of the Pod.\n\nSince these containers are defined as init containers, they benefit from the same\nordering and sequential guarantees as regular init containers, allowing you to mix\nsidecar containers with regular init containers for complex Pod initialization flows.\n\nCompared to regular init containers, sidecars defined within `initContainers` continue to\nrun after they have started. This is important when there is more than one entry inside\n`.spec.initContainers` for a Pod. After a sidecar-style init container is running (the kubelet\nhas set the `started` status for that init container to true), the kubelet then starts the\nnext init container from the ordered `.spec.initContainers` list.\nThat status either becomes true because there is a process running in the\ncontainer and no startup probe defined, or as a result of its `startupProbe` succeeding.\n\nUpon Pod [termination](/docs/concepts/workloads/pods/pod-lifecycle/#termination-with-sidecars),\nthe kubelet postpones terminating sidecar containers until the main application container has fully stopped.\nThe sidecar containers are then shut down in the opposite order of their appearance in the Pod specification.\nThis approach ensures that the sidecars remain operational, supporting other containers within the Pod,\nuntil their service is no longer required.\n\n### Jobs with sidecar containers\n\nIf you define a Job that uses sidecar using Kubernetes-style init containers,\nthe sidecar container in each Pod does not prevent the Job from completing after the\nmain container has finished.\n\nHere's an example of a Job with two containers, one of which is a sidecar:\n\n{{% code_sample language=\"yaml\" file=\"application/job/job-sidecar.yaml\" %}}\n\n",
    "parent_id": null,
    "source_file": "data/kubernetes/content/en/docs/concepts/workloads/pods/sidecar-containers.md",
    "section_path": [
      "concepts",
      "workloads",
      "pods"
    ],
    "chunk_type": "parent",
    "strategy": "recursive"
  },
  {
    "id": "sidecar-containers-p2",
    "content": "## Differences from application containers\n\nSidecar containers run alongside _app containers_ in the same pod. However, they do not\nexecute the primary application logic; instead, they provide supporting functionality to\nthe main application.\n\nSidecar containers have their own independent lifecycles. They can be started, stopped,\nand restarted independently of app containers. This means you can update, scale, or\nmaintain sidecar containers without affecting the primary application.\n\nSidecar containers share the same network and storage namespaces with the primary\ncontainer. This co-location allows them to interact closely and share resources.\n\nFrom a Kubernetes perspective, the sidecar container's graceful termination is less important.\nWhen other containers take all allotted graceful termination time, the sidecar containers\nwill receive the `SIGTERM` signal, followed by the `SIGKILL` signal, before they have time to terminate gracefully. \nSo exit codes different from `0` (`0` indicates successful exit), for sidecar containers are normal\non Pod termination and should be generally ignored by the external tooling.\n\n",
    "parent_id": null,
    "source_file": "data/kubernetes/content/en/docs/concepts/workloads/pods/sidecar-containers.md",
    "section_path": [
      "concepts",
      "workloads",
      "pods"
    ],
    "chunk_type": "parent",
    "strategy": "recursive"
  },
  {
    "id": "sidecar-containers-p3",
    "content": "## Differences from init containers\n\nSidecar containers work alongside the main container, extending its functionality and\nproviding additional services.\n\nSidecar containers run concurrently with the main application container. They are active\nthroughout the lifecycle of the pod and can be started and stopped independently of the\nmain container. Unlike [init containers](/docs/concepts/workloads/pods/init-containers/),\nsidecar containers support [probes](/docs/concepts/workloads/pods/pod-lifecycle/#types-of-probe) to control their lifecycle.\n\nSidecar containers can interact directly with the main application containers, because\nlike init containers they always share the same network, and can optionally also share\nvolumes (filesystems).\n\nInit containers stop before the main containers start up, so init containers cannot\nexchange messages with the app container in a Pod. Any data passing is one-way\n(for example, an init container can put information inside an `emptyDir` volume).\n\nChanging the image of a sidecar container will not cause the Pod to restart, but will\ntrigger a container restart.\n\n",
    "parent_id": null,
    "source_file": "data/kubernetes/content/en/docs/concepts/workloads/pods/sidecar-containers.md",
    "section_path": [
      "concepts",
      "workloads",
      "pods"
    ],
    "chunk_type": "parent",
    "strategy": "recursive"
  },
  {
    "id": "sidecar-containers-p4",
    "content": "## Resource sharing within containers\n\n{{< comment >}}\nThis section is also present in the [init containers](/docs/concepts/workloads/pods/init-containers/) page.\nIf you're editing this section, change both places.\n{{< /comment >}}\n\nGiven the order of execution for init, sidecar and app containers, the following rules\nfor resource usage apply:\n\n* The highest of any particular resource request or limit defined on all init\n  containers is the *effective init request/limit*. If any resource has no\n  resource limit specified this is considered as the highest limit.\n* The Pod's *effective request/limit* for a resource is the sum of\n[pod overhead](/docs/concepts/scheduling-eviction/pod-overhead/) and the higher of:\n  * the sum of all non-init containers(app and sidecar containers) request/limit for a\n  resource\n  * the effective init request/limit for a resource\n* Scheduling is done based on effective requests/limits, which means\n  init containers can reserve resources for initialization that are not used\n  during the life of the Pod.\n* The QoS (quality of service) tier of the Pod's *effective QoS tier* is the\n  QoS tier for all init, sidecar and app containers alike.\n\nQuota and limits are applied based on the effective Pod request and\nlimit.\n\n### Sidecar containers and Linux cgroups {#cgroups}\n\nOn Linux, resource allocations for Pod level control groups (cgroups) are based on the effective Pod\nrequest and limit, the same as the scheduler.\n\n",
    "parent_id": null,
    "source_file": "data/kubernetes/content/en/docs/concepts/workloads/pods/sidecar-containers.md",
    "section_path": [
      "concepts",
      "workloads",
      "pods"
    ],
    "chunk_type": "parent",
    "strategy": "recursive"
  },
  {
    "id": "sidecar-containers-p5",
    "content": "## {{% heading \"whatsnext\" %}}\n\n* Learn how to [Adopt Sidecar Containers](/docs/tutorials/configuration/pod-sidecar-containers/)\n* Read a blog post on [native sidecar containers](/blog/2023/08/25/native-sidecar-containers/).\n* Read about [creating a Pod that has an init container](/docs/tasks/configure-pod-container/configure-pod-initialization/#create-a-pod-that-has-an-init-container).\n* Learn about the [types of probes](/docs/concepts/workloads/pods/pod-lifecycle/#types-of-probe): liveness, readiness, startup probe.\n* Learn about [pod overhead](/docs/concepts/scheduling-eviction/pod-overhead/).\n",
    "parent_id": null,
    "source_file": "data/kubernetes/content/en/docs/concepts/workloads/pods/sidecar-containers.md",
    "section_path": [
      "concepts",
      "workloads",
      "pods"
    ],
    "chunk_type": "parent",
    "strategy": "recursive"
  },
  {
    "id": "user-namespaces-p0",
    "content": "## {{% heading \"prerequisites\" %}}\n\n{{% thirdparty-content %}}\n\nThis is a Linux-only feature and support is needed in Linux for idmap mounts on\nthe filesystems used. This means:\n\n* On the node, the filesystem you use for `/var/lib/kubelet/pods/`, or the\n  custom directory you configure for this, needs idmap mount support.\n* All the filesystems used in the pod's volumes must support idmap mounts.\n\nIn practice this means you need at least Linux 6.3, as tmpfs started supporting\nidmap mounts in that version. This is usually needed as several Kubernetes\nfeatures use tmpfs (the service account token that is mounted by default uses a\ntmpfs, Secrets use a tmpfs, etc.)\n\nSome popular filesystems that support idmap mounts in Linux 6.3 are: btrfs,\next4, xfs, fat, tmpfs, overlayfs.\n\nIn addition, the container runtime and its underlying OCI runtime must support\nuser namespaces. The following OCI runtimes offer support:\n\n* [crun](https://github.com/containers/crun) version 1.9 or greater (it's recommend version 1.13+).\n* [runc](https://github.com/opencontainers/runc) version 1.2 or greater\n\n{{< note >}}\nSome OCI runtimes do not include the support needed for using user namespaces in\nLinux pods. If you use a managed Kubernetes, or have downloaded it from packages\nand set it up, it's possible that nodes in your cluster use a runtime that doesn't\ninclude this support.\n{{< /note >}}\n\nTo use user namespaces with Kubernetes, you also need to use a CRI\n{{< glossary_tooltip text=\"container runtime\" term_id=\"container-runtime\" >}}\nto use this feature with Kubernetes pods:\n\n* containerd: version 2.0 (and later) supports user namespaces for containers.\n* CRI-O: version 1.25 (and later) supports user namespaces for containers.\n\nYou can see the status of user namespaces support in cri-dockerd tracked in an [issue][CRI-dockerd-issue]\non GitHub.\n\n[CRI-dockerd-issue]: https://github.com/Mirantis/cri-dockerd/issues/74\n\n",
    "parent_id": null,
    "source_file": "data/kubernetes/content/en/docs/concepts/workloads/pods/user-namespaces.md",
    "section_path": [
      "concepts",
      "workloads",
      "pods"
    ],
    "chunk_type": "parent",
    "strategy": "recursive"
  },
  {
    "id": "user-namespaces-p1",
    "content": "## Introduction\n\nUser namespaces is a Linux feature that allows to map users in the container to\ndifferent users in the host. Furthermore, the capabilities granted to a pod in\na user namespace are valid only in the namespace and void outside of it.\n\nA pod can opt-in to use user namespaces by setting the `pod.spec.hostUsers` field\nto `false`.\n\nThe kubelet will pick host UIDs/GIDs a pod is mapped to, and will do so in a way\nto guarantee that no two pods on the same node use the same mapping.\n\nThe `runAsUser`, `runAsGroup`, `fsGroup`, etc. fields in the `pod.spec` always\nrefer to the user inside the container. These users will be used for volume\nmounts (specified in `pod.spec.volumes`) and therefore the host UID/GID will not\nhave any effect on writes/reads from volumes the pod can mount. In other words,\nthe inodes created/read in volumes mounted by the pod will be the same as if the\npod wasn't using user namespaces.\n\nThis way, a pod can easily enable and disable user namespaces (without affecting\nits volume's file ownerships) and can also share volumes with pods without user\nnamespaces by just setting the appropriate users inside the container\n(`RunAsUser`, `RunAsGroup`, `fsGroup`, etc.). This applies to any volume the pod\ncan mount, including `hostPath` (if the pod is allowed to mount `hostPath`\nvolumes).\n\nBy default, the valid UIDs/GIDs when this feature is enabled is the range 0-65535.\nThis applies to files and processes (`runAsUser`, `runAsGroup`, etc.).\n\nFiles using a UID/GID outside this range will be seen as belonging to the\noverflow ID, usually 65534 (configured in `/proc/sys/kernel/overflowuid` and\n`/proc/sys/kernel/overflowgid`). However, it is not possible to modify those\nfiles, even by running as the 65534 user/group.\n\nIf the range 0-65535 is extended with a configuration knob, the aforementioned\nrestrictions apply to the extended range.\n\nMost applications that need to run as root but don't access other host\nnamespaces or resources, should continue to run fine without any changes needed\nif user namespaces is activated.\n\n",
    "parent_id": null,
    "source_file": "data/kubernetes/content/en/docs/concepts/workloads/pods/user-namespaces.md",
    "section_path": [
      "concepts",
      "workloads",
      "pods"
    ],
    "chunk_type": "parent",
    "strategy": "recursive"
  },
  {
    "id": "user-namespaces-p2",
    "content": "## Understanding user namespaces for pods {#pods-and-userns}\n\nSeveral container runtimes with their default configuration (like Docker Engine,\ncontainerd, CRI-O) use Linux namespaces for isolation. Other technologies exist\nand can be used with those runtimes too (e.g. Kata Containers uses VMs instead of\nLinux namespaces). This page is applicable for container runtimes using Linux\nnamespaces for isolation.\n\nWhen creating a pod, by default, several new namespaces are used for isolation:\na network namespace to isolate the network of the container, a PID namespace to\nisolate the view of processes, etc. If a user namespace is used, this will\nisolate the users in the container from the users in the node.\n\nThis means containers can run as root and be mapped to a non-root user on the\nhost. Inside the container the process will think it is running as root (and\ntherefore tools like `apt`, `yum`, etc. work fine), while in reality the process\ndoesn't have privileges on the host. You can verify this, for example, if you\ncheck which user the container process is running by executing `ps aux` from\nthe host. The user `ps` shows is not the same as the user you see if you\nexecute inside the container the command `id`.\n\nThis abstraction limits what can happen, for example, if the container manages\nto escape to the host. Given that the container is running as a non-privileged\nuser on the host, it is limited what it can do to the host.\n\nFurthermore, as users on each pod will be mapped to different non-overlapping\nusers in the host, it is limited what they can do to other pods too.\n\nCapabilities granted to a pod are also limited to the pod user namespace and\nmostly invalid out of it, some are even completely void. Here are two examples:\n- `CAP_SYS_MODULE` does not have any effect if granted to a pod using user\nnamespaces, the pod isn't able to load kernel modules.\n- `CAP_SYS_ADMIN` is limited to the pod's user namespace and invalid outside\nof it.\n\nWithout using a user namespace a container running as root, in the case of a\ncontainer breakout, has root privileges on the node. And if some capability were\ngranted to the container, the capabilities are valid on the host too. None of\nthis is true when we use user namespaces.\n\nIf you want to know more details about what changes when user namespaces are in\nuse, see `man 7 user_namespaces`.\n\n",
    "parent_id": null,
    "source_file": "data/kubernetes/content/en/docs/concepts/workloads/pods/user-namespaces.md",
    "section_path": [
      "concepts",
      "workloads",
      "pods"
    ],
    "chunk_type": "parent",
    "strategy": "recursive"
  },
  {
    "id": "user-namespaces-p3",
    "content": "## Set up a node to support user namespaces\n\nBy default, the kubelet assigns pods UIDs/GIDs above the range 0-65535, based on\nthe assumption that the host's files and processes use UIDs/GIDs within this\nrange, which is standard for most Linux distributions. This approach prevents\nany overlap between the UIDs/GIDs of the host and those of the pods.\n\nAvoiding the overlap is important to mitigate the impact of vulnerabilities such\nas [CVE-2021-25741][CVE-2021-25741], where a pod can potentially read arbitrary\nfiles in the host. If the UIDs/GIDs of the pod and the host don't overlap, it is\nlimited what a pod would be able to do: the pod UID/GID won't match the host's\nfile owner/group.\n\nThe kubelet can use a custom range for user IDs and group IDs for pods. To\nconfigure a custom range, the node needs to have:\n\n * A user `kubelet` in the system (you cannot use any other username here)\n * The binary `getsubids` installed (part of [shadow-utils][shadow-utils]) and\n   in the `PATH` for the kubelet binary.\n * A configuration of subordinate UIDs/GIDs for the `kubelet` user (see\n   [`man 5 subuid`](https://man7.org/linux/man-pages/man5/subuid.5.html) and\n   [`man 5 subgid`](https://man7.org/linux/man-pages/man5/subgid.5.html)).\n\nThis setting only gathers the UID/GID range configuration and does not change\nthe user executing the `kubelet`.\n\nYou must follow some constraints for the subordinate ID range that you assign\nto the `kubelet` user:\n\n* The subordinate user ID, that starts the UID range for Pods, **must** be a\n  multiple of 65536 and must also be greater than or equal to 65536. In other\n  words, you cannot use any ID from the range 0-65535 for Pods; the kubelet\n  imposes this restriction to make it difficult to create an accidentally insecure\n  configuration.\n\n* The subordinate ID count must be a multiple of 65536\n\n* The subordinate ID count must be at least `65536 x <maxPods>` where `<maxPods>`\n  is the maximum number of pods that can run on the node.\n\n* You must assign the same range for both user IDs and for group IDs, It doesn't\n  matter if other users have user ID ranges that don't align with the group ID\n  ranges.\n\n* None of the assigned ranges should overlap with any other assignment.\n\n* The subordinate configuration must be only one line. In other words, you can't\n  have multiple ranges.\n\nFor example, you could define `/etc/subuid` and `/etc/subgid` to both have\nthese entries for the `kubelet` user:\n\n```\n# The format is\n#   name:firstID:count of IDs\n# where\n# - firstID is 65536 (the minimum value possible)\n# - count of IDs is 110 * 65536\n#   (110 is the default limit for number of pods on the node)\n\nkubelet:65536:7208960\n```\n\n[CVE-2021-25741]: https://github.com/kubernetes/kubernetes/issues/104980\n[shadow-utils]: https://github.com/shadow-maint/shadow\n\n",
    "parent_id": null,
    "source_file": "data/kubernetes/content/en/docs/concepts/workloads/pods/user-namespaces.md",
    "section_path": [
      "concepts",
      "workloads",
      "pods"
    ],
    "chunk_type": "parent",
    "strategy": "recursive"
  },
  {
    "id": "user-namespaces-p4",
    "content": "## ID count for each of Pods\nStarting with Kubernetes v1.33, the ID count for each of Pods can be set in\n[`KubeletConfiguration`](/docs/reference/config-api/kubelet-config.v1beta1/).\n\n```yaml\napiVersion: kubelet.config.k8s.io/v1beta1\nkind: KubeletConfiguration\nuserNamespaces:\n  idsPerPod: 1048576\n```\n\nThe value of `idsPerPod` (uint32) must be a multiple of 65536.\nThe default value is 65536.\nThis value only applies to containers created after the kubelet was started with\nthis `KubeletConfiguration`.\nRunning containers are not affected by this config.\n\nIn Kubernetes prior to v1.33, the ID count for each of Pods was hard-coded to\n65536.\n\n",
    "parent_id": null,
    "source_file": "data/kubernetes/content/en/docs/concepts/workloads/pods/user-namespaces.md",
    "section_path": [
      "concepts",
      "workloads",
      "pods"
    ],
    "chunk_type": "parent",
    "strategy": "recursive"
  },
  {
    "id": "user-namespaces-p5",
    "content": "## Integration with Pod security admission checks\n\n{{< feature-state state=\"alpha\" for_k8s_version=\"v1.29\" >}}\n\nFor Linux Pods that enable user namespaces, Kubernetes relaxes the application of\n[Pod Security Standards](/docs/concepts/security/pod-security-standards) in a controlled way.\n\nIf you create a Pod that uses user\nnamespaces, the following fields won't be constrained even in contexts that enforce the\n_Baseline_ or _Restricted_ pod security standard. This behavior does not\npresent a security concern because `root` inside a Pod with user namespaces\nactually refers to the user inside the container, that is never mapped to a\nprivileged user on the host. Here's the list of fields that are **not** checks for Pods in those\ncircumstances:\n\n- `spec.securityContext.runAsNonRoot`\n- `spec.containers[*].securityContext.runAsNonRoot`\n- `spec.initContainers[*].securityContext.runAsNonRoot`\n- `spec.ephemeralContainers[*].securityContext.runAsNonRoot`\n- `spec.securityContext.runAsUser`\n- `spec.containers[*].securityContext.runAsUser`\n- `spec.initContainers[*].securityContext.runAsUser`\n- `spec.ephemeralContainers[*].securityContext.runAsUser`\n\nFurther, if the pod is in a context with the _Baseline_ pod security standard,\nvalidation for the following fields will similarly be relaxed:\n\n- `spec.containers[*].securityContext.procMount`\n- `spec.initContainers[*].securityContext.procMount`\n- `spec.ephemeralContainers[*].securityContext.procMount`\n\nwith the _Restricted_ pod security standard, a pod still must only use the\ndefault or empty ProcMount.\n\n\n",
    "parent_id": null,
    "source_file": "data/kubernetes/content/en/docs/concepts/workloads/pods/user-namespaces.md",
    "section_path": [
      "concepts",
      "workloads",
      "pods"
    ],
    "chunk_type": "parent",
    "strategy": "recursive"
  },
  {
    "id": "user-namespaces-p6",
    "content": "## Limitations\n\nWhen using a user namespace for the pod, it is disallowed to use other host\nnamespaces. In particular, if you set `hostUsers: false` then you are not\nallowed to set any of:\n\n * `hostNetwork: true`\n * `hostIPC: true`\n * `hostPID: true`\n\nNo container can use `volumeDevices` (raw block volumes, like /dev/sda) either.\nThis includes all the container arrays in the pod spec:\n * `containers`\n * `initContainers`\n * `ephemeralContainers`\n\n",
    "parent_id": null,
    "source_file": "data/kubernetes/content/en/docs/concepts/workloads/pods/user-namespaces.md",
    "section_path": [
      "concepts",
      "workloads",
      "pods"
    ],
    "chunk_type": "parent",
    "strategy": "recursive"
  },
  {
    "id": "user-namespaces-p7",
    "content": "## Metrics and observability\n\nThe kubelet exports two prometheus metrics specific to user-namespaces:\n * `started_user_namespaced_pods_total`: a counter that tracks the number of user namespaced pods that are attempted to be created.\n * `started_user_namespaced_pods_errors_total`: a counter that tracks the number of errors creating user namespaced pods.\n\n",
    "parent_id": null,
    "source_file": "data/kubernetes/content/en/docs/concepts/workloads/pods/user-namespaces.md",
    "section_path": [
      "concepts",
      "workloads",
      "pods"
    ],
    "chunk_type": "parent",
    "strategy": "recursive"
  },
  {
    "id": "user-namespaces-p8",
    "content": "## {{% heading \"whatsnext\" %}}\n\n* Take a look at [Use a User Namespace With a Pod](/docs/tasks/configure-pod-container/user-namespaces/)\n",
    "parent_id": null,
    "source_file": "data/kubernetes/content/en/docs/concepts/workloads/pods/user-namespaces.md",
    "section_path": [
      "concepts",
      "workloads",
      "pods"
    ],
    "chunk_type": "parent",
    "strategy": "recursive"
  },
  {
    "id": "workload-reference-p0",
    "content": "## Specifying a Workload reference\n\nWhen the [`GenericWorkload`]((/docs/reference/command-line-tools-reference/feature-gates/#GenericWorkload))\nfeature gate is enabled, you can use the `spec.workloadRef` field in your Pod manifest.\nThis field establishes a link to a specific pod group defined within a Workload resource\nin the same namespace.\n\n```yaml\napiVersion: v1\nkind: Pod\nmetadata:\n  name: worker-0\n  namespace: some-ns\nspec:\n  workloadRef:\n    # The name of the Workload object in the same namespace\n    name: training-job-workload\n    # The name of the specific pod group inside that Workload\n    podGroup: workers\n```\n\n### Pod group replicas\n\nFor more complex scenarios, you can replicate a single pod group into multiple, independent scheduling units.\nYou achieve this using the `podGroupReplicaKey` field within a Pod's `workloadRef`. This key acts as a label\nto create logical subgroups.\n\nFor example, if you have a pod group with `minCount: 2` and you create four Pods: two with `podGroupReplicaKey: \"0\"`\nand two with `podGroupReplicaKey: \"1\"`, they will be treated as two independent groups of two Pods.\n\n```yaml\nspec:\n  workloadRef:\n    name: training-job-workload\n    podGroup: workers\n    # All workers with the replica key \"0\" will be scheduled together as one group.\n    podGroupReplicaKey: \"0\"\n```\n\n### Behavior\n\nWhen you define a `workloadRef`, the Pod behaves differently depending on the\n[policy](/docs/concepts/workloads/workload-api/policies/) defined in the referenced pod group.\n\n* If the referenced group uses the `basic` policy, the workload reference acts primarily as a grouping label.\n* If the referenced group uses the `gang` policy\n  (and the [`GangScheduling`]((/docs/reference/command-line-tools-reference/feature-gates/#GangScheduling)) feature gate is enabled),\n  the Pod enters a gang scheduling lifecycle. It will wait for other Pods in the group to be created\n  and scheduled before binding to a node.\n\n### Missing references\n\nThe scheduler validates the `workloadRef` before making any placement decisions.\n\nIf a Pod references a Workload that does not exist, or a pod group that is not defined within that Workload,\nthe Pod will remain pending. It is not considered for placement until you create the missing Workload object\nor recreate it to include the missing `PodGroup` definition.\n\nThis behavior applies to all Pods with a `workloadRef`, regardless of whether the eventual policy will be `basic` or `gang`,\nas the scheduler requires the Workload definition to determine the policy.\n\n",
    "parent_id": null,
    "source_file": "data/kubernetes/content/en/docs/concepts/workloads/pods/workload-reference.md",
    "section_path": [
      "concepts",
      "workloads",
      "pods"
    ],
    "chunk_type": "parent",
    "strategy": "recursive"
  },
  {
    "id": "workload-reference-p1",
    "content": "## {{% heading \"whatsnext\" %}}\n\n* Learn about the [Workload API](/docs/concepts/workloads/workload-api/).\n* Read the details of [pod group policies](/docs/concepts/workloads/workload-api/policies/).\n",
    "parent_id": null,
    "source_file": "data/kubernetes/content/en/docs/concepts/workloads/pods/workload-reference.md",
    "section_path": [
      "concepts",
      "workloads",
      "pods"
    ],
    "chunk_type": "parent",
    "strategy": "recursive"
  },
  {
    "id": 1,
    "content": "## What is a Pod?\n\n{{< note >}}\nYou need to install a [container runtime](/docs/setup/production-environment/container-runtimes/)\ninto each node in the cluster so that Pods can run there.\n{{< /note >}}\n\nThe shared context of a Pod is a set of Linux namespaces, cgroups, and\npotentially other facets of isolation - the same things that isolate a {{< glossary_tooltip text=\"container\" term_id=\"container\" >}}. Within a Pod's context, the individual applications may have\nfurther sub-isolations applied.\n\nA Pod is similar to a set of containers with shared namespaces and shared filesystem volumes.\n\nPods in a Kubernetes cluster are used in two main ways:\n\n* **Pods that run a single container**. The \"one-container-per-Pod\" model is the\n  most common Kubernetes use case; in this case, you can think of a Pod as a\n  wrapper around a single container; Kubernetes manages Pods rather than managing\n  the containers directly.\n* **Pods that run multiple containers that need to work together**. A Pod can\n  encapsulate an application composed of\n  [multiple co-located containers](#how-pods-manage-multiple-containers) that are\n  tightly coupled and need to share resources. These co-located containers\n  form a single cohesive unit.\n\n  Grouping multiple co-located and co-managed containers in a single Pod is a\n  relatively advanced use case. You should use this pattern only in specific\n  instances in which your containers are tightly coupled.\n\n  You don't need to run multiple containers to provide replication (for resilience\n  or capacity); if you need multiple replicas, see\n  [Workload management](/docs/concepts/workloads/controllers/).\n\n",
    "parent_id": null,
    "source_file": "data/kubernetes/content/en/docs/concepts/workloads/pods/_index.md",
    "section_path": [
      "concepts",
      "workloads",
      "pods"
    ],
    "chunk_type": "parent",
    "strategy": "recursive"
  },
  {
    "id": 2,
    "content": "## Using Pods\n\nThe following is an example of a Pod which consists of a container running the image `nginx:1.14.2`.\n\n{{% code_sample file=\"pods/simple-pod.yaml\" %}}\n\nTo create the Pod shown above, run the following command:\n```shell\nkubectl apply -f https://k8s.io/examples/pods/simple-pod.yaml\n```\n\nPods are generally not created directly and are created using workload resources.\nSee [Working with Pods](#working-with-pods) for more information on how Pods are used\nwith workload resources.\n\n### Workload resources for managing pods\n\nUsually you don't need to create Pods directly, even singleton Pods. Instead, create them using workload resources such as {{< glossary_tooltip text=\"Deployment\"\nterm_id=\"deployment\" >}} or {{< glossary_tooltip text=\"Job\" term_id=\"job\" >}}.\nIf your Pods need to track state, consider the\n{{< glossary_tooltip text=\"StatefulSet\" term_id=\"statefulset\" >}} resource.\n\n\nEach Pod is meant to run a single instance of a given application. If you want to\nscale your application horizontally (to provide more overall resources by running\nmore instances), you should use multiple Pods, one for each instance. In\nKubernetes, this is typically referred to as _replication_.\nReplicated Pods are usually created and managed as a group by a workload resource\nand its {{< glossary_tooltip text=\"controller\" term_id=\"controller\" >}}.\n\nSee [Pods and controllers](#pods-and-controllers) for more information on how\nKubernetes uses workload resources, and their controllers, to implement application\nscaling and auto-healing.\n\nPods natively provide two kinds of shared resources for their constituent containers:\n[networking](#pod-networking) and [storage](#pod-storage).\n\n\n",
    "parent_id": null,
    "source_file": "data/kubernetes/content/en/docs/concepts/workloads/pods/_index.md",
    "section_path": [
      "concepts",
      "workloads",
      "pods"
    ],
    "chunk_type": "parent",
    "strategy": "recursive"
  },
  {
    "id": 3,
    "content": "## Working with Pods\n\nYou'll rarely create individual Pods directly in Kubernetes—even singleton Pods. This\nis because Pods are designed as relatively ephemeral, disposable entities. When\na Pod gets created (directly by you, or indirectly by a\n{{< glossary_tooltip text=\"controller\" term_id=\"controller\" >}}), the new Pod is\nscheduled to run on a {{< glossary_tooltip term_id=\"node\" >}} in your cluster.\nThe Pod remains on that node until the Pod finishes execution, the Pod object is deleted,\nthe Pod is *evicted* for lack of resources, or the node fails.\n\n{{< note >}}\nRestarting a container in a Pod should not be confused with restarting a Pod. A Pod\nis not a process, but an environment for running container(s). A Pod persists until\nit is deleted.\n{{< /note >}}\n\nThe name of a Pod must be a valid\n[DNS subdomain](/docs/concepts/overview/working-with-objects/names#dns-subdomain-names)\nvalue, but this can produce unexpected results for the Pod hostname.  For best compatibility,\nthe name should follow the more restrictive rules for a\n[DNS label](/docs/concepts/overview/working-with-objects/names#dns-label-names).\n\n### Pod OS\n\n{{< feature-state state=\"stable\" for_k8s_version=\"v1.25\" >}}\n\nYou should set the `.spec.os.name` field to either `windows` or `linux` to indicate the OS on\nwhich you want the pod to run. These two are the only operating systems supported for now by\nKubernetes. In the future, this list may be expanded.\n\nIn Kubernetes v{{< skew currentVersion >}}, the value of `.spec.os.name` does not affect\nhow the {{< glossary_tooltip text=\"kube-scheduler\" term_id=\"kube-scheduler\" >}}\npicks a node for the Pod to run on. In any cluster where there is more than one operating system for\nrunning nodes, you should set the\n[kubernetes.io/os](/docs/reference/labels-annotations-taints/#kubernetes-io-os)\nlabel correctly on each node, and define pods with a `nodeSelector` based on the operating system\nlabel. The kube-scheduler assigns your pod to a node based on other criteria and may or may not\nsucceed in picking a suitable node placement where the node OS is right for the containers in that Pod.\nThe [Pod security standards](/docs/concepts/security/pod-security-standards/) also use this\nfield to avoid enforcing policies that aren't relevant to the operating system.\n\n### Pods and controllers\n\nYou can use workload resources to create and manage multiple Pods for you. A controller\nfor the resource handles replication and rollout and automatic healing in case of\nPod failure. For example, if a Node fails, a controller notices that Pods on that\nNode have stopped working and creates a replacement Pod. The scheduler places the\nreplacement Pod onto a healthy Node.\n\nHere are some examples of workload resources that manage one or more Pods:\n\n* {{< glossary_tooltip text=\"Deployment\" term_id=\"deployment\" >}}\n* {{< glossary_tooltip text=\"StatefulSet\" term_id=\"statefulset\" >}}\n* {{< glossary_tooltip text=\"DaemonSet\" term_id=\"daemonset\" >}}\n\n### Specifying a Workload reference\n\n{{< feature-state feature_gate_name=\"GenericWorkload\" >}}\n\nBy default, Kubernetes schedules every Pod individually. However, some tightly-coupled applications\nneed a group of Pods to be scheduled simultaneously to function correctly.\n\nYou can link a Pod to a [Workload](/docs/concepts/workloads/workload-api/) object\nusing a [Workload reference](/docs/concepts/workloads/pods/workload-reference/).\nThis tells the `kube-scheduler` that the Pod is part of a specific group,\nenabling it to make coordinated placement decisions for the entire group at once.\n\n### Pod templates\n\nControllers for {{< glossary_tooltip text=\"workload\" term_id=\"workload\" >}} resources create Pods\nfrom a _pod template_ and manage those Pods on your behalf.\n\nPodTemplates are specifications for creating Pods, and are included in workload resources such as\n[Deployments](/docs/concepts/workloads/controllers/deployment/),\n[Jobs](/docs/concepts/workloads/controllers/job/), and\n[DaemonSets](/docs/concepts/workloads/controllers/daemonset/).\n\nEach controller for a workload resource uses the `PodTemplate` inside the workload\nobject to make actual Pods. The `PodTemplate` is part of the desired state of whatever\nworkload resource you used to run your app.\n\nWhen you create a Pod, you can include\n[environment variables](/docs/tasks/inject-data-application/define-environment-variable-container/)\nin the Pod template for the containers that run in the Pod.\n\nThe sample below is a manifest for a simple Job with a `template` that starts one\ncontainer. The container in that Pod prints a message then pauses.\n\n```yaml\napiVersion: batch/v1\nkind: Job\nmetadata:\n  name: hello\nspec:\n  template:\n    # This is the pod template\n    spec:\n      containers:\n      - name: hello\n        image: busybox:1.28\n        command: ['sh', '-c', 'echo \"Hello, Kubernetes!\" && sleep 3600']\n      restartPolicy: OnFailure\n    # The pod template ends here\n```\n\nModifying the pod template or switching to a new pod template has no direct effect\non the Pods that already exist. If you change the pod template for a workload\nresource, that resource needs to create replacement Pods that use the updated template.\n\nFor example, the StatefulSet controller ensures that the running Pods match the current\npod template for each StatefulSet object. If you edit the StatefulSet to change its pod\ntemplate, the StatefulSet starts to create new Pods based on the updated template.\nEventually, all of the old Pods are replaced with new Pods, and the update is complete.\n\nEach workload resource implements its own rules for handling changes to the Pod template.\nIf you want to read more about StatefulSet specifically, read\n[Update strategy](/docs/tutorials/stateful-application/basic-stateful-set/#updating-statefulsets) in the StatefulSet Basics tutorial.\n\nOn Nodes, the {{< glossary_tooltip term_id=\"kubelet\" text=\"kubelet\" >}} does not\ndirectly observe or manage any of the details around pod templates and updates; those\ndetails are abstracted away. That abstraction and separation of concerns simplifies\nsystem semantics, and makes it feasible to extend the cluster's behavior without\nchanging existing code.\n\n",
    "parent_id": null,
    "source_file": "data/kubernetes/content/en/docs/concepts/workloads/pods/_index.md",
    "section_path": [
      "concepts",
      "workloads",
      "pods"
    ],
    "chunk_type": "parent",
    "strategy": "recursive"
  },
  {
    "id": 4,
    "content": "## Pod update and replacement\n\nAs mentioned in the previous section, when the Pod template for a workload\nresource is changed, the controller creates new Pods based on the updated\ntemplate instead of updating or patching the existing Pods.\n\nKubernetes doesn't prevent you from managing Pods directly. It is possible to\nupdate some fields of a running Pod, in place. However, Pod update operations\nlike\n[`patch`](/docs/reference/generated/kubernetes-api/{{< param \"version\" >}}/#patch-pod-v1-core), and\n[`replace`](/docs/reference/generated/kubernetes-api/{{< param \"version\" >}}/#replace-pod-v1-core)\nhave some limitations:\n\n- Most of the metadata about a Pod is immutable. For example, you cannot\n  change the `namespace`, `name`, `uid`, or `creationTimestamp` fields.\n\n- If the `metadata.deletionTimestamp` is set, no new entry can be added to the\n  `metadata.finalizers` list.\n- Pod updates may not change fields other than `spec.containers[*].image`,\n  `spec.initContainers[*].image`, `spec.activeDeadlineSeconds`, `spec.terminationGracePeriodSeconds`,\n  `spec.tolerations` or `spec.schedulingGates`. For `spec.tolerations`, you can only add new entries.\n- When updating the `spec.activeDeadlineSeconds` field, two types of updates\n  are allowed:\n\n  1. setting the unassigned field to a positive number;\n  1. updating the field from a positive number to a smaller, non-negative\n     number.\n\n### Pod subresources\n\nThe above update rules apply to regular pod updates, but other pod fields can be updated through _subresources_.\n\n- **Resize:** The `resize` subresource allows container resources (`spec.containers[*].resources`) to be updated.\n  See [Resize Container Resources](/docs/tasks/configure-pod-container/resize-container-resources/) for more details.\n- **Ephemeral Containers:** The `ephemeralContainers` subresource allows\n  {{< glossary_tooltip text=\"ephemeral containers\" term_id=\"ephemeral-container\" >}}\n  to be added to a Pod.\n  See [Ephemeral Containers](/docs/concepts/workloads/pods/ephemeral-containers/) for more details.\n- **Status:** The `status` subresource allows the pod status to be updated.\n  This is typically only used by the Kubelet and other system controllers.\n- **Binding:** The `binding` subresource allows setting the pod's `spec.nodeName` via a `Binding` request.\n  This is typically only used by the {{< glossary_tooltip text=\"scheduler\" term_id=\"kube-scheduler\" >}}.\n\n### Pod generation\n\n- The `metadata.generation` field is unique. It will be automatically set by the\n  system such that new pods have a `metadata.generation` of 1, and every update to\n  mutable fields in the pod's spec will increment the `metadata.generation` by 1.\n\n{{< feature-state feature_gate_name=\"PodObservedGenerationTracking\" >}}\n\n- `observedGeneration` is a field that is captured in the `status` section of the Pod\n  object. The Kubelet will set `status.observedGeneration`\n  to track the pod state to the current pod status. The pod's `status.observedGeneration` will reflect the\n  `metadata.generation` of the pod at the point that the pod status is being reported.\n\n{{< note >}}\nThe `status.observedGeneration` field is managed by the kubelet and external controllers should **not** modify this field.\n{{< /note >}}\n\nDifferent status fields may either be associated with the `metadata.generation` of the current sync loop, or with the\n`metadata.generation` of the previous sync loop. The key distinction is whether a change in the `spec` is reflected\ndirectly in the `status` or is an indirect result of a running process.\n\n#### Direct Status Updates\n\nFor status fields where the allocated spec is directly reflected, the `observedGeneration` will\nbe associated with the current `metadata.generation` (Generation N).\n\nThis behavior applies to:\n\n- **Resize Status**: The status of a resource resize operation.\n- **Allocated Resources**: The resources allocated to the Pod after a resize.\n- **Ephemeral Containers**: When a new ephemeral container is added, and it is in `Waiting` state.\n\n#### Indirect Status Updates\n\nFor status fields that are an indirect result of running the spec, the `observedGeneration` will be associated\nwith the `metadata.generation` of the previous sync loop (Generation N-1).\n\nThis behavior applies to:\n\n- **Container Image**: The `ContainerStatus.ImageID` reflects the image from the previous generation until the new image\n  is pulled and the container is updated.\n- **Actual Resources**: During an in-progress resize, the actual resources in use still belong to the previous generation's\n  request.\n- **Container state**: During an in-progress resize, with require restart policy reflects the previous generation's\n  request.\n- **activeDeadlineSeconds** & **terminationGracePeriodSeconds** & **deletionTimestamp**: The effects of these fields on the\n  Pod's status are a result of the previously observed specification.\n\n",
    "parent_id": null,
    "source_file": "data/kubernetes/content/en/docs/concepts/workloads/pods/_index.md",
    "section_path": [
      "concepts",
      "workloads",
      "pods"
    ],
    "chunk_type": "parent",
    "strategy": "recursive"
  },
  {
    "id": 5,
    "content": "## Resource sharing and communication\n\nPods enable data sharing and communication among their constituent\ncontainers.\n\n### Storage in Pods {#pod-storage}\n\nA Pod can specify a set of shared storage\n{{< glossary_tooltip text=\"volumes\" term_id=\"volume\" >}}. All containers\nin the Pod can access the shared volumes, allowing those containers to\nshare data. Volumes also allow persistent data in a Pod to survive\nin case one of the containers within needs to be restarted. See\n[Storage](/docs/concepts/storage/) for more information on how\nKubernetes implements shared storage and makes it available to Pods.\n\n### Pod networking\n\nEach Pod is assigned a unique IP address for each address family. Every\ncontainer in a Pod shares the network namespace, including the IP address and\nnetwork ports. Inside a Pod (and **only** then), the containers that belong to the Pod\ncan communicate with one another using `localhost`. When containers in a Pod communicate\nwith entities *outside the Pod*,\nthey must coordinate how they use the shared network resources (such as ports).\nWithin a Pod, containers share an IP address and port space, and\ncan find each other via `localhost`. The containers in a Pod can also communicate\nwith each other using standard inter-process communications like SystemV semaphores\nor POSIX shared memory.  Containers in different Pods have distinct IP addresses\nand can not communicate by OS-level IPC without special configuration.\nContainers that want to interact with a container running in a different Pod can\nuse IP networking to communicate.\n\nContainers within the Pod see the system hostname as being the same as the configured\n`name` for the Pod. There's more about this in the [networking](/docs/concepts/cluster-administration/networking/)\nsection.\n\n",
    "parent_id": null,
    "source_file": "data/kubernetes/content/en/docs/concepts/workloads/pods/_index.md",
    "section_path": [
      "concepts",
      "workloads",
      "pods"
    ],
    "chunk_type": "parent",
    "strategy": "recursive"
  },
  {
    "id": 6,
    "content": "## Pod security settings {#pod-security}\n\nTo set security constraints on Pods and containers, you use the\n`securityContext` field in the Pod specification. This field gives you\ngranular control over what a Pod or individual containers can do. See [Advanced Pod Configuration](/docs/concepts/workloads/pods/advanced-pod-config/) for more details.\n\nFor basic security configuration, you should meet the Baseline Pod security standard and run containers as non-root. You can set simple security contexts:\n\n```yaml\napiVersion: v1\nkind: Pod\nmetadata:\n  name: security-context-demo\nspec:\n  securityContext:\n    runAsUser: 1000\n    runAsGroup: 3000\n    fsGroup: 2000\n  containers:\n  - name: sec-ctx-demo\n    image: busybox\n    command: [\"sh\", \"-c\", \"sleep 1h\"]\n```\n\nFor advanced security context configuration including capabilities, seccomp profiles, and detailed security options, see the [security concepts](/docs/concepts/security/) section.\n\n* To learn about kernel-level security constraints that you can use,\n  see [Linux kernel security constraints for Pods and containers](/docs/concepts/security/linux-kernel-security-constraints).\n* To learn more about the Pod security context, see\n  [Configure a Security Context for a Pod or Container](/docs/tasks/configure-pod-container/security-context/).\n\n",
    "parent_id": null,
    "source_file": "data/kubernetes/content/en/docs/concepts/workloads/pods/_index.md",
    "section_path": [
      "concepts",
      "workloads",
      "pods"
    ],
    "chunk_type": "parent",
    "strategy": "recursive"
  },
  {
    "id": 7,
    "content": "## Static Pods\n\n_Static Pods_ are managed directly by the kubelet daemon on a specific node,\nwithout the {{< glossary_tooltip text=\"API server\" term_id=\"kube-apiserver\" >}}\nobserving them.\nWhereas most Pods are managed by the control plane (for example, a\n{{< glossary_tooltip text=\"Deployment\" term_id=\"deployment\" >}}), for static\nPods, the kubelet directly supervises each static Pod (and restarts it if it fails).\n\nStatic Pods are always bound to one {{< glossary_tooltip term_id=\"kubelet\" >}} on a specific node.\nThe main use for static Pods is to run a self-hosted control plane: in other words,\nusing the kubelet to supervise the individual [control plane components](/docs/concepts/architecture/#control-plane-components).\n\nThe kubelet automatically tries to create a {{< glossary_tooltip text=\"mirror Pod\" term_id=\"mirror-pod\" >}}\non the Kubernetes API server for each static Pod.\nThis means that the Pods running on a node are visible on the API server,\nbut cannot be controlled from there. See the guide [Create static Pods](/docs/tasks/configure-pod-container/static-pod)\nfor more information.\n\n{{< note >}}\nThe `spec` of a static Pod cannot refer to other API objects\n(e.g., {{< glossary_tooltip text=\"ServiceAccount\" term_id=\"service-account\" >}},\n{{< glossary_tooltip text=\"ConfigMap\" term_id=\"configmap\" >}},\n{{< glossary_tooltip text=\"Secret\" term_id=\"secret\" >}}, etc).\n{{< /note >}}\n\n",
    "parent_id": null,
    "source_file": "data/kubernetes/content/en/docs/concepts/workloads/pods/_index.md",
    "section_path": [
      "concepts",
      "workloads",
      "pods"
    ],
    "chunk_type": "parent",
    "strategy": "recursive"
  },
  {
    "id": 8,
    "content": "## Pods with multiple containers {#how-pods-manage-multiple-containers}\n\nPods are designed to support multiple cooperating processes (as containers) that form\na cohesive unit of service. The containers in a Pod are automatically co-located and\nco-scheduled on the same physical or virtual machine in the cluster. The containers\ncan share resources and dependencies, communicate with one another, and coordinate\nwhen and how they are terminated.\n\n<!--intentionally repeats some text from earlier in the page, with more detail -->\nPods in a Kubernetes cluster are used in two main ways:\n\n* **Pods that run a single container**. The \"one-container-per-Pod\" model is the\n  most common Kubernetes use case; in this case, you can think of a Pod as a\n  wrapper around a single container; Kubernetes manages Pods rather than managing\n  the containers directly.\n* **Pods that run multiple containers that need to work together**. A Pod can\n  encapsulate an application composed of\n  multiple co-located containers that are\n  tightly coupled and need to share resources. These co-located containers\n  form a single cohesive unit of service—for example, one container serving data\n  stored in a shared volume to the public, while a separate\n  {{< glossary_tooltip text=\"sidecar container\" term_id=\"sidecar-container\" >}}\n  refreshes or updates those files.\n  The Pod wraps these containers, storage resources, and an ephemeral network\n  identity together as a single unit.\n\nFor example, you might have a container that\nacts as a web server for files in a shared volume, and a separate\n[sidecar container](/docs/concepts/workloads/pods/sidecar-containers/)\nthat updates those files from a remote source, as in the following diagram:\n\n{{< figure src=\"/images/docs/pod.svg\" alt=\"Pod creation diagram\" class=\"diagram-medium\" >}}\n\nSome Pods have {{< glossary_tooltip text=\"init containers\" term_id=\"init-container\" >}}\nas well as {{< glossary_tooltip text=\"app containers\" term_id=\"app-container\" >}}.\nBy default, init containers run and complete before the app containers are started.\n\nYou can also have [sidecar containers](/docs/concepts/workloads/pods/sidecar-containers/)\nthat provide auxiliary services to the main application Pod (for example: a service mesh).\n\n{{< feature-state feature_gate_name=\"SidecarContainers\" >}}\n\nEnabled by default, the `SidecarContainers` [feature gate](/docs/reference/command-line-tools-reference/feature-gates/)\nallows you to specify `restartPolicy: Always` for init containers.\nSetting the `Always` restart policy ensures that the containers where you set it are\ntreated as _sidecars_ that are kept running during the entire lifetime of the Pod.\nContainers that you explicitly define as sidecar containers\nstart up before the main application Pod and remain running until the Pod is\nshut down.\n\n\n",
    "parent_id": null,
    "source_file": "data/kubernetes/content/en/docs/concepts/workloads/pods/_index.md",
    "section_path": [
      "concepts",
      "workloads",
      "pods"
    ],
    "chunk_type": "parent",
    "strategy": "recursive"
  },
  {
    "id": 9,
    "content": "## Container probes\n\nA _probe_ is a diagnostic performed periodically by the kubelet on a container.\nTo perform a diagnostic, the kubelet can invoke different actions:\n\n- `ExecAction` (performed with the help of the container runtime)\n- `TCPSocketAction` (checked directly by the kubelet)\n- `HTTPGetAction` (checked directly by the kubelet)\n\nYou can read more about [probes](/docs/concepts/workloads/pods/pod-lifecycle/#container-probes)\nin the Pod Lifecycle documentation.\n\n",
    "parent_id": null,
    "source_file": "data/kubernetes/content/en/docs/concepts/workloads/pods/_index.md",
    "section_path": [
      "concepts",
      "workloads",
      "pods"
    ],
    "chunk_type": "parent",
    "strategy": "recursive"
  },
  {
    "id": 10,
    "content": "## {{% heading \"whatsnext\" %}}\n\n* Learn about the [lifecycle of a Pod](/docs/concepts/workloads/pods/pod-lifecycle/).\n* Read about [PodDisruptionBudget](/docs/concepts/workloads/pods/disruptions/)\n  and how you can use it to manage application availability during disruptions.\n* Pod is a top-level resource in the Kubernetes REST API.\n  The {{< api-reference page=\"workload-resources/pod-v1\" >}}\n  object definition describes the object in detail.\n* [The Distributed System Toolkit: Patterns for Composite Containers](/blog/2015/06/the-distributed-system-toolkit-patterns/) explains common layouts for Pods with more than one container.\n* Read about [Pod topology spread constraints](/docs/concepts/scheduling-eviction/topology-spread-constraints/)\n* Read [Advanced Pod Configuration](/docs/concepts/workloads/pods/advanced-pod-config/) to learn the topic in detail.\n  That page covers aspects of Pod configuration beyond the essentials, including:\n  * PriorityClasses\n  * RuntimeClasses\n  * advanced ways to configure _scheduling_: the way that Kubernetes decides which node a Pod should run on.\n\nTo understand the context for why Kubernetes wraps a common Pod API in other resources\n(such as {{< glossary_tooltip text=\"StatefulSets\" term_id=\"statefulset\" >}} or\n{{< glossary_tooltip text=\"Deployments\" term_id=\"deployment\" >}}),\nyou can read about the prior art, including:\n\n* [Aurora](https://aurora.apache.org/documentation/latest/reference/configuration/#job-schema)\n* [Borg](https://research.google/pubs/large-scale-cluster-management-at-google-with-borg/)\n* [Marathon](https://github.com/d2iq-archive/marathon)\n* [Omega](https://research.google/pubs/pub41684/)\n* [Tupperware](https://engineering.fb.com/data-center-engineering/tupperware/).\n",
    "parent_id": null,
    "source_file": "data/kubernetes/content/en/docs/concepts/workloads/pods/_index.md",
    "section_path": [
      "concepts",
      "workloads",
      "pods"
    ],
    "chunk_type": "parent",
    "strategy": "recursive"
  },
  {
    "id": 155,
    "content": "## PriorityClasses\n\n_PriorityClasses_ allow you to set the importance of Pods relative to other Pods.\nIf you assign a priority class to a Pod, Kubernetes sets the `.spec.priority` field for that Pod\nbased on the PriorityClass you specified (you cannot set `.spec.priority` directly).\nIf or when a Pod cannot be scheduled, and the problem is due to a lack of resources, the {{< glossary_tooltip term_id=\"kube-scheduler\" text=\"kube-scheduler\" >}}\ntries to {{< glossary_tooltip text=\"preempt\" term_id=\"preemption\" >}} lower priority\nPods, in order to make scheduling of the higher priority Pod possible.\n\nA PriorityClass is a cluster-scoped API object that maps a priority class name to an integer priority value. Higher numbers indicate higher priority.\n\n### Defining a PriorityClass\n\n```yaml\napiVersion: scheduling.k8s.io/v1\nkind: PriorityClass\nmetadata:\n  name: high-priority\nvalue: 10000\nglobalDefault: false\ndescription: \"Priority class for high-priority workloads\"\n```\n\n### Specify pod priority using a PriorityClass\n\n{{< highlight yaml \"hl_lines=9\" >}}\napiVersion: v1\nkind: Pod\nmetadata:\n  name: nginx\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n  priorityClassName: high-priority\n{{< /highlight >}}\n\n### Built-in PriorityClasses\n\nKubernetes provides two built-in PriorityClasses:\n- `system-cluster-critical`: For system components that are critical to the cluster\n- `system-node-critical`: For system components that are critical to individual nodes. This is the highest priority that Pods can have in Kubernetes.\n\nFor more information, see [Pod Priority and Preemption](/docs/concepts/scheduling-eviction/pod-priority-preemption/).\n\n",
    "parent_id": null,
    "source_file": "data/kubernetes/content/en/docs/concepts/workloads/pods/advanced-pod-config.md",
    "section_path": [
      "concepts",
      "workloads",
      "pods"
    ],
    "chunk_type": "parent",
    "strategy": "recursive"
  },
  {
    "id": 156,
    "content": "## RuntimeClasses\n\nA _RuntimeClass_ allows you to specify the low-level container runtime for a Pod. It is useful when you want to specify different container runtimes for different kinds of Pod, such as when you need different isolation levels or runtime features.\n\n### Example Pod {#runtimeclass-pod-example}\n\n{{< highlight yaml \"hl_lines=6\" >}}\napiVersion: v1\nkind: Pod\nmetadata:\n  name: mypod\nspec:\n  runtimeClassName: myclass\n  containers:\n  - name: mycontainer\n    image: nginx\n{{< /highlight >}}\n\nA [RuntimeClass](/docs/concepts/containers/runtime-class/) is a cluster-scoped object that represents a container runtime that is available on some or all of your node.\n\nThe cluster administrator installs and configures the concrete runtimes backing the RuntimeClass.\n\nThey might set up that special container runtime configuration on all nodes, or perhaps just on some of them.\n\nFor more information, see the [RuntimeClass](/docs/concepts/containers/runtime-class/) documentation.\n\n",
    "parent_id": null,
    "source_file": "data/kubernetes/content/en/docs/concepts/workloads/pods/advanced-pod-config.md",
    "section_path": [
      "concepts",
      "workloads",
      "pods"
    ],
    "chunk_type": "parent",
    "strategy": "recursive"
  },
  {
    "id": 157,
    "content": "## Pod and container level security context configuration {#security-context}\n\nThe `Security context` field in the Pod specification provides granular control over security settings for Pods and containers.\n\n### Pod-wide `securityContext` {#pod-level-security-context}\n\nSome aspects of security apply to the whole Pod; for other aspects,\nyou might want to set a default, without any container-level overrides.\n\nHere's an example of using `securityContext` at the Pod level:\n\n#### Example Pod {#pod-level-security-context-example}\n\n{{< highlight yaml \"hl_lines=5-9\" >}}\napiVersion: v1\nkind: Pod\nmetadata:\n  name: security-context-demo\nspec:\n  securityContext:  # This applies to the entire Pod\n    runAsUser: 1000\n    runAsGroup: 3000\n    fsGroup: 2000\n  containers:\n  - name: sec-ctx-demo\n    image: registry.k8s.io/e2e-test-images/agnhost:2.45\n    command: [\"sh\", \"-c\", \"sleep 1h\"]\n{{< /highlight >}}\n\n### Container-level security context {#container-level-security-context}\n\nYou can specify the security context just for a specific container.\nHere's an example:\n\n#### Example Pod {#container-level-security-context-example}\n\n{{< highlight yaml \"hl_lines=9-17\" >}}\napiVersion: v1\nkind: Pod\nmetadata:\n  name: security-context-demo-2\nspec:\n  containers:\n  - name: sec-ctx-demo-2\n    image: gcr.io/google-samples/node-hello:1.0\n    securityContext:\n      allowPrivilegeEscalation: false\n      runAsNonRoot: true\n      runAsUser: 1000\n      capabilities:\n        drop:\n        - ALL\n      seccompProfile:\n        type: RuntimeDefault\n{{< /highlight >}}\n\n### Security context options\n\n- **User and Group IDs**: Control which user/group the container runs as\n- **Capabilities**: Add or drop Linux capabilities\n- **Seccomp Profiles**: Set security computing profiles\n- **SELinux Options**: Configure SELinux context\n- **AppArmor**: Configure AppArmor profiles for additional access control\n- **Windows Options**: Configure Windows-specific security settings\n\n{{< caution >}}\nYou can also use the Pod `securityContext` to allow\n[_privileged mode_](/docs/concepts/security/linux-kernel-security-constraints/#privileged-containers)\nin Linux containers. Privileged mode overrides many of the other security settings in the `securityContext`.\nAvoid using this setting unless you can't grant the equivalent permissions by using other fields in the `securityContext`.\nYou can run Windows containers in a similarly\nprivileged mode by setting the `windowsOptions.hostProcess` flag on the\nPod-level security context. For details and instructions, see\n[Create a Windows HostProcess Pod](/docs/tasks/configure-pod-container/create-hostprocess-pod/).\n{{< /caution >}}\n\nFor more information, see [Configure a Security Context for a Pod or Container](/docs/tasks/configure-pod-container/security-context/).\n\n",
    "parent_id": null,
    "source_file": "data/kubernetes/content/en/docs/concepts/workloads/pods/advanced-pod-config.md",
    "section_path": [
      "concepts",
      "workloads",
      "pods"
    ],
    "chunk_type": "parent",
    "strategy": "recursive"
  },
  {
    "id": 158,
    "content": "## Influencing Pod scheduling decisions {#scheduling}\n\nKubernetes provides several mechanisms to control which nodes your Pods are scheduled on.\n\n### Node selectors\n\nThe simplest form of node selection constraint:\n\n{{< highlight yaml \"hl_lines=9-11\" >}}\napiVersion: v1\nkind: Pod\nmetadata:\n  name: nginx\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n  nodeSelector:\n    disktype: ssd\n{{< /highlight >}}\n\n### Node affinity\n\nNode affinity allows you to specify rules that constrain which nodes your Pod can be scheduled on. Here's an example of a Pod that prefers running on nodes labelled as being on a particular continent, selecting based on the value of [`topology.kubernetes.io/zone`](/docs/reference/labels-annotations-taints/#topologykubernetesiozone) label.\n\n{{< highlight yaml \"hl_lines=6-15\" >}}\napiVersion: v1\nkind: Pod\nmetadata:\n  name: with-node-affinity\nspec:\n  affinity:\n    nodeAffinity:\n      requiredDuringSchedulingIgnoredDuringExecution:\n        nodeSelectorTerms:\n        - matchExpressions:\n          - key: topology.kubernetes.io/zone\n            operator: In\n            values:\n            - antarctica-east1\n            - antarctica-west1\n  containers:\n  - name: with-node-affinity\n    image: registry.k8s.io/pause:3.8\n{{< /highlight >}}\n\n### Pod affinity and anti-affinity\n\nIn addition to node affinity, you can also constrain which nodes a Pod can be scheduled on based on the labels of _other Pods_ that are already running on nodes. Pod affinity allows you to specify rules about where a Pod should be placed relative to other Pods.\n\n{{< highlight yaml \"hl_lines=6-15\" >}}\napiVersion: v1\nkind: Pod\nmetadata:\n  name: with-pod-affinity\nspec:\n  affinity:\n    podAffinity:\n      requiredDuringSchedulingIgnoredDuringExecution:\n      - labelSelector:\n          matchExpressions:\n          - key: app\n            operator: In\n            values:\n            - database\n        topologyKey: topology.kubernetes.io/zone\n  containers:\n  - name: with-pod-affinity\n    image: registry.k8s.io/pause:3.8\n{{< /highlight >}}\n\n### Tolerations\n\n_Tolerations_ allow Pods to be scheduled on nodes with matching taints:\n\n{{< highlight yaml \"hl_lines=9-13\" >}}\napiVersion: v1\nkind: Pod\nmetadata:\n  name: mypod\nspec:\n  containers:\n  - name: myapp\n    image: nginx\n  tolerations:\n  - key: \"key\"\n    operator: \"Equal\"\n    value: \"value\"\n    effect: \"NoSchedule\"\n{{< /highlight >}}\n\nFor more information, see [Assign Pods to Nodes](/docs/concepts/scheduling-eviction/assign-pod-node/).\n\n",
    "parent_id": null,
    "source_file": "data/kubernetes/content/en/docs/concepts/workloads/pods/advanced-pod-config.md",
    "section_path": [
      "concepts",
      "workloads",
      "pods"
    ],
    "chunk_type": "parent",
    "strategy": "recursive"
  },
  {
    "id": 159,
    "content": "## Pod overhead\n\nPod overhead allows you to account for the resources consumed by the Pod infrastructure on top of the container requests and limits.\n\n{{< highlight yaml \"hl_lines=7-10\" >}}\n---\napiVersion: node.k8s.io/v1\nkind: RuntimeClass\nmetadata:\n  name: kvisor-runtime\nhandler: kvisor-runtime\noverhead:\n  podFixed:\n    memory: \"2Gi\"\n    cpu: \"500m\"\n---\napiVersion: v1\nkind: Pod\nmetadata:\n  name: mypod\nspec:\n  runtimeClassName: kvisor-runtime\n  containers:\n  - name: myapp\n    image: nginx\n    resources:\n      requests:\n        memory: \"64Mi\"\n        cpu: \"250m\"\n      limits:\n        memory: \"128Mi\"\n        cpu: \"500m\"\n{{< /highlight >}}\n\n\n",
    "parent_id": null,
    "source_file": "data/kubernetes/content/en/docs/concepts/workloads/pods/advanced-pod-config.md",
    "section_path": [
      "concepts",
      "workloads",
      "pods"
    ],
    "chunk_type": "parent",
    "strategy": "recursive"
  },
  {
    "id": 160,
    "content": "## {{% heading \"whatsnext\" %}}\n\n* Read about [Pod Priority and Preemption](/docs/concepts/scheduling-eviction/pod-priority-preemption/)\n* Read about [RuntimeClasses](/docs/concepts/containers/runtime-class/)\n* Explore [Configure a Security Context for a Pod or Container](/docs/tasks/configure-pod-container/security-context/)\n* Learn how Kubernetes [assigns Pods to Nodes](/docs/concepts/scheduling-eviction/assign-pod-node/)\n* [Pod Overhead](/docs/concepts/scheduling-eviction/pod-overhead/)\n",
    "parent_id": null,
    "source_file": "data/kubernetes/content/en/docs/concepts/workloads/pods/advanced-pod-config.md",
    "section_path": [
      "concepts",
      "workloads",
      "pods"
    ],
    "chunk_type": "parent",
    "strategy": "recursive"
  },
  {
    "id": 200,
    "content": "## Voluntary and involuntary disruptions\n\nPods do not disappear until someone (a person or a controller) destroys them, or\nthere is an unavoidable hardware or system software error.\n\nWe call these unavoidable cases *involuntary disruptions* to\nan application.  Examples are:\n\n- a hardware failure of the physical machine backing the node\n- cluster administrator deletes VM (instance) by mistake\n- cloud provider or hypervisor failure makes VM disappear\n- a kernel panic\n- the node disappears from the cluster due to cluster network partition\n- eviction of a pod due to the node being [out-of-resources](/docs/concepts/scheduling-eviction/node-pressure-eviction/).\n\nExcept for the out-of-resources condition, all these conditions\nshould be familiar to most users; they are not specific\nto Kubernetes.\n\nWe call other cases *voluntary disruptions*.  These include both\nactions initiated by the application owner and those initiated by a Cluster\nAdministrator.  Typical application owner actions include:\n\n- deleting the deployment or other controller that manages the pod\n- updating a deployment's pod template causing a restart\n- directly deleting a pod (e.g. by accident)\n\nCluster administrator actions include:\n\n- [Draining a node](/docs/tasks/administer-cluster/safely-drain-node/) for repair or upgrade.\n- Draining a node from a cluster to scale the cluster down (learn about\n[Node Autoscaling](/docs/concepts/cluster-administration/node-autoscaling/)).\n- Removing a pod from a node to permit something else to fit on that node.\n\nThese actions might be taken directly by the cluster administrator, or by automation\nrun by the cluster administrator, or by your cluster hosting provider.\n\nAsk your cluster administrator or consult your cloud provider or distribution documentation\nto determine if any sources of voluntary disruptions are enabled for your cluster.\nIf none are enabled, you can skip creating Pod Disruption Budgets.\n\n{{< caution >}}\nNot all voluntary disruptions are constrained by Pod Disruption Budgets. For example,\ndeleting deployments or pods bypasses Pod Disruption Budgets.\n{{< /caution >}}\n\n",
    "parent_id": null,
    "source_file": "data/kubernetes/content/en/docs/concepts/workloads/pods/disruptions.md",
    "section_path": [
      "concepts",
      "workloads",
      "pods"
    ],
    "chunk_type": "parent",
    "strategy": "recursive"
  },
  {
    "id": 201,
    "content": "## Dealing with disruptions\n\nHere are some ways to mitigate involuntary disruptions:\n\n- Ensure your pod [requests the resources](/docs/tasks/configure-pod-container/assign-memory-resource) it needs.\n- Replicate your application if you need higher availability.  (Learn about running replicated\n  [stateless](/docs/tasks/run-application/run-stateless-application-deployment/)\n  and [stateful](/docs/tasks/run-application/run-replicated-stateful-application/) applications.)\n- For even higher availability when running replicated applications,\n  spread applications across racks (using\n  [anti-affinity](/docs/concepts/scheduling-eviction/assign-pod-node/#affinity-and-anti-affinity))\n  or across zones (if using a\n  [multi-zone cluster](/docs/setup/multiple-zones).)\n\nThe frequency of voluntary disruptions varies.  On a basic Kubernetes cluster, there are\nno automated voluntary disruptions (only user-triggered ones).  However, your cluster administrator or hosting provider\nmay run some additional services which cause voluntary disruptions. For example,\nrolling out node software updates can cause voluntary disruptions. Also, some implementations\nof cluster (node) autoscaling may cause voluntary disruptions to defragment and compact nodes.\nYour cluster administrator or hosting provider should have documented what level of voluntary\ndisruptions, if any, to expect. Certain configuration options, such as\n[using PriorityClasses](/docs/concepts/scheduling-eviction/pod-priority-preemption/)\nin your pod spec can also cause voluntary (and involuntary) disruptions.\n\n\n",
    "parent_id": null,
    "source_file": "data/kubernetes/content/en/docs/concepts/workloads/pods/disruptions.md",
    "section_path": [
      "concepts",
      "workloads",
      "pods"
    ],
    "chunk_type": "parent",
    "strategy": "recursive"
  },
  {
    "id": 202,
    "content": "## Pod disruption budgets\n\n{{< feature-state for_k8s_version=\"v1.21\" state=\"stable\" >}}\n\nKubernetes offers features to help you run highly available applications even when you\nintroduce frequent voluntary disruptions.\n\nAs an application owner, you can create a PodDisruptionBudget (PDB) for each application.\nA PDB limits the number of Pods of a replicated application that are down simultaneously from\nvoluntary disruptions. For example, a quorum-based application would\nlike to ensure that the number of replicas running is never brought below the\nnumber needed for a quorum. A web front end might want to\nensure that the number of replicas serving load never falls below a certain\npercentage of the total.\n\nCluster managers and hosting providers should use tools which\nrespect PodDisruptionBudgets by calling the [Eviction API](/docs/tasks/administer-cluster/safely-drain-node/#eviction-api)\ninstead of directly deleting pods or deployments.\n\nFor example, the `kubectl drain` subcommand lets you mark a node as going out of\nservice. When you run `kubectl drain`, the tool tries to evict all of the Pods on\nthe Node you're taking out of service. The eviction request that `kubectl` submits on\nyour behalf may be temporarily rejected, so the tool periodically retries all failed\nrequests until all Pods on the target node are terminated, or until a configurable timeout\nis reached.\n\nA PDB specifies the number of replicas that an application can tolerate having, relative to how\nmany it is intended to have.  For example, a Deployment which has a `.spec.replicas: 5` is\nsupposed to have 5 pods at any given time.  If its PDB allows for there to be 4 at a time,\nthen the Eviction API will allow voluntary disruption of one (but not two) pods at a time.\n\nThe group of pods that comprise the application is specified using a label selector, the same\nas the one used by the application's controller (deployment, stateful-set, etc).\n\nThe \"intended\" number of pods is computed from the `.spec.replicas` of the workload resource\nthat is managing those pods. The control plane discovers the owning workload resource by\nexamining the `.metadata.ownerReferences` of the Pod.\n\n[Involuntary disruptions](#voluntary-and-involuntary-disruptions) cannot be prevented by PDBs; however they\ndo count against the budget.\n\nPods which are deleted or unavailable due to a rolling upgrade to an application do count\nagainst the disruption budget, but workload resources (such as Deployment and StatefulSet)\nare not limited by PDBs when doing rolling upgrades. Instead, the handling of failures\nduring application updates is configured in the spec for the specific workload resource.\n\nIt is recommended to set `AlwaysAllow` [Unhealthy Pod Eviction Policy](/docs/tasks/run-application/configure-pdb/#unhealthy-pod-eviction-policy)\nto your PodDisruptionBudgets to support eviction of misbehaving applications during a node drain.\nThe default behavior is to wait for the application pods to become [healthy](/docs/tasks/run-application/configure-pdb/#healthiness-of-a-pod)\nbefore the drain can proceed.\n\nWhen a pod is evicted using the eviction API, it is gracefully\n[terminated](/docs/concepts/workloads/pods/pod-lifecycle/#pod-termination), honoring the\n`terminationGracePeriodSeconds` setting in its [PodSpec](/docs/reference/generated/kubernetes-api/{{< param \"version\" >}}/#podspec-v1-core).\n\n",
    "parent_id": null,
    "source_file": "data/kubernetes/content/en/docs/concepts/workloads/pods/disruptions.md",
    "section_path": [
      "concepts",
      "workloads",
      "pods"
    ],
    "chunk_type": "parent",
    "strategy": "recursive"
  },
  {
    "id": 203,
    "content": "## PodDisruptionBudget example {#pdb-example}\n\nConsider a cluster with 3 nodes, `node-1` through `node-3`.\nThe cluster is running several applications.  One of them has 3 replicas initially called\n`pod-a`, `pod-b`, and `pod-c`.  Another, unrelated pod without a PDB, called `pod-x`, is also shown.\nInitially, the pods are laid out as follows:\n\n|       node-1         |       node-2        |       node-3       |\n|:--------------------:|:-------------------:|:------------------:|\n| pod-a  *available*   | pod-b *available*   | pod-c *available*  |\n| pod-x  *available*   |                     |                    |\n\nAll 3 pods are part of a deployment, and they collectively have a PDB which requires\nthere be at least 2 of the 3 pods to be available at all times.\n\nFor example, assume the cluster administrator wants to reboot into a new kernel version to fix a bug in the kernel.\nThe cluster administrator first tries to drain `node-1` using the `kubectl drain` command.\nThat tool tries to evict `pod-a` and `pod-x`.  This succeeds immediately.\nBoth pods go into the `terminating` state at the same time.\nThis puts the cluster in this state:\n\n|   node-1 *draining*  |       node-2        |       node-3       |\n|:--------------------:|:-------------------:|:------------------:|\n| pod-a  *terminating* | pod-b *available*   | pod-c *available*  |\n| pod-x  *terminating* |                     |                    |\n\nThe deployment notices that one of the pods is terminating, so it creates a replacement\ncalled `pod-d`.  Since `node-1` is cordoned, it lands on another node.  Something has\nalso created `pod-y` as a replacement for `pod-x`.\n\n(Note: for a StatefulSet, `pod-a`, which would be called something like `pod-0`, would need\nto terminate completely before its replacement, which is also called `pod-0` but has a\ndifferent UID, could be created.  Otherwise, the example applies to a StatefulSet as well.)\n\nNow the cluster is in this state:\n\n|   node-1 *draining*  |       node-2        |       node-3       |\n|:--------------------:|:-------------------:|:------------------:|\n| pod-a  *terminating* | pod-b *available*   | pod-c *available*  |\n| pod-x  *terminating* | pod-d *starting*    | pod-y              |\n\nAt some point, the pods terminate, and the cluster looks like this:\n\n|    node-1 *drained*  |       node-2        |       node-3       |\n|:--------------------:|:-------------------:|:------------------:|\n|                      | pod-b *available*   | pod-c *available*  |\n|                      | pod-d *starting*    | pod-y              |\n\nAt this point, if an impatient cluster administrator tries to drain `node-2` or\n`node-3`, the drain command will block, because there are only 2 available\npods for the deployment, and its PDB requires at least 2.  After some time passes, `pod-d` becomes available.\n\nThe cluster state now looks like this:\n\n|    node-1 *drained*  |       node-2        |       node-3       |\n|:--------------------:|:-------------------:|:------------------:|\n|                      | pod-b *available*   | pod-c *available*  |\n|                      | pod-d *available*   | pod-y              |\n\nNow, the cluster administrator tries to drain `node-2`.\nThe drain command will try to evict the two pods in some order, say\n`pod-b` first and then `pod-d`.  It will succeed at evicting `pod-b`.\nBut, when it tries to evict `pod-d`, it will be refused because that would leave only\none pod available for the deployment.\n\nThe deployment creates a replacement for `pod-b` called `pod-e`.\nBecause there are not enough resources in the cluster to schedule\n`pod-e` the drain will again block.  The cluster may end up in this\nstate:\n\n|    node-1 *drained*  |       node-2        |       node-3       | *no node*          |\n|:--------------------:|:-------------------:|:------------------:|:------------------:|\n|                      | pod-b *terminating* | pod-c *available*  | pod-e *pending*    |\n|                      | pod-d *available*   | pod-y              |                    |\n\nAt this point, the cluster administrator needs to\nadd a node back to the cluster to proceed with the upgrade.\n\nYou can see how Kubernetes varies the rate at which disruptions\ncan happen, according to:\n\n- how many replicas an application needs\n- how long it takes to gracefully shutdown an instance\n- how long it takes a new instance to start up\n- the type of controller\n- the cluster's resource capacity\n\n",
    "parent_id": null,
    "source_file": "data/kubernetes/content/en/docs/concepts/workloads/pods/disruptions.md",
    "section_path": [
      "concepts",
      "workloads",
      "pods"
    ],
    "chunk_type": "parent",
    "strategy": "recursive"
  },
  {
    "id": 204,
    "content": "## Pod disruption conditions {#pod-disruption-conditions}\n\n{{< feature-state feature_gate_name=\"PodDisruptionConditions\" >}}\n\nA dedicated Pod `DisruptionTarget` [condition](/docs/concepts/workloads/pods/pod-lifecycle/#pod-conditions)\nis added to indicate\nthat the Pod is about to be deleted due to a {{<glossary_tooltip term_id=\"disruption\" text=\"disruption\">}}.\nThe `reason` field of the condition additionally\nindicates one of the following reasons for the Pod termination:\n\n`PreemptionByScheduler`\n: Pod is due to be {{<glossary_tooltip term_id=\"preemption\" text=\"preempted\">}} by a scheduler in order to accommodate a new Pod with a higher priority. For more information, see [Pod priority preemption](/docs/concepts/scheduling-eviction/pod-priority-preemption/).\n\n`DeletionByTaintManager`\n: Pod is due to be deleted by Taint Manager (which is part of the node lifecycle controller within `kube-controller-manager`) due to a `NoExecute` taint that the Pod does not tolerate; see {{<glossary_tooltip term_id=\"taint\" text=\"taint\">}}-based evictions.\n\n`EvictionByEvictionAPI`\n: Pod has been marked for {{<glossary_tooltip term_id=\"api-eviction\" text=\"eviction using the Kubernetes API\">}} .\n\n`DeletionByPodGC`\n: Pod, that is bound to a no longer existing Node, is due to be deleted by [Pod garbage collection](/docs/concepts/workloads/pods/pod-lifecycle/#pod-garbage-collection).\n\n`TerminationByKubelet`\n: Pod has been terminated by the kubelet, because of either {{<glossary_tooltip term_id=\"node-pressure-eviction\" text=\"node pressure eviction\">}},\n  the [graceful node shutdown](/docs/concepts/architecture/nodes/#graceful-node-shutdown),\n  or preemption for [system critical pods](/docs/tasks/administer-cluster/guaranteed-scheduling-critical-addon-pods/).\n\nIn all other disruption scenarios, like eviction due to exceeding\n[Pod container limits](/docs/concepts/configuration/manage-resources-containers/),\nPods don't receive the `DisruptionTarget` condition because the disruptions were\nprobably caused by the Pod and would reoccur on retry.\n\n{{< note >}}\nA Pod disruption might be interrupted. The control plane might re-attempt to\ncontinue the disruption of the same Pod, but it is not guaranteed. As a result,\nthe `DisruptionTarget` condition might be added to a Pod, but that Pod might then not actually be\ndeleted. In such a situation, after some time, the\nPod disruption condition will be cleared.\n{{< /note >}}\n\nAlong with cleaning up the pods, the Pod garbage collector (PodGC) will also mark them as failed if they are in a non-terminal\nphase (see also [Pod garbage collection](/docs/concepts/workloads/pods/pod-lifecycle/#pod-garbage-collection)).\n\nWhen using a Job (or CronJob), you may want to use these Pod disruption conditions as part of your Job's\n[Pod failure policy](/docs/concepts/workloads/controllers/job#pod-failure-policy).\n\n",
    "parent_id": null,
    "source_file": "data/kubernetes/content/en/docs/concepts/workloads/pods/disruptions.md",
    "section_path": [
      "concepts",
      "workloads",
      "pods"
    ],
    "chunk_type": "parent",
    "strategy": "recursive"
  },
  {
    "id": 205,
    "content": "## Separating Cluster Owner and Application Owner Roles\n\nOften, it is useful to think of the Cluster Manager\nand Application Owner as separate roles with limited knowledge\nof each other.   This separation of responsibilities\nmay make sense in these scenarios:\n\n- when there are many application teams sharing a Kubernetes cluster, and\n  there is natural specialization of roles\n- when third-party tools or services are used to automate cluster management\n\nPod Disruption Budgets support this separation of roles by providing an\ninterface between the roles.\n\nIf you do not have such a separation of responsibilities in your organization,\nyou may not need to use Pod Disruption Budgets.\n\n",
    "parent_id": null,
    "source_file": "data/kubernetes/content/en/docs/concepts/workloads/pods/disruptions.md",
    "section_path": [
      "concepts",
      "workloads",
      "pods"
    ],
    "chunk_type": "parent",
    "strategy": "recursive"
  },
  {
    "id": 206,
    "content": "## How to perform Disruptive Actions on your Cluster\n\nIf you are a Cluster Administrator, and you need to perform a disruptive action on all\nthe nodes in your cluster, such as a node or system software upgrade, here are some options:\n\n- Accept downtime during the upgrade.\n- Failover to another complete replica cluster.\n   -  No downtime, but may be costly both for the duplicated nodes\n     and for human effort to orchestrate the switchover.\n- Write disruption tolerant applications and use PDBs.\n   - No downtime.\n   - Minimal resource duplication.\n   - Allows more automation of cluster administration.\n   - Writing disruption-tolerant applications is tricky, but the work to tolerate voluntary\n     disruptions largely overlaps with work to support autoscaling and tolerating\n     involuntary disruptions.\n\n\n\n\n",
    "parent_id": null,
    "source_file": "data/kubernetes/content/en/docs/concepts/workloads/pods/disruptions.md",
    "section_path": [
      "concepts",
      "workloads",
      "pods"
    ],
    "chunk_type": "parent",
    "strategy": "recursive"
  },
  {
    "id": 207,
    "content": "## {{% heading \"whatsnext\" %}}\n\n\n* Follow steps to protect your application by [configuring a Pod Disruption Budget](/docs/tasks/run-application/configure-pdb/).\n\n* Learn more about [draining nodes](/docs/tasks/administer-cluster/safely-drain-node/)\n\n* Learn about [updating a deployment](/docs/concepts/workloads/controllers/deployment/#updating-a-deployment)\n  including steps to maintain its availability during the rollout.\n\n",
    "parent_id": null,
    "source_file": "data/kubernetes/content/en/docs/concepts/workloads/pods/disruptions.md",
    "section_path": [
      "concepts",
      "workloads",
      "pods"
    ],
    "chunk_type": "parent",
    "strategy": "recursive"
  },
  {
    "id": 289,
    "content": "## Available fields\n\nOnly some Kubernetes API fields are available through the downward API. This\nsection lists which fields you can make available.\n\nYou can pass information from available Pod-level fields using `fieldRef`.\nAt the API level, the `spec` for a Pod always defines at least one\n[Container](/docs/reference/kubernetes-api/workload-resources/pod-v1/#Container).\nYou can pass information from available Container-level fields using\n`resourceFieldRef`.\n\n### Information available via `fieldRef` {#downwardapi-fieldRef}\n\nFor some Pod-level fields, you can provide them to a container either as\nan environment variable or using a `downwardAPI` volume. The fields available\nvia either mechanism are:\n\n`metadata.name`\n: the pod's name\n\n`metadata.namespace`\n: the pod's {{< glossary_tooltip text=\"namespace\" term_id=\"namespace\" >}}\n\n`metadata.uid`\n: the pod's unique ID\n\n`metadata.annotations['<KEY>']`\n: the value of the pod's {{< glossary_tooltip text=\"annotation\" term_id=\"annotation\" >}} named `<KEY>` (for example, `metadata.annotations['myannotation']`)\n\n`metadata.labels['<KEY>']`\n: the text value of the pod's {{< glossary_tooltip text=\"label\" term_id=\"label\" >}} named `<KEY>` (for example, `metadata.labels['mylabel']`)\n\nThe following information is available through environment variables\n**but not as a downwardAPI volume fieldRef**:\n\n`spec.serviceAccountName`\n: the name of the pod's {{< glossary_tooltip text=\"service account\" term_id=\"service-account\" >}}\n\n`spec.nodeName`\n: the name of the {{< glossary_tooltip term_id=\"node\" text=\"node\">}} where the Pod is executing\n\n`status.hostIP`\n: the primary IP address of the node to which the Pod is assigned\n\n`status.hostIPs`\n: the IP addresses is a dual-stack version of `status.hostIP`, the first is always the same as `status.hostIP`.\n\n`status.podIP`\n: the pod's primary IP address (usually, its IPv4 address)\n\n`status.podIPs`\n: the IP addresses is a dual-stack version of `status.podIP`, the first is always the same as `status.podIP`\n\nThe following information is available through a `downwardAPI` volume \n`fieldRef`, **but not as environment variables**:\n\n`metadata.labels`\n: all of the pod's labels, formatted as `label-key=\"escaped-label-value\"` with one label per line\n\n`metadata.annotations`\n: all of the pod's annotations, formatted as `annotation-key=\"escaped-annotation-value\"` with one annotation per line  \n\n### Information available via `resourceFieldRef` {#downwardapi-resourceFieldRef}\n\nThese container-level fields allow you to provide information about\n[requests and limits](/docs/concepts/configuration/manage-resources-containers/#requests-and-limits)\nfor resources such as CPU and memory.\n\n{{< note >}}\n{{< feature-state feature_gate_name=\"InPlacePodVerticalScaling\" >}}\nContainer CPU and memory resources can be resized while the container is running.\nIf this happens, a downward API volume will be updated,\nbut environment variables will not be updated unless the container restarts.\nSee [Resize CPU and Memory Resources assigned to Containers](/docs/tasks/configure-pod-container/resize-container-resources/)\nfor more details.\n{{< /note >}}\n\n\n`resource: limits.cpu`\n: A container's CPU limit\n\n`resource: requests.cpu`\n: A container's CPU request\n\n`resource: limits.memory`\n: A container's memory limit\n\n`resource: requests.memory`\n: A container's memory request\n\n`resource: limits.hugepages-*`\n: A container's hugepages limit\n\n`resource: requests.hugepages-*`\n: A container's hugepages request\n\n`resource: limits.ephemeral-storage`\n: A container's ephemeral-storage limit\n\n`resource: requests.ephemeral-storage`\n: A container's ephemeral-storage request\n\n#### Fallback information for resource limits\n\nIf CPU and memory limits are not specified for a container, and you use the\ndownward API to try to expose that information, then the\nkubelet defaults to exposing the maximum allocatable value for CPU and memory\nbased on the [node allocatable](/docs/tasks/administer-cluster/reserve-compute-resources/#node-allocatable)\ncalculation.\n\n",
    "parent_id": null,
    "source_file": "data/kubernetes/content/en/docs/concepts/workloads/pods/downward-api.md",
    "section_path": [
      "concepts",
      "workloads",
      "pods"
    ],
    "chunk_type": "parent",
    "strategy": "recursive"
  },
  {
    "id": 290,
    "content": "## {{% heading \"whatsnext\" %}}\n\nYou can read about [`downwardAPI` volumes](/docs/concepts/storage/volumes/#downwardapi).\n\nYou can try using the downward API to expose container- or Pod-level information:\n* as [environment variables](/docs/tasks/inject-data-application/environment-variable-expose-pod-information/)\n* as [files in `downwardAPI` volume](/docs/tasks/inject-data-application/downward-api-volume-expose-pod-information/)\n",
    "parent_id": null,
    "source_file": "data/kubernetes/content/en/docs/concepts/workloads/pods/downward-api.md",
    "section_path": [
      "concepts",
      "workloads",
      "pods"
    ],
    "chunk_type": "parent",
    "strategy": "recursive"
  },
  {
    "id": 315,
    "content": "## Understanding ephemeral containers\n\n{{< glossary_tooltip text=\"Pods\" term_id=\"pod\" >}} are the fundamental building\nblock of Kubernetes applications. Since Pods are intended to be disposable and\nreplaceable, you cannot add a container to a Pod once it has been created.\nInstead, you usually delete and replace Pods in a controlled fashion using\n{{< glossary_tooltip text=\"deployments\" term_id=\"deployment\" >}}.\n\nSometimes it's necessary to inspect the state of an existing Pod, however, for\nexample to troubleshoot a hard-to-reproduce bug. In these cases you can run\nan ephemeral container in an existing Pod to inspect its state and run\narbitrary commands.\n\n### What is an ephemeral container?\n\nEphemeral containers differ from other containers in that they lack guarantees\nfor resources or execution, and they will never be automatically restarted, so\nthey are not appropriate for building applications.  Ephemeral containers are\ndescribed using the same `ContainerSpec` as regular containers, but many fields\nare incompatible and disallowed for ephemeral containers.\n\n- Ephemeral containers may not have ports, so fields such as `ports`,\n  `livenessProbe`, `readinessProbe` are disallowed.\n- Pod resource allocations are immutable, so setting `resources` is disallowed.\n- For a complete list of allowed fields, see the [EphemeralContainer reference\n  documentation](/docs/reference/generated/kubernetes-api/{{< param \"version\" >}}/#ephemeralcontainer-v1-core).\n\nEphemeral containers are created using a special `ephemeralcontainers` handler\nin the API rather than by adding them directly to `pod.spec`, so it's not\npossible to add an ephemeral container using `kubectl edit`.\n\nLike regular containers, you may not change or remove an ephemeral container\nafter you have added it to a Pod.\n\n{{< note >}}\nEphemeral containers are not supported by [static pods](/docs/tasks/configure-pod-container/static-pod/).\n{{< /note >}}\n\n",
    "parent_id": null,
    "source_file": "data/kubernetes/content/en/docs/concepts/workloads/pods/ephemeral-containers.md",
    "section_path": [
      "concepts",
      "workloads",
      "pods"
    ],
    "chunk_type": "parent",
    "strategy": "recursive"
  },
  {
    "id": 316,
    "content": "## Uses for ephemeral containers\n\nEphemeral containers are useful for interactive troubleshooting when `kubectl\nexec` is insufficient because a container has crashed or a container image\ndoesn't include debugging utilities.\n\nIn particular, [distroless images](https://github.com/GoogleContainerTools/distroless)\nenable you to deploy minimal container images that reduce attack surface\nand exposure to bugs and vulnerabilities. Since distroless images do not include a\nshell or any debugging utilities, it's difficult to troubleshoot distroless\nimages using `kubectl exec` alone.\n\nWhen using ephemeral containers, it's helpful to enable [process namespace\nsharing](/docs/tasks/configure-pod-container/share-process-namespace/) so\nyou can view processes in other containers.\n\n",
    "parent_id": null,
    "source_file": "data/kubernetes/content/en/docs/concepts/workloads/pods/ephemeral-containers.md",
    "section_path": [
      "concepts",
      "workloads",
      "pods"
    ],
    "chunk_type": "parent",
    "strategy": "recursive"
  },
  {
    "id": 317,
    "content": "## {{% heading \"whatsnext\" %}}\n\n* Learn how to [debug pods using ephemeral containers](/docs/tasks/debug/debug-application/debug-running-pod/#ephemeral-container).\n\n",
    "parent_id": null,
    "source_file": "data/kubernetes/content/en/docs/concepts/workloads/pods/ephemeral-containers.md",
    "section_path": [
      "concepts",
      "workloads",
      "pods"
    ],
    "chunk_type": "parent",
    "strategy": "recursive"
  },
  {
    "id": 337,
    "content": "## Understanding init containers\n\nA {{< glossary_tooltip text=\"Pod\" term_id=\"pod\" >}} can have multiple containers\nrunning apps within it, but it can also have one or more init containers, which are run\nbefore the app containers are started.\n\nInit containers are exactly like regular containers, except:\n\n* Init containers always run to completion.\n* Each init container must complete successfully before the next one starts.\n\nIf a Pod's init container fails, the kubelet repeatedly restarts that init container until it succeeds.\nHowever, if the Pod has a `restartPolicy` of Never, and an init container fails during startup of that Pod, Kubernetes treats the overall Pod as failed.\n\nTo specify an init container for a Pod, add the `initContainers` field into\nthe [Pod specification](/docs/reference/kubernetes-api/workload-resources/pod-v1/#PodSpec),\nas an array of `container` items (similar to the app `containers` field and its contents).\nSee [Container](/docs/reference/kubernetes-api/workload-resources/pod-v1/#Container) in the\nAPI reference for more details.\n\nThe status of the init containers is returned in `.status.initContainerStatuses`\nfield as an array of the container statuses (similar to the `.status.containerStatuses`\nfield).\n\n### Differences from regular containers\n\nInit containers support all the fields and features of app containers,\nincluding resource limits, [volumes](/docs/concepts/storage/volumes/), and security settings. However, the\nresource requests and limits for an init container are handled differently,\nas documented in [Resource sharing within containers](#resource-sharing-within-containers).\n\nRegular init containers (in other words: excluding sidecar containers) do not support the\n`lifecycle`, `livenessProbe`, `readinessProbe`, or `startupProbe` fields. Init containers\nmust run to completion before the Pod can be ready; sidecar containers continue running\nduring a Pod's lifetime, and _do_ support some probes. See [sidecar container](/docs/concepts/workloads/pods/sidecar-containers/)\nfor further details about sidecar containers.\n\nIf you specify multiple init containers for a Pod, kubelet runs each init\ncontainer sequentially. Each init container must succeed before the next can run.\nWhen all of the init containers have run to completion, kubelet initializes\nthe application containers for the Pod and runs them as usual.\n\n### Differences from sidecar containers\n\nInit containers run and complete their tasks before the main application container starts.\nUnlike [sidecar containers](/docs/concepts/workloads/pods/sidecar-containers),\ninit containers are not continuously running alongside the main containers.\n\nInit containers run to completion sequentially, and the main container does not start\nuntil all the init containers have successfully completed.\n\ninit containers do not support `lifecycle`, `livenessProbe`, `readinessProbe`, or\n`startupProbe` whereas sidecar containers support all these [probes](/docs/concepts/workloads/pods/pod-lifecycle/#types-of-probe) to control their lifecycle.\n\nInit containers share the same resources (CPU, memory, network) with the main application\ncontainers but do not interact directly with them. They can, however, use shared volumes\nfor data exchange.\n\n",
    "parent_id": null,
    "source_file": "data/kubernetes/content/en/docs/concepts/workloads/pods/init-containers.md",
    "section_path": [
      "concepts",
      "workloads",
      "pods"
    ],
    "chunk_type": "parent",
    "strategy": "recursive"
  },
  {
    "id": 338,
    "content": "## Using init containers\n\nBecause init containers have separate images from app containers, they\nhave some advantages for start-up related code:\n\n* Init containers can contain utilities or custom code for setup that are not present in an app\n  image. For example, there is no need to make an image `FROM` another image just to use a tool like\n  `sed`, `awk`, `python`, or `dig` during setup.\n* The application image builder and deployer roles can work independently without\n  the need to jointly build a single app image.\n* Init containers can run with a different view of the filesystem than app containers in the\n  same Pod. Consequently, they can be given access to\n  {{< glossary_tooltip text=\"Secrets\" term_id=\"secret\" >}} that app containers cannot access.\n* Because init containers run to completion before any app containers start, init containers offer\n  a mechanism to block or delay app container startup until a set of preconditions are met. Once\n  preconditions are met, all of the app containers in a Pod can start in parallel.\n* Init containers can securely run utilities or custom code that would otherwise make an app\n  container image less secure. By keeping unnecessary tools separate you can limit the attack\n  surface of your app container image.\n\n\n### Examples\nHere are some ideas for how to use init containers:\n\n* Wait for a {{< glossary_tooltip text=\"Service\" term_id=\"service\">}} to\n  be created, using a shell one-line command like:\n  ```shell\n  for i in {1..100}; do sleep 1; if nslookup myservice; then exit 0; fi; done; exit 1\n  ```\n\n* Register this Pod with a remote server from the downward API with a command like:\n  ```shell\n  curl -X POST http://$MANAGEMENT_SERVICE_HOST:$MANAGEMENT_SERVICE_PORT/register -d 'instance=$(<POD_NAME>)&ip=$(<POD_IP>)'\n  ```\n\n* Wait for some time before starting the app container with a command like\n  ```shell\n  sleep 60\n  ```\n\n* Clone a Git repository into a {{< glossary_tooltip text=\"Volume\" term_id=\"volume\" >}}\n\n* Place values into a configuration file and run a template tool to dynamically\n  generate a configuration file for the main app container. For example,\n  place the `POD_IP` value in a configuration and generate the main app\n  configuration file using Jinja.\n\n#### Init containers in use\n\nThis example defines a simple Pod that has two init containers.\nThe first waits for `myservice`, and the second waits for `mydb`. Once both\ninit containers complete, the Pod runs the app container from its `spec` section.\n\n```yaml\napiVersion: v1\nkind: Pod\nmetadata:\n  name: myapp-pod\n  labels:\n    app.kubernetes.io/name: MyApp\nspec:\n  containers:\n  - name: myapp-container\n    image: busybox:1.28\n    command: ['sh', '-c', 'echo The app is running! && sleep 3600']\n  initContainers:\n  - name: init-myservice\n    image: busybox:1.28\n    command: ['sh', '-c', \"until nslookup myservice.$(cat /var/run/secrets/kubernetes.io/serviceaccount/namespace).svc.cluster.local; do echo waiting for myservice; sleep 2; done\"]\n  - name: init-mydb\n    image: busybox:1.28\n    command: ['sh', '-c', \"until nslookup mydb.$(cat /var/run/secrets/kubernetes.io/serviceaccount/namespace).svc.cluster.local; do echo waiting for mydb; sleep 2; done\"]\n```\n\nYou can start this Pod by running:\n\n```shell\nkubectl apply -f myapp.yaml\n```\nThe output is similar to this:\n```\npod/myapp-pod created\n```\n\nAnd check on its status with:\n```shell\nkubectl get -f myapp.yaml\n```\nThe output is similar to this:\n```\nNAME        READY     STATUS     RESTARTS   AGE\nmyapp-pod   0/1       Init:0/2   0          6m\n```\n\nor for more details:\n```shell\nkubectl describe -f myapp.yaml\n```\nThe output is similar to this:\n```\nName:          myapp-pod\nNamespace:     default\n[...]\nLabels:        app.kubernetes.io/name=MyApp\nStatus:        Pending\n[...]\nInit Containers:\n  init-myservice:\n[...]\n    State:         Running\n[...]\n  init-mydb:\n[...]\n    State:         Waiting\n      Reason:      PodInitializing\n    Ready:         False\n[...]\nContainers:\n  myapp-container:\n[...]\n    State:         Waiting\n      Reason:      PodInitializing\n    Ready:         False\n[...]\nEvents:\n  FirstSeen    LastSeen    Count    From                      SubObjectPath                           Type          Reason        Message\n  ---------    --------    -----    ----                      -------------                           --------      ------        -------\n  16s          16s         1        {default-scheduler }                                              Normal        Scheduled     Successfully assigned myapp-pod to 172.17.4.201\n  16s          16s         1        {kubelet 172.17.4.201}    spec.initContainers{init-myservice}     Normal        Pulling       pulling image \"busybox\"\n  13s          13s         1        {kubelet 172.17.4.201}    spec.initContainers{init-myservice}     Normal        Pulled        Successfully pulled image \"busybox\"\n  13s          13s         1        {kubelet 172.17.4.201}    spec.initContainers{init-myservice}     Normal        Created       Created container init-myservice\n  13s          13s         1        {kubelet 172.17.4.201}    spec.initContainers{init-myservice}     Normal        Started       Started container init-myservice\n```\n\nTo see logs for the init containers in this Pod, run:\n```shell\nkubectl logs myapp-pod -c init-myservice # Inspect the first init container\nkubectl logs myapp-pod -c init-mydb      # Inspect the second init container\n```\n\nAt this point, those init containers will be waiting to discover {{< glossary_tooltip text=\"Services\" term_id=\"service\" >}} named\n`mydb` and `myservice`.\n\nHere's a configuration you can use to make those Services appear:\n\n```yaml\n---\napiVersion: v1\nkind: Service\nmetadata:\n  name: myservice\nspec:\n  ports:\n  - protocol: TCP\n    port: 80\n    targetPort: 9376\n---\napiVersion: v1\nkind: Service\nmetadata:\n  name: mydb\nspec:\n  ports:\n  - protocol: TCP\n    port: 80\n    targetPort: 9377\n```\n\nTo create the `mydb` and `myservice` services:\n\n```shell\nkubectl apply -f services.yaml\n```\nThe output is similar to this:\n```\nservice/myservice created\nservice/mydb created\n```\n\nYou'll then see that those init containers complete, and that the `myapp-pod`\nPod moves into the Running state:\n\n```shell\nkubectl get -f myapp.yaml\n```\nThe output is similar to this:\n```\nNAME        READY     STATUS    RESTARTS   AGE\nmyapp-pod   1/1       Running   0          9m\n```\n\nThis simple example should provide some inspiration for you to create your own\ninit containers. [What's next](#what-s-next) contains a link to a more detailed example.\n\n",
    "parent_id": null,
    "source_file": "data/kubernetes/content/en/docs/concepts/workloads/pods/init-containers.md",
    "section_path": [
      "concepts",
      "workloads",
      "pods"
    ],
    "chunk_type": "parent",
    "strategy": "recursive"
  },
  {
    "id": 339,
    "content": "## Detailed behavior\n\nDuring Pod startup, the kubelet delays running init containers until the networking\nand storage are ready. Then the kubelet runs the Pod's init containers in the order\nthey appear in the Pod's spec.\n\nEach init container must exit successfully before\nthe next container starts. If a container fails to start due to the runtime or\nexits with failure, it is retried according to the Pod `restartPolicy`. However,\nif the Pod `restartPolicy` is set to Always, the init containers use\n`restartPolicy` OnFailure.\n\nA Pod cannot be `Ready` until all init containers have succeeded. The ports on an\ninit container are not aggregated under a Service. A Pod that is initializing\nis in the `Pending` state but should have a condition `Initialized` set to false.\n\nIf the Pod [restarts](#pod-restart-reasons), or is restarted, all init containers\nmust execute again.\n\nChanges to the init container spec are limited to the container image field.\nDirectly altering the `image` field of  an init container does _not_ restart the\nPod or trigger its recreation. If the Pod has yet to start, that change may\nhave an effect on how the Pod boots up.\n\nFor a [pod template](/docs/concepts/workloads/pods/#pod-templates)\nyou can typically change any field for an init container; the impact of making\nthat change depends on where the pod template is used.\n\nBecause init containers can be restarted, retried, or re-executed, init container\ncode should be idempotent. In particular, code that writes into any `emptyDir` volume\nshould be prepared for the possibility that an output file already exists.\n\nInit containers have all of the fields of an app container. However, Kubernetes\nprohibits `readinessProbe` from being used because init containers cannot\ndefine readiness distinct from completion. This is enforced during validation.\n\nUse `activeDeadlineSeconds` on the Pod to prevent init containers from failing forever.\nThe active deadline includes init containers.\nHowever it is recommended to use `activeDeadlineSeconds` only if teams deploy their application\nas a Job, because `activeDeadlineSeconds` has an effect even after initContainer finished.\nThe Pod which is already running correctly would be killed by `activeDeadlineSeconds` if you set.\n\nThe name of each app and init container in a Pod must be unique; a\nvalidation error is thrown for any container sharing a name with another.\n\n### Resource sharing within containers\n\nGiven the order of execution for init, sidecar and app containers, the following rules\nfor resource usage apply:\n\n* The highest of any particular resource request or limit defined on all init\n  containers is the *effective init request/limit*. If any resource has no\n  resource limit specified this is considered as the highest limit.\n* The Pod's *effective request/limit* for a resource is the higher of:\n  * the sum of all app containers request/limit for a resource\n  * the effective init request/limit for a resource\n* Scheduling is done based on effective requests/limits, which means\n  init containers can reserve resources for initialization that are not used\n  during the life of the Pod.\n* The QoS (quality of service) tier of the Pod's *effective QoS tier* is the\n  QoS tier for init containers and app containers alike.\n\nQuota and limits are applied based on the effective Pod request and\nlimit.\n\n### Init containers and Linux cgroups {#cgroups}\n\nOn Linux, resource allocations for Pod level control groups (cgroups) are based on the effective Pod\nrequest and limit, the same as the scheduler.\n\n{{< comment >}}\nThis section also present under [sidecar containers](/docs/concepts/workloads/pods/sidecar-containers/) page.\nIf you're editing this section, change both places.\n{{< /comment >}}\n\n### Pod restart reasons\n\nA Pod can restart, causing re-execution of init containers, for the following\nreasons:\n\n* The Pod infrastructure container is restarted. This is uncommon and would\n  have to be done by someone with root access to nodes.\n* All containers in a Pod are terminated while `restartPolicy` is set to Always,\n  forcing a restart, and the init container completion record has been lost due\n  to {{< glossary_tooltip text=\"garbage collection\" term_id=\"garbage-collection\" >}}.\n\nThe Pod will not be restarted when the init container image is changed, or the\ninit container completion record has been lost due to garbage collection. This\napplies for Kubernetes v1.20 and later. If you are using an earlier version of\nKubernetes, consult the documentation for the version you are using.\n\n",
    "parent_id": null,
    "source_file": "data/kubernetes/content/en/docs/concepts/workloads/pods/init-containers.md",
    "section_path": [
      "concepts",
      "workloads",
      "pods"
    ],
    "chunk_type": "parent",
    "strategy": "recursive"
  },
  {
    "id": 340,
    "content": "## {{% heading \"whatsnext\" %}}\n\nLearn more about the following:\n* [Creating a Pod that has an init container](/docs/tasks/configure-pod-container/configure-pod-initialization/#create-a-pod-that-has-an-init-container).\n* [Debug init containers](/docs/tasks/debug/debug-application/debug-init-containers/).\n* Overview of [kubelet](/docs/reference/command-line-tools-reference/kubelet/) and [kubectl](/docs/reference/kubectl/).\n* [Types of probes](/docs/concepts/workloads/pods/pod-lifecycle/#types-of-probe): liveness, readiness, startup probe.\n* [Sidecar containers](/docs/concepts/workloads/pods/sidecar-containers).\n",
    "parent_id": null,
    "source_file": "data/kubernetes/content/en/docs/concepts/workloads/pods/init-containers.md",
    "section_path": [
      "concepts",
      "workloads",
      "pods"
    ],
    "chunk_type": "parent",
    "strategy": "recursive"
  },
  {
    "id": 424,
    "content": "## Default Pod hostname\n\nWhen a Pod is created, its hostname (as observed from within the Pod) \nis derived from the Pod's metadata.name value. \nBoth the hostname and its corresponding fully qualified domain name (FQDN) \nare set to the metadata.name value (from the Pod's perspective)\n\n```yaml\napiVersion: v1\nkind: Pod\nmetadata:\n  name: busybox-1\nspec:\n  containers:\n  - image: busybox:1.28\n    command:\n      - sleep\n      - \"3600\"\n    name: busybox\n```\nThe Pod created by this manifest will have its hostname and fully qualified domain name (FQDN) set to `busybox-1`.\n\n",
    "parent_id": null,
    "source_file": "data/kubernetes/content/en/docs/concepts/workloads/pods/pod-hostname.md",
    "section_path": [
      "concepts",
      "workloads",
      "pods"
    ],
    "chunk_type": "parent",
    "strategy": "recursive"
  },
  {
    "id": 425,
    "content": "## Hostname with pod's hostname and subdomain fields\nThe Pod spec includes an optional `hostname` field. \nWhen set, this value takes precedence over the Pod's `metadata.name` as the \nhostname (observed from within the Pod).\nFor example, a Pod with spec.hostname set to `my-host` will have its hostname set to `my-host`.\n\nThe Pod spec also includes an optional `subdomain` field, \nindicating the Pod belongs to a subdomain within its namespace. \nIf a Pod has `spec.hostname` set to \"foo\" and spec.subdomain set \nto \"bar\" in the namespace `my-namespace`, its hostname becomes `foo` and its \nfully qualified domain name (FQDN) becomes \n`foo.bar.my-namespace.svc.cluster-domain.example` (observed from within the Pod).\n\nWhen both hostname and subdomain are set, the cluster's DNS server will \ncreate A and/or AAAA records based on these fields. \nRefer to: [Pod's hostname and subdomain fields](/docs/concepts/services-networking/dns-pod-service/#pod-hostname-and-subdomain-field).\n\n",
    "parent_id": null,
    "source_file": "data/kubernetes/content/en/docs/concepts/workloads/pods/pod-hostname.md",
    "section_path": [
      "concepts",
      "workloads",
      "pods"
    ],
    "chunk_type": "parent",
    "strategy": "recursive"
  },
  {
    "id": 426,
    "content": "## Hostname with pod's setHostnameAsFQDN fields\n\n{{< feature-state for_k8s_version=\"v1.22\" state=\"stable\" >}}\n\nWhen a Pod is configured to have fully qualified domain name (FQDN), its\nhostname is the short hostname. For example, if you have a Pod with the fully\nqualified domain name `busybox-1.busybox-subdomain.my-namespace.svc.cluster-domain.example`,\nthen by default the `hostname` command inside that Pod returns `busybox-1` and the\n`hostname --fqdn` command returns the FQDN.\n\nWhen both `setHostnameAsFQDN: true` and the subdomain field is set in the Pod spec,\nthe kubelet writes the Pod's FQDN\ninto the hostname for that Pod's namespace. In this case, both `hostname` and `hostname --fqdn`\nreturn the Pod's FQDN.\n\nThe Pod's FQDN is constructed in the same manner as previously defined.\nIt is composed of the Pod's `spec.hostname` (if specified) or `metadata.name` field,\nthe `spec.subdomain`, the `namespace` name, and the cluster domain suffix.\n\n{{< note >}}\nIn Linux, the hostname field of the kernel (the `nodename` field of `struct utsname`) is limited to 64 characters.\n\nIf a Pod enables this feature and its FQDN is longer than 64 character, it will fail to start.\nThe Pod will remain in `Pending` status (`ContainerCreating` as seen by `kubectl`) generating\nerror events, such as \"Failed to construct FQDN from Pod hostname and cluster domain\".\n\nThis means that when using this field, \nyou must ensure the combined length of the Pod's `metadata.name` (or `spec.hostname`) \nand `spec.subdomain` fields results in an FQDN that does not exceed 64 characters.\n{{< /note >}}\n\n",
    "parent_id": null,
    "source_file": "data/kubernetes/content/en/docs/concepts/workloads/pods/pod-hostname.md",
    "section_path": [
      "concepts",
      "workloads",
      "pods"
    ],
    "chunk_type": "parent",
    "strategy": "recursive"
  },
  {
    "id": 427,
    "content": "## Hostname with pod's hostnameOverride\n{{< feature-state feature_gate_name=\"HostnameOverride\" >}}\n\nSetting a value for `hostnameOverride` in the Pod spec causes the kubelet \nto unconditionally set both the Pod's hostname and fully qualified domain name (FQDN)\nto the `hostnameOverride` value. \n\nThe `hostnameOverride` field has a length limitation of 64 characters \nand must adhere to the DNS subdomain names standard defined in [RFC 1123](https://datatracker.ietf.org/doc/html/rfc1123).\n\nExample:\n```yaml\napiVersion: v1\nkind: Pod\nmetadata:\n  name: busybox-2-busybox-example-domain\nspec:\n  hostnameOverride: busybox-2.busybox.example.domain\n  containers:\n  - image: busybox:1.28\n    command:\n      - sleep\n      - \"3600\"\n    name: busybox\n```\n{{< note >}}\nThis only affects the hostname within the Pod; it does not affect the Pod's A or AAAA records in the cluster DNS server.\n{{< /note >}}\n\nIf `hostnameOverride` is set alongside `hostname` and `subdomain` fields:\n* The hostname inside the Pod is overridden to the `hostnameOverride` value.\n  \n* The Pod's A and/or AAAA records in the cluster DNS server are still generated based on the `hostname` and `subdomain` fields.\n\nNote: If `hostnameOverride` is set, you cannot simultaneously set the `hostNetwork` and `setHostnameAsFQDN` fields.\nThe API server will explicitly reject any create request attempting this combination.\n\nFor details on behavior when `hostnameOverride` is set in combination with \nother fields (hostname, subdomain, setHostnameAsFQDN, hostNetwork), \nsee the table in the [KEP-4762 design details](https://github.com/kubernetes/enhancements/blob/master/keps/sig-network/4762-allow-arbitrary-fqdn-as-pod-hostname/README.md#design-details ).",
    "parent_id": null,
    "source_file": "data/kubernetes/content/en/docs/concepts/workloads/pods/pod-hostname.md",
    "section_path": [
      "concepts",
      "workloads",
      "pods"
    ],
    "chunk_type": "parent",
    "strategy": "recursive"
  },
  {
    "id": 458,
    "content": "## Pod lifetime\n\nWhilst a Pod is running, the kubelet is able to restart containers to handle some\nkind of faults. Within a Pod, Kubernetes tracks different container\n[states](#container-states) and determines what action to take to make the Pod\nhealthy again.\n\nIn the Kubernetes API, Pods have both a specification and an actual status. The\nstatus for a Pod object consists of a set of [Pod conditions](#pod-conditions).\nYou can also inject [custom readiness information](#pod-readiness-gate) into the\ncondition data for a Pod, if that is useful to your application.\n\nPods are only [scheduled](/docs/concepts/scheduling-eviction/) once in their lifetime;\nassigning a Pod to a specific node is called _binding_, and the process of selecting\nwhich node to use is called _scheduling_.\nOnce a Pod has been scheduled and is bound to a node, Kubernetes tries\nto run that Pod on the node. The Pod runs on that node until it stops, or until the Pod\nis [terminated](#pod-termination); if Kubernetes isn't able to start the Pod on the selected\nnode (for example, if the node crashes before the Pod starts), then that particular Pod\nnever starts.\n\nYou can use [Pod Scheduling Readiness](/docs/concepts/scheduling-eviction/pod-scheduling-readiness/)\nto delay scheduling for a Pod until all its _scheduling gates_ are removed. For example,\nyou might want to define a set of Pods but only trigger scheduling once all the Pods\nhave been created.\n\n### Pods and fault recovery {#pod-fault-recovery}\n\nIf one of the containers in the Pod fails, then Kubernetes may try to restart that\nspecific container.\nRead [How Pods handle problems with containers](#container-restarts) to learn more.\n\nPods can however fail in a way that the cluster cannot recover from, and in that case\nKubernetes does not attempt to heal the Pod further; instead, Kubernetes deletes the\nPod and relies on other components to provide automatic healing.\n\nIf a Pod is scheduled to a {{< glossary_tooltip text=\"node\" term_id=\"node\" >}} and that\nnode then fails, the Pod is treated as unhealthy and Kubernetes eventually deletes the Pod.\nA Pod won't survive an {{< glossary_tooltip text=\"eviction\" term_id=\"eviction\" >}} due to\na lack of resources or Node maintenance.\n\nKubernetes uses a higher-level abstraction, called a\n{{< glossary_tooltip term_id=\"controller\" text=\"controller\" >}}, that handles the work of\nmanaging the relatively disposable Pod instances.\n\nA given Pod (as defined by a UID) is never \"rescheduled\" to a different node; instead,\nthat Pod can be replaced by a new, near-identical Pod. If you make a replacement Pod, it can\neven have same name (as in `.metadata.name`) that the old Pod had, but the replacement\nwould have a different `.metadata.uid` from the old Pod.\n\nKubernetes does not guarantee that a replacement for an existing Pod would be scheduled to\nthe same node as the old Pod that was being replaced.\n\n### Associated lifetimes\n\nWhen something is said to have the same lifetime as a Pod, such as a\n{{< glossary_tooltip term_id=\"volume\" text=\"volume\" >}},\nthat means that the thing exists as long as that specific Pod (with that exact UID)\nexists. If that Pod is deleted for any reason, and even if an identical replacement\nis created, the related thing (a volume, in this example) is also destroyed and\ncreated anew.\n\n{{< figure src=\"/images/docs/pod.svg\" title=\"Figure 1.\" class=\"diagram-medium\" caption=\"A multi-container Pod that contains a file puller [sidecar](/docs/concepts/workloads/pods/sidecar-containers/) and a web server. The Pod uses an [ephemeral `emptyDir` volume](/docs/concepts/storage/volumes/#emptydir) for shared storage between the containers.\" >}}\n\n",
    "parent_id": null,
    "source_file": "data/kubernetes/content/en/docs/concepts/workloads/pods/pod-lifecycle.md",
    "section_path": [
      "concepts",
      "workloads",
      "pods"
    ],
    "chunk_type": "parent",
    "strategy": "recursive"
  },
  {
    "id": 459,
    "content": "## Pod phase\n\nA Pod's `status` field is a\n[PodStatus](/docs/reference/generated/kubernetes-api/{{< param \"version\" >}}/#podstatus-v1-core)\nobject, which has a `phase` field.\n\nThe phase of a Pod is a simple, high-level summary of where the Pod is in its\nlifecycle. The phase is not intended to be a comprehensive rollup of observations\nof container or Pod state, nor is it intended to be a comprehensive state machine.\n\nThe number and meanings of Pod phase values are tightly guarded.\nOther than what is documented here, nothing should be assumed about Pods that\nhave a given `phase` value.\n\nHere are the possible values for `phase`:\n\nValue       | Description\n:-----------|:-----------\n`Pending`   | The Pod has been accepted by the Kubernetes cluster, but one or more of the containers has not been set up and made ready to run. This includes time a Pod spends waiting to be scheduled as well as the time spent downloading container images over the network.\n`Running`   | The Pod has been bound to a node, and all of the containers have been created. At least one container is still running, or is in the process of starting or restarting.\n`Succeeded` | All containers in the Pod have terminated in success, and will not be restarted.\n`Failed`    | All containers in the Pod have terminated, and at least one container has terminated in failure. That is, the container either exited with non-zero status or was terminated by the system, and is not set for automatic restarting.\n`Unknown`   | For some reason the state of the Pod could not be obtained. This phase typically occurs due to an error in communicating with the node where the Pod should be running.\n\n{{< note >}}\n\nWhen a pod is failing to start repeatedly, `CrashLoopBackOff` may appear in the `Status` field of some kubectl commands.\nSimilarly, when a pod is being deleted, `Terminating` may appear in the `Status` field of some kubectl commands.\n\nMake sure not to confuse _Status_, a kubectl display field for user intuition, with the pod's `phase`.\nPod phase is an explicit part of the Kubernetes data model and of the\n[Pod API](/docs/reference/kubernetes-api/workload-resources/pod-v1/). \n\n```\n  NAMESPACE               NAME               READY   STATUS             RESTARTS   AGE\n  alessandras-namespace   alessandras-pod    0/1     CrashLoopBackOff   200        2d9h\n```\n\nA Pod is granted a term to terminate gracefully, which defaults to 30 seconds.\nYou can use the flag `--force` to [terminate a Pod by force](/docs/concepts/workloads/pods/pod-lifecycle/#pod-termination-forced).\n{{< /note >}}\n\nSince Kubernetes 1.27, the kubelet transitions deleted Pods, except for\n[static Pods](/docs/tasks/configure-pod-container/static-pod/) and\n[force-deleted Pods](/docs/concepts/workloads/pods/pod-lifecycle/#pod-termination-forced)\nwithout a finalizer, to a terminal phase (`Failed` or `Succeeded` depending on\nthe exit statuses of the pod containers) before their deletion from the API server.\n\nIf a node dies or is disconnected from the rest of the cluster, Kubernetes\napplies a policy for setting the `phase` of all Pods on the lost node to Failed.\n\n",
    "parent_id": null,
    "source_file": "data/kubernetes/content/en/docs/concepts/workloads/pods/pod-lifecycle.md",
    "section_path": [
      "concepts",
      "workloads",
      "pods"
    ],
    "chunk_type": "parent",
    "strategy": "recursive"
  },
  {
    "id": 460,
    "content": "## Container states\n\nAs well as the [phase](#pod-phase) of the Pod overall, Kubernetes tracks the state of\neach container inside a Pod. You can use\n[container lifecycle hooks](/docs/concepts/containers/container-lifecycle-hooks/) to\ntrigger events to run at certain points in a container's lifecycle.\n\nOnce the {{< glossary_tooltip text=\"scheduler\" term_id=\"kube-scheduler\" >}}\nassigns a Pod to a Node, the kubelet starts creating containers for that Pod\nusing a {{< glossary_tooltip text=\"container runtime\" term_id=\"container-runtime\" >}}.\nThere are three possible container states: `Waiting`, `Running`, and `Terminated`.\n\nTo check the state of a Pod's containers, you can use\n`kubectl describe pod <name-of-pod>`. The output shows the state for each container\nwithin that Pod.\n\nEach state has a specific meaning:\n\n### `Waiting` {#container-state-waiting}\n\nIf a container is not in either the `Running` or `Terminated` state, it is `Waiting`.\nA container in the `Waiting` state is still running the operations it requires in\norder to complete start up: for example, pulling the container image from a container\nimage registry, or applying {{< glossary_tooltip text=\"Secret\" term_id=\"secret\" >}}\ndata.\nWhen you use `kubectl` to query a Pod with a container that is `Waiting`, you also see\na Reason field to summarize why the container is in that state.\n\n### `Running` {#container-state-running}\n\nThe `Running` status indicates that a container is executing without issues. If there\nwas a `postStart` hook configured, it has already executed and finished. When you use\n`kubectl` to query a Pod with a container that is `Running`, you also see information\nabout when the container entered the `Running` state.\n\n### `Terminated` {#container-state-terminated}\n\nA container in the `Terminated` state began execution and then either ran to\ncompletion or failed for some reason. When you use `kubectl` to query a Pod with\na container that is `Terminated`, you see a reason, an exit code, and the start and\nfinish time for that container's period of execution.\n\nIf a container has a `preStop` hook configured, this hook runs before the container enters\nthe `Terminated` state.\n\n",
    "parent_id": null,
    "source_file": "data/kubernetes/content/en/docs/concepts/workloads/pods/pod-lifecycle.md",
    "section_path": [
      "concepts",
      "workloads",
      "pods"
    ],
    "chunk_type": "parent",
    "strategy": "recursive"
  },
  {
    "id": 461,
    "content": "## How Pods handle problems with containers {#container-restarts}\n\nKubernetes manages container failures within Pods using a [`restartPolicy`](#restart-policy) defined\nin the Pod `spec`. This policy determines how Kubernetes reacts to containers exiting due to errors\nor other reasons, which falls in the following sequence:\n\n1. **Initial crash**: Kubernetes attempts an immediate restart based on the Pod `restartPolicy`.\n1. **Repeated crashes**: After the initial crash Kubernetes applies an exponential\n   backoff delay for subsequent restarts, described in [`restartPolicy`](#restart-policy).\n   This prevents rapid, repeated restart attempts from overloading the system.\n1. **CrashLoopBackOff state**: This indicates that the backoff delay mechanism is currently\n   in effect for a given container that is in a crash loop, failing and restarting repeatedly.\n1. **Backoff reset**: If a container runs successfully for a certain duration\n   (e.g., 10 minutes), Kubernetes resets the backoff delay, treating any new crash\n   as the first one.\n\nIn practice, a `CrashLoopBackOff` is a condition or event that might be seen as output\nfrom the `kubectl` command, while describing or listing Pods, when a container in the Pod\nfails to start properly and then continually tries and fails in a loop.\n\nIn other words, when a container enters the crash loop, Kubernetes applies the\nexponential backoff delay mentioned in the [Container restart policy](#restart-policy).\nThis mechanism prevents a faulty container from overwhelming the system with continuous\nfailed start attempts.\n\nThe `CrashLoopBackOff` can be caused by issues like the following:\n\n* Application errors that cause the container to exit.\n* Configuration errors, such as incorrect environment variables or missing\n  configuration files.\n* Resource constraints, where the container might not have enough memory or CPU\n  to start properly.\n* Health checks failing if the application doesn't start serving within the\n  expected time.\n* Container liveness probes or startup probes returning a `Failure` result\n  as mentioned in the [probes section](#container-probes).\n\nTo investigate the root cause of a `CrashLoopBackOff` issue, a user can:\n\n1. **Check logs**: Use `kubectl logs <name-of-pod>` to check the logs of the container.\n   This is often the most direct way to diagnose the issue causing the crashes.\n1. **Inspect events**: Use `kubectl describe pod <name-of-pod>` to see events\n   for the Pod, which can provide hints about configuration or resource issues.\n1. **Review configuration**: Ensure that the Pod configuration, including\n   environment variables and mounted volumes, is correct and that all required\n   external resources are available.\n1. **Check resource limits**: Make sure that the container has enough CPU\n   and memory allocated. Sometimes, increasing the resources in the Pod definition\n   can resolve the issue.\n1. **Debug application**: There might exist bugs or misconfigurations in the\n   application code. Running this container image locally or in a development\n   environment can help diagnose application specific issues.\n\n### Container restarts {#restart-policy}\n\nWhen a container in your Pod stops, or experiences failure, Kubernetes can restart it.\nA restart isn't always appropriate; for example,\n{{< glossary_tooltip text=\"init containers\" term_id=\"init-container\" >}} run only once (if successful),\nduring Pod startup.\nYou can configure restarts as a policy that applies to all Pods, or using container-level configuration (for example: when you define a\n{{< glossary_tooltip text=\"sidecar container\" term_id=\"sidecar-container\" >}}) or define container-level override.\n\n#### Container restarts and resilience {#container-restart-resilience}\n\nThe Kubernetes project recommends following cloud-native principles, including resilient\ndesign that accounts for unannounced or arbitrary restarts. You can achieve this either\nby failing the Pod and relying on automatic\n[replacement](/docs/concepts/workloads/controllers/), or you can design for container-level resilience.\nEither approach helps to ensure that your overall workload remains available despite\npartial failure.\n\n#### Pod-level container restart policy\n\nThe `spec` of a Pod has a `restartPolicy` field with possible values Always, OnFailure,\nand Never. The default value is Always.\n\nThe `restartPolicy` for a Pod applies to {{< glossary_tooltip text=\"app containers\" term_id=\"app-container\" >}}\nin the Pod and to regular [init containers](/docs/concepts/workloads/pods/init-containers/).\n[Sidecar containers](/docs/concepts/workloads/pods/sidecar-containers/)\nignore the Pod-level `restartPolicy` field: in Kubernetes, a sidecar is defined as an\nentry inside `initContainers` that has its container-level `restartPolicy` set to `Always`.\nFor init containers that exit with an error, the kubelet restarts the init container if\nthe Pod level `restartPolicy` is either `OnFailure` or `Always`:\n\n* `Always`: Automatically restarts the container after any termination.\n* `OnFailure`: Only restarts the container if it exits with an error (non-zero exit status).\n* `Never`: Does not automatically restart the terminated container.\n\nWhen the kubelet is handling container restarts according to the configured restart\npolicy, that only applies to restarts that make replacement containers inside the\nsame Pod and running on the same node. After containers in a Pod exit, the kubelet\nrestarts them with an exponential backoff delay (10s, 20s, 40s, …), that is capped at\n300 seconds (5 minutes). Once a container has executed for 10 minutes without any\nproblems, the kubelet resets the restart backoff timer for that container.\n[Sidecar containers and Pod lifecycle](/docs/concepts/workloads/pods/sidecar-containers/#sidecar-containers-and-pod-lifecycle)\nexplains the behaviour of `init containers` when specify `restartPolicy` field on it.\n\n#### Individual container restart policy and rules {#container-restart-rules}\n\n{{< feature-state feature_gate_name=\"ContainerRestartRules\" >}}\n\nIf your cluster has the feature gate `ContainerRestartRules` enabled, you can specify\n`restartPolicy` and `restartPolicyRules` on _individual containers_ to override the Pod\nrestart policy. Container restart policy and rules applies to {{< glossary_tooltip text=\"app containers\" term_id=\"app-container\" >}}\nin the Pod and to regular [init containers](/docs/concepts/workloads/pods/init-containers/).\n\nA Kubernetes-native [sidecar container](/docs/concepts/workloads/pods/sidecar-containers/)\nhas its container-level `restartPolicy` set to `Always`.\n\nThe container restarts will follow the same exponential backoff as pod restart policy described above. \nSupported container restart policies:\n\n* `Always`: Automatically restarts the container after any termination.\n* `OnFailure`: Only restarts the container if it exits with an error (non-zero exit status).\n* `Never`: Does not automatically restart the terminated container.\n\nAdditionally, _individual containers_ can specify `restartPolicyRules`. If the `restartPolicyRules`\nfield is specified, then container `restartPolicy` **must** also be specified. The `restartPolicyRules`\ndefine a list of rules to apply on container exit. Each rule will consist of a condition\nand an action. The supported condition is `exitCodes`, which compares the exit code of the container\nwith a list of given values. The supported action is `Restart`, which means the container will be\nrestarted. The rules will be evaluated in order. On the first match, the action will be applied.\nIf none of the rules’ conditions matched, Kubernetes fallback to container’s configured\n`restartPolicy`.\n\nFor example, a Pod with OnFailure restart policy that have a `try-once` container. This allows\nPod to only restart certain containers:\n\n```yaml\napiVersion: v1\nkind: Pod\nmetadata:\n  name: on-failure-pod\nspec:\n  restartPolicy: OnFailure\n  containers:\n  - name: try-once-container    # This container will run only once because the restartPolicy is Never.\n    image: registry.k8s.io/busybox:1.27.2\n    command: ['sh', '-c', 'echo \"Only running once\" && sleep 10 && exit 1']\n    restartPolicy: Never     \n  - name: on-failure-container  # This container will be restarted on failure.\n    image: registry.k8s.io/busybox:1.27.2\n    command: ['sh', '-c', 'echo \"Keep restarting\" && sleep 1800 && exit 1']\n```\n\nA Pod with `Always` restart policy with an init container that only execute once. If the init\ncontainer fails, the Pod fails. This allows the Pod to fail if the initialization failed,\nbut also keep running once the initialization succeeds:\n\n```yaml\napiVersion: v1\nkind: Pod\nmetadata:\n  name: fail-pod-if-init-fails\nspec:\n  restartPolicy: Always\n  initContainers:\n  - name: init-once      # This init container will only try once. If it fails, the pod will fail.\n    image: registry.k8s.io/busybox:1.27.2\n    command: ['sh', '-c', 'echo \"Failing initialization\" && sleep 10 && exit 1']\n    restartPolicy: Never\n  containers:\n  - name: main-container # This container will always be restarted once initialization succeeds.\n    image: registry.k8s.io/busybox:1.27.2\n    command: ['sh', '-c', 'sleep 1800 && exit 0']\n```\n\nA Pod with Never restart policy with a container that ignores and restarts on specific exit codes.\nThis is useful to differentiate between restartable errors and non-restartable errors:\n\n```yaml\napiVersion: v1\nkind: Pod\nmetadata:\n  name: restart-on-exit-codes\nspec:\n  restartPolicy: Never\n  containers:\n  - name: restart-on-exit-codes\n    image: registry.k8s.io/busybox:1.27.2\n    command: ['sh', '-c', 'sleep 60 && exit 0']\n    restartPolicy: Never     # Container restart policy must be specified if rules are specified\n    restartPolicyRules:      # Only restart the container if it exits with code 42\n    - action: Restart\n      exitCodes:\n        operator: In\n        values: [42]\n```\n\nRestart rules can be used for many more advanced lifecycle management scenarios. Note, restart rules\nare affected by the same inconsistencies as the regular restart policy. The kubelet restarts, container\nruntime garbage collection, intermitted connectivity issues with the control plane may cause the state\nloss and containers may be re-run even when you expect a container not to be restarted.\n\n#### Restart All Containers {#restart-all-containers}\n\n{{< feature-state feature_gate_name=\"RestartAllContainersOnContainerExits\" >}}\n\nIf your cluster has the feature gate `RestartAllContainersOnContainerExits` enabled, you can specify\n`RestartAllContainers` as an action in `restartPolicyRules` at container level. When a container's exit\nmatches a rule with this action, the entire Pod is terminated and restarted in-place.\n\nThis \"in-place\" restart offers a more efficient way to reset a Pod's state compared to full deletion\nand recreation. This is especially valuable for workloads where rescheduling is costly, such as\nbatch jobs or AI/ML training tasks.\n\n##### How in-place Pod restarts work\n\nWhen a `RestartAllContainers` action is triggered, the kubelet performs the following steps:\n\n1. **Fast Termination**: All running containers in the Pod are terminated.\n   The configured `terminationGracePeriodSeconds` is not respected, and any configured `preStop` hooks\n   are not executed. This ensures a swift shutdown.\n1. **Preservation of Pod Resources**: The Pod's essential resources are preserved:\n\n   * Pod UID, IP address, and network namespace\n   * Pod sandbox and any attached devices\n   * All volumes, including `emptyDir` and mounted volumes\n\n1. **Pod Status Update**: The Pod's status is updated with a `PodRestartInPlace` condition set to `True`.\n   This makes the restart process observable.\n1. **Full Restart Sequence**: Once all containers are terminated, the `PodRestartInPlace` condition\n   is set to `False`, and the Pod begins the standard startup process:\n\n   * **Init containers are re-run** in order.\n   * Sidecar and regular containers are started.\n\nA key aspect of this feature is that **all** containers are restarted, including those that\npreviously completed successfully or failed. The `RestartAllContainers` action overrides\nany configured container-level or Pod-level `restartPolicy`.\n\nThis mechanism is useful in scenarios where a clean slate for all containers is necessary, such as:\n\n- When an `init` container sets up an environment that can become corrupted, this feature ensures\n  the setup process is re-executed.\n- A sidecar container can monitor the health of a main application and trigger a full Pod restart\n  if the application enters an unrecoverable state.\n\nConsider a workload where a watcher sidecar is responsible for restarting the main application\nfrom a known-good state if it encounters an error. The watcher can exit with a specific code\nto trigger a full, in-place restart of the worker Pod.\n\n{{% code_sample file=\"pods/restart-policy/restart-all-containers.yaml\" %}}\n\nIn this example:\n\n- The Pod's overall `restartPolicy` is `Never`.\n- The `watcher-sidecar` runs a command and then exits with code `88`.\n- The exit code matches the rule, triggering the `RestartAllContainers` action.\n- The entire Pod, including the `setup-environment` init container and the `main-application` container,\n  is then restarted in-place. The pod keeps its UID, sandbox, IP, and volumes.\n\n### Reduced container restart delay\n\n{{< feature-state\nfeature_gate_name=\"ReduceDefaultCrashLoopBackOffDecay\" >}}\n\nWith the alpha feature gate `ReduceDefaultCrashLoopBackOffDecay` enabled,\ncontainer start retries across your cluster will be reduced to begin at 1s\n(instead of 10s) and increase exponentially by 2x each restart until a maximum\ndelay of 60s (instead of 300s which is 5 minutes).\n\nIf you use this feature along with the alpha feature\n`KubeletCrashLoopBackOffMax` (described below), individual nodes may have\ndifferent maximum delays.\n\n### Configurable container restart delay\n\n{{< feature-state feature_gate_name=\"KubeletCrashLoopBackOffMax\" >}}\n\nWith the feature gate `KubeletCrashLoopBackOffMax` enabled, you can\nreconfigure the maximum delay between container start retries from the default\nof 300s (5 minutes). This configuration is set per node using kubelet\nconfiguration. In your [kubelet configuration](/docs/tasks/administer-cluster/kubelet-config-file/),\nunder `crashLoopBackOff` set the `maxContainerRestartPeriod` field between `\"1s\"` and\n`\"300s\"`. As described above in [Container restart policy](#restart-policy),\ndelays on that node will still start at 10s and increase exponentially by 2x\neach restart, but will now be capped at your configured maximum. If the\n`maxContainerRestartPeriod` you configure is less than the default initial value\nof 10s, the initial delay will instead be set to the configured maximum.\n\nSee the following kubelet configuration examples:\n\n```yaml\n# container restart delays will start at 10s, increasing\n# 2x each time they are restarted, to a maximum of 100s\nkind: KubeletConfiguration\ncrashLoopBackOff:\n    maxContainerRestartPeriod: \"100s\"\n```\n\n```yaml\n# delays between container restarts will always be 2s\nkind: KubeletConfiguration\ncrashLoopBackOff:\n    maxContainerRestartPeriod: \"2s\"\n```\n\nIf you use this feature along with the alpha feature\n`ReduceDefaultCrashLoopBackOffDecay` (described above), your cluster defaults\nfor initial backoff and maximum backoff will no longer be 10s and 300s, but 1s\nand 60s. Per node configuration takes precedence over the defaults set by\n`ReduceDefaultCrashLoopBackOffDecay`, even if this would result in a node having\na longer maximum backoff than other nodes in the cluster.\n\n",
    "parent_id": null,
    "source_file": "data/kubernetes/content/en/docs/concepts/workloads/pods/pod-lifecycle.md",
    "section_path": [
      "concepts",
      "workloads",
      "pods"
    ],
    "chunk_type": "parent",
    "strategy": "recursive"
  },
  {
    "id": 462,
    "content": "## Pod conditions\n\nA Pod has a PodStatus, which has an array of\n[PodConditions](/docs/reference/generated/kubernetes-api/{{< param \"version\" >}}/#podcondition-v1-core)\nthrough which the Pod has or has not passed. The kubelet manages the following\nPodConditions:\n\n* `PodScheduled`: the Pod has been scheduled to a node.\n* `PodReadyToStartContainers`: (beta feature; enabled by [default](#pod-has-network)) the\n  Pod sandbox has been successfully created and networking configured.\n* `ContainersReady`: all containers in the Pod are ready.\n* `Initialized`: all [init containers](/docs/concepts/workloads/pods/init-containers/)\n  have completed successfully.\n* `Ready`: the Pod is able to serve requests and should be added to the load\n  balancing pools of all matching Services.\n* `DisruptionTarget`: the pod is about to be terminated due to a disruption (such as preemption, eviction or garbage-collection).\n* `PodResizePending`: a pod resize was requested but cannot be applied. See [Pod resize status](/docs/tasks/configure-pod-container/resize-container-resources#pod-resize-status).\n* `PodResizeInProgress`: the pod is in the process of resizing. See\n  [Pod resize status](/docs/tasks/configure-pod-container/resize-container-resources#pod-resize-status).\n\nField name           | Description\n:--------------------|:-----------\n`type`               | Name of this Pod condition.\n`status`             | Indicates whether that condition is applicable, with possible values \"`True`\", \"`False`\", or \"`Unknown`\".\n`lastProbeTime`      | Timestamp of when the Pod condition was last probed.\n`lastTransitionTime` | Timestamp for when the Pod last transitioned from one status to another.\n`reason`             | Machine-readable, UpperCamelCase text indicating the reason for the condition's last transition.\n`message`            | Human-readable message indicating details about the last status transition.\n\n### Pod readiness {#pod-readiness-gate}\n\n{{< feature-state for_k8s_version=\"v1.14\" state=\"stable\" >}}\n\nYour application can inject extra feedback or signals into PodStatus:\n_Pod readiness_. To use this, set `readinessGates` in the Pod's `spec` to\nspecify a list of additional conditions that the kubelet evaluates for Pod readiness.\n\nReadiness gates are determined by the current state of `status.condition`\nfields for the Pod. If Kubernetes cannot find such a condition in the\n`status.conditions` field of a Pod, the status of the condition\nis defaulted to \"`False`\".\n\nHere is an example:\n\n```yaml\nkind: Pod\n...\nspec:\n  readinessGates:\n    - conditionType: \"www.example.com/feature-1\"\nstatus:\n  conditions:\n    - type: Ready                              # a built-in PodCondition\n      status: \"False\"\n      lastProbeTime: null\n      lastTransitionTime: 2018-01-01T00:00:00Z\n    - type: \"www.example.com/feature-1\"        # an extra PodCondition\n      status: \"False\"\n      lastProbeTime: null\n      lastTransitionTime: 2018-01-01T00:00:00Z\n  containerStatuses:\n    - containerID: docker://abcd...\n      ready: true\n...\n```\n\nThe Pod conditions you add must have names that meet the Kubernetes\n[label key format](/docs/concepts/overview/working-with-objects/labels/#syntax-and-character-set).\n\n### Status for Pod readiness {#pod-readiness-status}\n\nThe `kubectl patch` command does not support patching object status.\nTo set these `status.conditions` for the Pod, applications and\n{{< glossary_tooltip term_id=\"operator-pattern\" text=\"operators\">}} should use\nthe `PATCH` action.\nYou can use a [Kubernetes client library](/docs/reference/using-api/client-libraries/) to\nwrite code that sets custom Pod conditions for Pod readiness.\n\nFor a Pod that uses custom conditions, that Pod is evaluated to be ready **only**\nwhen both the following statements apply:\n\n* All containers in the Pod are ready.\n* All conditions specified in `readinessGates` are `True`.\n\nWhen a Pod's containers are Ready but at least one custom condition is missing or\n`False`, the kubelet sets the Pod's [condition](#pod-conditions) to `ContainersReady`.\n\n### Pod network readiness {#pod-has-network}\n\n{{< feature-state for_k8s_version=\"v1.29\" state=\"beta\" >}}\n\n{{< note >}}\nDuring its early development, this condition was named `PodHasNetwork`.\n{{< /note >}}\n\nAfter a Pod gets scheduled on a node, it needs to be admitted by the kubelet and\nto have any required storage volumes mounted. Once these phases are complete,\nthe kubelet works with\na container runtime (using {{< glossary_tooltip term_id=\"cri\" >}}) to set up a\nruntime sandbox and configure networking for the Pod. If the\n`PodReadyToStartContainersCondition`\n[feature gate](/docs/reference/command-line-tools-reference/feature-gates/) is enabled\n(it is enabled by default for Kubernetes {{< skew currentVersion >}}), the\n`PodReadyToStartContainers` condition will be added to the `status.conditions` field of a Pod.\n\nThe `PodReadyToStartContainers` condition is set to `False` by the kubelet when it detects a\nPod does not have a runtime sandbox with networking configured. This occurs in\nthe following scenarios:\n\n- Early in the lifecycle of the Pod, when the kubelet has not yet begun to set up a sandbox for\n  the Pod using the container runtime.\n- Later in the lifecycle of the Pod, when the Pod sandbox has been destroyed due to either:\n  - the node rebooting, without the Pod getting evicted\n  - for container runtimes that use virtual machines for isolation, the Pod\n    sandbox virtual machine rebooting, which then requires creating a new sandbox and\n    fresh container network configuration.\n\nThe `PodReadyToStartContainers` condition is set to `True` by the kubelet after the\nsuccessful completion of sandbox creation and network configuration for the Pod\nby the runtime plugin. The kubelet can start pulling container images and create\ncontainers after `PodReadyToStartContainers` condition has been set to `True`.\n\nFor a Pod with init containers, the kubelet sets the `Initialized` condition to\n`True` after the init containers have successfully completed (which happens\nafter successful sandbox creation and network configuration by the runtime\nplugin). For a Pod without init containers, the kubelet sets the `Initialized`\ncondition to `True` before sandbox creation and network configuration starts.\n\n",
    "parent_id": null,
    "source_file": "data/kubernetes/content/en/docs/concepts/workloads/pods/pod-lifecycle.md",
    "section_path": [
      "concepts",
      "workloads",
      "pods"
    ],
    "chunk_type": "parent",
    "strategy": "recursive"
  },
  {
    "id": 463,
    "content": "## Resizing Pods {#pod-resize}\n\n{{< feature-state feature_gate_name=\"InPlacePodVerticalScaling\" >}}\n\nKubernetes supports changing the CPU and memory resources allocated to Pods\nafter they are created. (For other infrastructure resources, you would need to\nuse different techniques specific to those resources.) There are two main\napproaches to resizing CPU and memory:\n\n### In-place Pod resize {#pod-resize-inplace}\n\nYou can resize a Pod's container-level CPU and memory resources without recreating the Pod.\nThis is also called _in-place Pod vertical scaling_. This allows you to adjust resource\nallocation for running containers while potentially avoiding application disruption.\n\nTo perform an in-place resize, you update the Pod's desired state using the `/resize`\nsubresource. The kubelet then attempts to apply the new resource values to the running\ncontainers. The Pod {{< glossary_tooltip text=\"conditions\" term_id=\"condition\" >}}\n`PodResizePending` and `PodResizeInProgress` (described in [Pod conditions](#pod-conditions))\nindicate the status of the resize operation. For more details about resize status, see\n[Container Resize Status](/docs/tasks/configure-pod-container/resize-container-resources/#container-resize-status).\n\nKey considerations for in-place resize:\n- Only CPU and memory resources can be resized in-place.\n- The Pod's [Quality of Service (QoS) class](/docs/concepts/workloads/pods/pod-qos/)\n  is determined at creation and cannot be changed by resizing.\n- You can configure whether a container restart is required for the resize using\n  `resizePolicy` in the container specification.\n\nFor detailed instructions on performing in-place resize, see\n[Resize CPU and Memory Resources assigned to Containers](/docs/tasks/configure-pod-container/resize-container-resources/).\n\n### Resizing by launching replacement Pods\n\nThe more cloud native approach to changing a Pod's resources is through the\nworkload resource that manages it (such as a Deployment or StatefulSet).\nWhen you update the resource specifications in the Pod template,\nthe workload's controller creates new Pods with the updated resources and terminates\nthe old Pods according to its update strategy.\n\nThis approach:\n- Works with any Kubernetes version.\n- Can change any Pod specification, not just resources.\n- Results in Pod replacement, so you should design your workload to handle\n  [planned disruptions](/docs/concepts/workloads/pods/disruptions/). Consider using a\n  [PodDisruptionBudget](/docs/tasks/run-application/configure-pdb/) to control availability.\n- Requires that your Pods are managed by a workload resource.\n\nYou can also use a\n[VerticalPodAutoscaler](/docs/concepts/workloads/autoscaling/vertical-pod-autoscale/)\nto automatically manage Pod resource recommendations and updates.\n\n",
    "parent_id": null,
    "source_file": "data/kubernetes/content/en/docs/concepts/workloads/pods/pod-lifecycle.md",
    "section_path": [
      "concepts",
      "workloads",
      "pods"
    ],
    "chunk_type": "parent",
    "strategy": "recursive"
  },
  {
    "id": 464,
    "content": "## Container probes\n\nA _probe_ is a diagnostic performed periodically by the [kubelet](/docs/reference/command-line-tools-reference/kubelet/)\non a container. To perform a diagnostic, the kubelet either executes code within the container,\nor makes a network request.\n\n### Check mechanisms {#probe-check-methods}\n\nThere are four different ways to check a container using a probe.\nEach probe must define exactly one of these four mechanisms:\n\n`exec`\n: Executes a specified command inside the container. The diagnostic\n  is considered successful if the command exits with a status code of 0.\n\n`grpc`\n: Performs a remote procedure call using [gRPC](https://grpc.io/).\n  The target should implement\n  [gRPC health checks](https://grpc.io/grpc/core/md_doc_health-checking.html).\n  The diagnostic is considered successful if the `status`\n  of the response is `SERVING`.  \n\n`httpGet`\n: Performs an HTTP `GET` request against the Pod's IP\n  address on a specified port and path. The diagnostic is\n  considered successful if the response has a status code\n  greater than or equal to 200 and less than 400.\n\n`tcpSocket`\n: Performs a TCP check against the Pod's IP address on\n  a specified port. The diagnostic is considered successful if\n  the port is open. If the remote system (the container) closes\n  the connection immediately after it opens, this counts as healthy.\n\n{{< caution >}}\nUnlike the other mechanisms, `exec` probe's implementation involves\nthe creation/forking of multiple processes each time when executed.\nAs a result, in case of the clusters having higher pod densities,\nlower intervals of `initialDelaySeconds`, `periodSeconds`,\nconfiguring any probe with exec mechanism might introduce an overhead on the cpu usage of the node.\nIn such scenarios, consider using the alternative probe mechanisms to avoid the overhead.\n{{< /caution >}}\n\n### Probe outcome\n\nEach probe has one of three results:\n\n`Success`\n: The container passed the diagnostic.\n\n`Failure`\n: The container failed the diagnostic.\n\n`Unknown`\n: The diagnostic failed (no action should be taken, and the kubelet\n  will make further checks).\n\n### Types of probe\n\nThe kubelet can optionally perform and react to three kinds of probes on running\ncontainers:\n\n`livenessProbe`\n: Indicates whether the container is running. If\n  the liveness probe fails, the kubelet kills the container, and the container\n  is subjected to its [restart policy](#restart-policy). If a container does not\n  provide a liveness probe, the default state is `Success`.\n\n`readinessProbe`\n: Indicates whether the container is ready to respond to requests.\n  If the readiness probe fails, the {{< glossary_tooltip term_id=\"endpoint-slice\" text=\"EndpointSlice\" >}}\n  controller removes the Pod's IP address from the EndpointSlices of all Services that match the Pod.\n  The default state of readiness before the initial delay is `Failure`. If a container does\n  not provide a readiness probe, the default state is `Success`.\n\n`startupProbe`\n: Indicates whether the application within the container is started.\n  All other probes are disabled if a startup probe is provided, until it succeeds.\n  If the startup probe fails, the kubelet kills the container, and the container\n  is subjected to its [restart policy](#restart-policy). If a container does not\n  provide a startup probe, the default state is `Success`.\n\nFor more information about how to set up a liveness, readiness, or startup probe,\nsee [Configure Liveness, Readiness and Startup Probes](/docs/tasks/configure-pod-container/configure-liveness-readiness-startup-probes/).\n\n#### When should you use a liveness probe?\n\nIf the process in your container is able to crash on its own whenever it\nencounters an issue or becomes unhealthy, you do not necessarily need a liveness\nprobe; the kubelet will automatically perform the correct action in accordance\nwith the Pod's `restartPolicy`.\n\nIf you'd like your container to be killed and restarted if a probe fails, then\nspecify a liveness probe, and specify a `restartPolicy` of Always or OnFailure.\n\n#### When should you use a readiness probe?\n\nIf you'd like to start sending traffic to a Pod only when a probe succeeds,\nspecify a readiness probe. In this case, the readiness probe might be the same\nas the liveness probe, but the existence of the readiness probe in the spec means\nthat the Pod will start without receiving any traffic and only start receiving\ntraffic after the probe starts succeeding.\n\nIf you want your container to be able to take itself down for maintenance, you\ncan specify a readiness probe that checks an endpoint specific to readiness that\nis different from the liveness probe.\n\nIf your app has a strict dependency on back-end services, you can implement both\na liveness and a readiness probe. The liveness probe passes when the app itself\nis healthy, but the readiness probe additionally checks that each required\nback-end service is available. This helps you avoid directing traffic to Pods\nthat can only respond with error messages.\n\nIf your container needs to work on loading large data, configuration files, or\nmigrations during startup, you can use a\n[startup probe](#when-should-you-use-a-startup-probe). However, if you want to\ndetect the difference between an app that has failed and an app that is still\nprocessing its startup data, you might prefer a readiness probe.\n\n{{< note >}}\nIf you want to be able to drain requests when the Pod is deleted, you do not\nnecessarily need a readiness probe; when the Pod is deleted, the corresponding endpoint\nin the `EndpointSlice` will update its [conditions](/docs/concepts/services-networking/endpoint-slices/#conditions):\nthe endpoint `ready` condition will be set to `false`, so load balancers\nwill not use the Pod for regular traffic. See [Pod termination](#pod-termination)\nfor more information about how the kubelet handles Pod deletion.\n{{< /note >}}\n\n#### When should you use a startup probe?\n\nStartup probes are useful for Pods that have containers that take a long time to\ncome into service. Rather than set a long liveness interval, you can configure\na separate configuration for probing the container as it starts up, allowing\na time longer than the liveness interval would allow.\n\n<!-- ensure front matter contains math: true -->\nIf your container usually starts in more than\n\\\\( initialDelaySeconds + failureThreshold \\times  periodSeconds \\\\), you should specify a\nstartup probe that checks the same endpoint as the liveness probe. The default for\n`periodSeconds` is 10s. You should then set its `failureThreshold` high enough to\nallow the container to start, without changing the default values of the liveness\nprobe. This helps to protect against deadlocks.\n\n",
    "parent_id": null,
    "source_file": "data/kubernetes/content/en/docs/concepts/workloads/pods/pod-lifecycle.md",
    "section_path": [
      "concepts",
      "workloads",
      "pods"
    ],
    "chunk_type": "parent",
    "strategy": "recursive"
  },
  {
    "id": 465,
    "content": "## Termination of Pods {#pod-termination}\n\nBecause Pods represent processes running on nodes in the cluster, it is important to\nallow those processes to gracefully terminate when they are no longer needed (rather\nthan being abruptly stopped with a `KILL` signal and having no chance to clean up).\n\nThe design aim is for you to be able to request deletion and know when processes\nterminate, but also be able to ensure that deletes eventually complete.\nWhen you request deletion of a Pod, the cluster records and tracks the intended grace period\nbefore the Pod is allowed to be forcefully killed. With that forceful shutdown tracking in\nplace, the {{< glossary_tooltip text=\"kubelet\" term_id=\"kubelet\" >}} attempts graceful\nshutdown.\n\nTypically, with this graceful termination of the pod, kubelet makes requests to the container runtime\nto attempt to stop the containers in the pod by first sending a TERM (aka. SIGTERM) signal,\nwith a grace period timeout, to the main process in each container.\nThe requests to stop the containers are processed by the container runtime asynchronously.\nThere is no guarantee to the order of processing for these requests.\nMany container runtimes respect the `STOPSIGNAL` value defined in the container image and,\nif different, send the container image configured STOPSIGNAL instead of TERM.\nOnce the grace period has expired, the KILL signal is sent to any remaining\nprocesses, and the Pod is then deleted from the\n{{< glossary_tooltip text=\"API Server\" term_id=\"kube-apiserver\" >}}. If the kubelet or the\ncontainer runtime's management service is restarted while waiting for processes to terminate, the\ncluster retries from the start including the full original grace period.\n\n### Stop Signals {#pod-termination-stop-signals}\n\nThe stop signal used to kill the container can be defined in the container image with the `STOPSIGNAL` instruction.\nIf no stop signal is defined in the image, the default signal of the container runtime\n(SIGTERM for both containerd and CRI-O) would be used to kill the container.\n\n### Defining custom stop signals\n\n{{< feature-state feature_gate_name=\"ContainerStopSignals\" >}}\n\nIf the `ContainerStopSignals` feature gate is enabled, you can configure a custom stop signal\nfor your containers from the container Lifecycle. We require the Pod's `spec.os.name` field\nto be present as a requirement for defining stop signals in the container lifecycle.\nThe list of signals that are valid depends on the OS the Pod is scheduled to.\nFor Pods scheduled to Windows nodes, we only support SIGTERM and SIGKILL as valid signals.\n\nHere is an example Pod spec defining a custom stop signal:\n\n```yaml\nspec:\n  os:\n    name: linux\n  containers:\n    - name: my-container\n      image: container-image:latest\n      lifecycle:\n        stopSignal: SIGUSR1\n```\n\nIf a stop signal is defined in the lifecycle, this will override the signal defined in the container image.\nIf no stop signal is defined in the container spec, the container would fall back to the default behavior.\n\n### Pod Termination Flow {#pod-termination-flow}\n\nPod termination flow, illustrated with an example:\n\n1. You use the `kubectl` tool to manually delete a specific Pod, with the default grace period\n   (30 seconds).\n\n1. The Pod in the API server is updated with the time beyond which the Pod is considered \"dead\"\n   along with the grace period.\n   If you use `kubectl describe` to check the Pod you're deleting, that Pod shows up as \"Terminating\".\n   On the node where the Pod is running: as soon as the kubelet sees that a Pod has been marked\n   as terminating (a graceful shutdown duration has been set), the kubelet begins the local Pod\n   shutdown process.\n\n   1. If one of the Pod's containers has defined a `preStop`\n      [hook](/docs/concepts/containers/container-lifecycle-hooks) and the `terminationGracePeriodSeconds`\n      in the Pod spec is not set to 0, the kubelet runs that hook inside of the container.\n      The default `terminationGracePeriodSeconds` setting is 30 seconds.\n\n      If the `preStop` hook is still running after the grace period expires, the kubelet requests\n      a small, one-off grace period extension of 2 seconds.\n\n   {{% note %}}\n   If the `preStop` hook needs longer to complete than the default grace period allows,\n   you must modify `terminationGracePeriodSeconds` to suit this.\n   {{% /note %}}\n\n   1. The kubelet triggers the container runtime to send a TERM signal to process 1 inside each\n      container.\n\n      There is [special ordering](#termination-with-sidecars) if the Pod has any\n      {{< glossary_tooltip text=\"sidecar containers\" term_id=\"sidecar-container\" >}} defined.\n      Otherwise, the containers in the Pod receive the TERM signal at different times and in\n      an arbitrary order. If the order of shutdowns matters, consider using a `preStop` hook\n      to synchronize (or switch to using sidecar containers).\n\n1. At the same time as the kubelet is starting graceful shutdown of the Pod, the control plane\n   evaluates whether to remove that shutting-down Pod from EndpointSlice objects,\n   where those objects represent a {{< glossary_tooltip term_id=\"service\" text=\"Service\" >}}\n   with a configured {{< glossary_tooltip text=\"selector\" term_id=\"selector\" >}}.\n   {{< glossary_tooltip text=\"ReplicaSets\" term_id=\"replica-set\" >}} and other workload resources\n   no longer treat the shutting-down Pod as a valid, in-service replica.\n\n   Pods that shut down slowly should not continue to serve regular traffic and should start\n   terminating and finish processing open connections.  Some applications need to go beyond\n   finishing open connections and need more graceful termination, for example, session draining\n   and completion.\n\n   Any endpoints that represent the terminating Pods are not immediately removed from\n   EndpointSlices, and a status indicating [terminating state](/docs/concepts/services-networking/endpoint-slices/#conditions)\n   is exposed from the EndpointSlice API.\n   Terminating endpoints always have their `ready` status as `false` (for backward compatibility\n   with versions before 1.26), so load balancers will not use it for regular traffic.\n\n   If traffic draining on terminating Pod is needed, the actual readiness can be checked as a\n   condition `serving`.  You can find more details on how to implement connections draining in the\n   tutorial [Pods And Endpoints Termination Flow](/docs/tutorials/services/pods-and-endpoint-termination-flow/)\n\n   <a id=\"pod-termination-beyond-grace-period\" />\n\n1. The kubelet ensures the Pod is shut down and terminated\n\n   1. When the grace period expires, if there is still any container running in the Pod, the\n      kubelet triggers forcible shutdown.\n      The container runtime sends `SIGKILL` to any processes still running in any container in the Pod.\n      The kubelet also cleans up a hidden `pause` container if that container runtime uses one.\n   1. The kubelet transitions the Pod into a terminal phase (`Failed` or `Succeeded` depending on\n      the end state of its containers).\n   1. The kubelet triggers forcible removal of the Pod object from the API server, by setting grace period\n      to 0 (immediate deletion).\n   1. The API server deletes the Pod's API object, which is then no longer visible from any client.\n\n### Forced Pod termination {#pod-termination-forced}\n\n{{< caution >}}\nForced deletions can be potentially disruptive for some workloads and their Pods.\n{{< /caution >}}\n\nBy default, all deletes are graceful within 30 seconds. The `kubectl delete` command supports\nthe `--grace-period=<seconds>` option which allows you to override the default and specify your\nown value.\n\nSetting the grace period to `0` forcibly and immediately deletes the Pod from the API\nserver. If the Pod was still running on a node, that forcible deletion triggers the kubelet to\nbegin immediate cleanup.\n\nUsing kubectl, You must specify an additional flag `--force` along with `--grace-period=0`\nin order to perform force deletions.\n\nWhen a force deletion is performed, the API server does not wait for confirmation\nfrom the kubelet that the Pod has been terminated on the node it was running on. It\nremoves the Pod in the API immediately so a new Pod can be created with the same\nname. On the node, Pods that are set to terminate immediately will still be given\na small grace period before being force killed.\n\n{{< caution >}}\nImmediate deletion does not wait for confirmation that the running resource has been terminated.\nThe resource may continue to run on the cluster indefinitely.\n{{< /caution >}}\n\nIf you need to force-delete Pods that are part of a StatefulSet, refer to the task\ndocumentation for\n[deleting Pods from a StatefulSet](/docs/tasks/run-application/force-delete-stateful-set-pod/).\n\n### Pod shutdown and sidecar containers {#termination-with-sidecars}\n\nIf your Pod includes one or more\n[sidecar containers](/docs/concepts/workloads/pods/sidecar-containers/)\n(init containers with an `Always` restart policy), the kubelet will delay sending\nthe TERM signal to these sidecar containers until the last main container has fully terminated.\nThe sidecar containers will be terminated in the reverse order they are defined in the Pod spec.\nThis ensures that sidecar containers continue serving the other containers in the Pod until they\nare no longer needed.\n\nThis means that slow termination of a main container will also delay the termination of the sidecar containers.\nIf the grace period expires before the termination process is complete, the Pod may enter [forced termination](#pod-termination-beyond-grace-period).\nIn this case, all remaining containers in the Pod will be terminated simultaneously with a short grace period.\n\nSimilarly, if the Pod has a `preStop` hook that exceeds the termination grace period, emergency termination may occur.\nIn general, if you have used `preStop` hooks to control the termination order without sidecar containers, you can now\nremove them and allow the kubelet to manage sidecar termination automatically.\n\n### Garbage collection of Pods {#pod-garbage-collection}\n\nFor failed Pods, the API objects remain in the cluster's API until a human or\n{{< glossary_tooltip term_id=\"controller\" text=\"controller\" >}} process\nexplicitly removes them.\n\nThe Pod garbage collector (PodGC), which is a controller in the control plane, cleans up\nterminated Pods (with a phase of `Succeeded` or `Failed`), when the number of Pods exceeds the\nconfigured threshold (determined by `terminated-pod-gc-threshold` in the kube-controller-manager).\nThis avoids a resource leak as Pods are created and terminated over time.\n\nAdditionally, PodGC cleans up any Pods which satisfy any of the following conditions:\n\n1. are orphan Pods - bound to a node which no longer exists,\n1. are unscheduled terminating Pods,\n1. are terminating Pods, bound to a non-ready node tainted with\n   [`node.kubernetes.io/out-of-service`](/docs/reference/labels-annotations-taints/#node-kubernetes-io-out-of-service).\n\nAlong with cleaning up the Pods, PodGC will also mark them as failed if they are in a non-terminal\nphase. Also, PodGC adds a Pod disruption condition when cleaning up an orphan Pod.\nSee [Pod disruption conditions](/docs/concepts/workloads/pods/disruptions#pod-disruption-conditions)\nfor more details.\n\n",
    "parent_id": null,
    "source_file": "data/kubernetes/content/en/docs/concepts/workloads/pods/pod-lifecycle.md",
    "section_path": [
      "concepts",
      "workloads",
      "pods"
    ],
    "chunk_type": "parent",
    "strategy": "recursive"
  },
  {
    "id": 466,
    "content": "## Pod behavior during kubelet restarts {#kubelet-restarts}\n\nIf you restart the kubelet, Pods (and their containers) continue to run\neven during the restart.\nWhen there are running Pods on a node, stopping or restarting the kubelet\non that node does **not** cause the kubelet to stop all local Pods\nbefore the kubelet itself stops.\nTo stop the Pods on a node, you can use `kubectl drain`.\n\n### Detection of kubelet restarts\n\n{{< feature-state feature_gate_name=\"ChangeContainerStatusOnKubeletRestart\" >}}\n\nWhen the kubelet starts, it checks to see if there is already a Node with bound Pods.\nIf the Node's [`Ready` condition](/docs/reference/node/node-status/#condition) remains unchanged,\nin other words the condition has not transitioned from true to false, Kubernetes detects this a _kubelet restart_.\n(It's possible to restart the kubelet in other ways, for example to fix a node bug,\nbut in these cases, Kubernetes picks the safe option and treats this as if you\nstopped the kubelet and then later started it).\n\nWhen the kubelet restarts, the container statuses are managed differently based on the feature gate setting:\n\n* By default, the kubelet does not change container statuses after a restart.\n  Containers that were in set to `ready: true` state remain remain ready.\n\n  If you stop the kubelet long enough for it to fail a series of\n  [node heartbeat](/docs/concepts/architecture/leases/#node-heart-beats) checks,\n  and then you wait before you start the kubelet again, Kubernetes may begin to evict Pods from that Node.\n  However, even though Pod evictions begin to happen, Kubernetes does not mark the\n  individual containers in those Pods as `ready: false`. The Pod-level eviction\n  happens after the control plane taints the node as `node.kubernetes.io/not-ready` (due to the failed heartbeats).\n\n* In Kubernetes {{< skew currentVersion >}} you can opt in to a legacy behavior where the kubelet always modify\n  the containers `ready` value, after a kubelet restart, to be false.\n\n  This legacy behavior was the default for a long time, but caused issue for people using Kubernetes,\n  especially in large scale deployments. Althought the feature gate allows reverting to this legacy\n  behavior temporarily, the Kubernetes project recommends that you file a bug report if you encounter problems.\n  The `ChangeContainerStatusOnKubeletRestart`\n  [feature gate](/docs/reference/command-line-tools-reference/feature-gates/#ChangeContainerStatusOnKubeletRestart)\n  will be removed in the future.\n\n",
    "parent_id": null,
    "source_file": "data/kubernetes/content/en/docs/concepts/workloads/pods/pod-lifecycle.md",
    "section_path": [
      "concepts",
      "workloads",
      "pods"
    ],
    "chunk_type": "parent",
    "strategy": "recursive"
  },
  {
    "id": 467,
    "content": "## {{% heading \"whatsnext\" %}}\n\n* Get hands-on experience\n  [attaching handlers to container lifecycle events](/docs/tasks/configure-pod-container/attach-handler-lifecycle-event/).\n\n* Get hands-on experience\n  [configuring Liveness, Readiness and Startup Probes](/docs/tasks/configure-pod-container/configure-liveness-readiness-startup-probes/).\n\n* Learn more about [container lifecycle hooks](/docs/concepts/containers/container-lifecycle-hooks/).\n\n* Learn more about [sidecar containers](/docs/concepts/workloads/pods/sidecar-containers/).\n\n* For detailed information about Pod and container status in the API, see\n  the API reference documentation covering\n  [`status`](/docs/reference/kubernetes-api/workload-resources/pod-v1/#PodStatus) for Pod.\n",
    "parent_id": null,
    "source_file": "data/kubernetes/content/en/docs/concepts/workloads/pods/pod-lifecycle.md",
    "section_path": [
      "concepts",
      "workloads",
      "pods"
    ],
    "chunk_type": "parent",
    "strategy": "recursive"
  },
  {
    "id": 800,
    "content": "## Quality of Service classes\n\nKubernetes classifies the Pods that you run and allocates each Pod into a specific\n_quality of service (QoS) class_. Kubernetes uses that classification to influence how different\npods are handled. Kubernetes does this classification based on the\n[resource requests](/docs/concepts/configuration/manage-resources-containers/)\nof the {{< glossary_tooltip text=\"Containers\" term_id=\"container\" >}} in that Pod, along with\nhow those requests relate to resource limits.\nThis is known as {{< glossary_tooltip text=\"Quality of Service\" term_id=\"qos-class\" >}}\n(QoS) class. Kubernetes assigns every Pod a QoS class based on the resource requests\nand limits of its component Containers. QoS classes are used by Kubernetes to decide\nwhich Pods to evict from a Node experiencing\n[Node Pressure](/docs/concepts/scheduling-eviction/node-pressure-eviction/). The possible\nQoS classes are `Guaranteed`, `Burstable`, and `BestEffort`. When a Node runs out of resources,\nKubernetes will first evict `BestEffort` Pods running on that Node, followed by `Burstable` and\nfinally `Guaranteed` Pods. When this eviction is due to resource pressure, only Pods exceeding\nresource requests are candidates for eviction.\n\n### Guaranteed\n\nPods that are `Guaranteed` have the strictest resource limits and are least likely\nto face eviction. They are guaranteed not to be killed until they exceed their limits\nor there are no lower-priority Pods that can be preempted from the Node. They may\nnot acquire resources beyond their specified limits. These Pods can also make\nuse of exclusive CPUs using the\n[`static`](/docs/tasks/administer-cluster/cpu-management-policies/#static-policy-configuration) CPU management policy.\n\n#### Criteria\n\nFor a Pod to be given a QoS class of `Guaranteed`:\n\n* Every Container in the Pod must have a memory limit and a memory request.\n* For every Container in the Pod, the memory limit must equal the memory request.\n* Every Container in the Pod must have a CPU limit and a CPU request.\n* For every Container in the Pod, the CPU limit must equal the CPU request.\n\nIf instead the Pod uses [Pod-level resources](/docs/concepts/configuration/manage-resources-containers/#pod-level-resource-specification):\n\n{{< feature-state feature_gate_name=\"PodLevelResources\" >}}\n\n* The Pod must have a Pod-level memory limit and memory request, and their values must be equal.\n* The Pod must have a Pod-level CPU limit and CPU request, and their values must be equal.\n\n### Burstable\n\nPods that are `Burstable` have some lower-bound resource guarantees based on the request, but\ndo not require a specific limit. If a limit is not specified, it defaults to a\nlimit equivalent to the capacity of the Node, which allows the Pods to flexibly increase\ntheir resources if resources are available. In the event of Pod eviction due to Node\nresource pressure, these Pods are evicted only after all `BestEffort` Pods are evicted.\nBecause a `Burstable` Pod can include a Container that has no resource limits or requests, a Pod\nthat is `Burstable` can try to use any amount of node resources.\n\n#### Criteria\n\nA Pod is given a QoS class of `Burstable` if:\n\n* The Pod does not meet the criteria for QoS class `Guaranteed`.\n* At least one Container in the Pod has a memory or CPU request or limit,\n  or the Pod has a Pod-level memory or CPU request or limit.\n\n### BestEffort\n\nPods in the `BestEffort` QoS class can use node resources that aren't specifically assigned\nto Pods in other QoS classes. For example, if you have a node with 16 CPU cores available to the\nkubelet, and you assign 4 CPU cores to a `Guaranteed` Pod, then a Pod in the `BestEffort`\nQoS class can try to use any amount of the remaining 12 CPU cores.\n\nThe kubelet prefers to evict `BestEffort` Pods if the node comes under resource pressure.\n\n#### Criteria\n\nA Pod has a QoS class of `BestEffort` if it doesn't meet the criteria for either `Guaranteed`\nor `Burstable`. In other words, a Pod is `BestEffort` only if none of the Containers in the Pod have a\nmemory limit or a memory request, and none of the Containers in the Pod have a\nCPU limit or a CPU request, and the Pod does not have any Pod-level memory or CPU limits or requests.\nContainers in a Pod can request other resources (not CPU or memory) and still be classified as\n`BestEffort`.\n\n",
    "parent_id": null,
    "source_file": "data/kubernetes/content/en/docs/concepts/workloads/pods/pod-qos.md",
    "section_path": [
      "concepts",
      "workloads",
      "pods"
    ],
    "chunk_type": "parent",
    "strategy": "recursive"
  },
  {
    "id": 801,
    "content": "## Memory QoS with cgroup v2\n\n{{< feature-state feature_gate_name=\"MemoryQoS\" >}}\n\nMemory QoS uses the memory controller of cgroup v2 to guarantee memory resources in Kubernetes.\nMemory requests and limits of containers in pod are used to set specific interfaces `memory.min`\nand `memory.high` provided by the memory controller. When `memory.min` is set to memory requests,\nmemory resources are reserved and never reclaimed by the kernel; this is how Memory QoS ensures\nmemory availability for Kubernetes pods. And if memory limits are set in the container,\nthis means that the system needs to limit container memory usage; Memory QoS uses `memory.high`\nto throttle workload approaching its memory limit, ensuring that the system is not overwhelmed\nby instantaneous memory allocation.\n\nMemory QoS relies on QoS class to determine which settings to apply; however, these are different\nmechanisms that both provide controls over quality of service.\n\n",
    "parent_id": null,
    "source_file": "data/kubernetes/content/en/docs/concepts/workloads/pods/pod-qos.md",
    "section_path": [
      "concepts",
      "workloads",
      "pods"
    ],
    "chunk_type": "parent",
    "strategy": "recursive"
  },
  {
    "id": 802,
    "content": "## Some behavior is independent of QoS class {#class-independent-behavior}\n\nCertain behavior is independent of the QoS class assigned by Kubernetes. For example:\n\n* Any Container exceeding a resource limit will be killed and restarted by the kubelet without\n  affecting other Containers in that Pod.\n\n* If a Container exceeds its resource request and the node it runs on faces\n  resource pressure, the Pod it is in becomes a candidate for [eviction](/docs/concepts/scheduling-eviction/node-pressure-eviction/).\n  If this occurs, all Containers in the Pod will be terminated. Kubernetes may create a\n  replacement Pod, usually on a different node.\n\n* The resource request of a Pod is equal to the sum of the resource requests of\n  its component Containers, and the resource limit of a Pod is equal to the sum of\n  the resource limits of its component Containers.\n\n* The kube-scheduler does not consider QoS class when selecting which Pods to\n  [preempt](/docs/concepts/scheduling-eviction/pod-priority-preemption/#preemption).\n  Preemption can occur when a cluster does not have enough resources to run all the Pods\n  you defined.\n\n* The QoS class is determined when the Pod is created and remains unchanged for the\n  lifetime of the Pod. If you later attempt an\n  [in-place resize](/docs/concepts/workloads/pods/pod-lifecycle/#pod-resize)\n  that would result in a different QoS class, the resize is rejected by admission.\n\n",
    "parent_id": null,
    "source_file": "data/kubernetes/content/en/docs/concepts/workloads/pods/pod-qos.md",
    "section_path": [
      "concepts",
      "workloads",
      "pods"
    ],
    "chunk_type": "parent",
    "strategy": "recursive"
  },
  {
    "id": 803,
    "content": "## {{% heading \"whatsnext\" %}}\n\n* Learn about [resource management for Pods and Containers](/docs/concepts/configuration/manage-resources-containers/).\n* Learn about [Node-pressure eviction](/docs/concepts/scheduling-eviction/node-pressure-eviction/).\n* Learn about [Pod priority and preemption](/docs/concepts/scheduling-eviction/pod-priority-preemption/).\n* Learn about [Pod disruptions](/docs/concepts/workloads/pods/disruptions/).\n* Learn how to [assign memory resources to containers and pods](/docs/tasks/configure-pod-container/assign-memory-resource/).\n* Learn how to [assign CPU resources to containers and pods](/docs/tasks/configure-pod-container/assign-cpu-resource/).\n* Learn how to [configure Quality of Service for Pods](/docs/tasks/configure-pod-container/quality-service-pod/).\n",
    "parent_id": null,
    "source_file": "data/kubernetes/content/en/docs/concepts/workloads/pods/pod-qos.md",
    "section_path": [
      "concepts",
      "workloads",
      "pods"
    ],
    "chunk_type": "parent",
    "strategy": "recursive"
  },
  {
    "id": 854,
    "content": "## Sidecar containers in Kubernetes {#pod-sidecar-containers}\n\nKubernetes implements sidecar containers as a special case of\n[init containers](/docs/concepts/workloads/pods/init-containers/); sidecar containers remain\nrunning after Pod startup. This document uses the term _regular init containers_ to clearly\nrefer to containers that only run during Pod startup.\n\nProvided that your cluster has the `SidecarContainers`\n[feature gate](/docs/reference/command-line-tools-reference/feature-gates/) enabled\n(the feature is active by default since Kubernetes v1.29), you can specify a `restartPolicy`\nfor containers listed in a Pod's `initContainers` field.\nThese restartable _sidecar_ containers are independent from other init containers and from\nthe main application container(s) within the same pod.\nThese can be started, stopped, or restarted without affecting the main application container\nand other init containers.\n\nYou can also run a Pod with multiple containers that are not marked as init or sidecar\ncontainers. This is appropriate if the containers within the Pod are required for the\nPod to work overall, but you don't need to control which containers start or stop first.\nYou could also do this if you need to support older versions of Kubernetes that don't\nsupport a container-level `restartPolicy` field.\n\n### Example application {#sidecar-example}\n\nHere's an example of a Deployment with two containers, one of which is a sidecar:\n\n{{% code_sample language=\"yaml\" file=\"application/deployment-sidecar.yaml\" %}}\n\n",
    "parent_id": null,
    "source_file": "data/kubernetes/content/en/docs/concepts/workloads/pods/sidecar-containers.md",
    "section_path": [
      "concepts",
      "workloads",
      "pods"
    ],
    "chunk_type": "parent",
    "strategy": "recursive"
  },
  {
    "id": 855,
    "content": "## Sidecar containers and Pod lifecycle\n\nIf an init container is created with its `restartPolicy` set to `Always`, it will\nstart and remain running during the entire life of the Pod. This can be helpful for\nrunning supporting services separated from the main application containers.\n\nIf a `readinessProbe` is specified for this init container, its result will be used\nto determine the `ready` state of the Pod.\n\nSince these containers are defined as init containers, they benefit from the same\nordering and sequential guarantees as regular init containers, allowing you to mix\nsidecar containers with regular init containers for complex Pod initialization flows.\n\nCompared to regular init containers, sidecars defined within `initContainers` continue to\nrun after they have started. This is important when there is more than one entry inside\n`.spec.initContainers` for a Pod. After a sidecar-style init container is running (the kubelet\nhas set the `started` status for that init container to true), the kubelet then starts the\nnext init container from the ordered `.spec.initContainers` list.\nThat status either becomes true because there is a process running in the\ncontainer and no startup probe defined, or as a result of its `startupProbe` succeeding.\n\nUpon Pod [termination](/docs/concepts/workloads/pods/pod-lifecycle/#termination-with-sidecars),\nthe kubelet postpones terminating sidecar containers until the main application container has fully stopped.\nThe sidecar containers are then shut down in the opposite order of their appearance in the Pod specification.\nThis approach ensures that the sidecars remain operational, supporting other containers within the Pod,\nuntil their service is no longer required.\n\n### Jobs with sidecar containers\n\nIf you define a Job that uses sidecar using Kubernetes-style init containers,\nthe sidecar container in each Pod does not prevent the Job from completing after the\nmain container has finished.\n\nHere's an example of a Job with two containers, one of which is a sidecar:\n\n{{% code_sample language=\"yaml\" file=\"application/job/job-sidecar.yaml\" %}}\n\n",
    "parent_id": null,
    "source_file": "data/kubernetes/content/en/docs/concepts/workloads/pods/sidecar-containers.md",
    "section_path": [
      "concepts",
      "workloads",
      "pods"
    ],
    "chunk_type": "parent",
    "strategy": "recursive"
  },
  {
    "id": 856,
    "content": "## Differences from application containers\n\nSidecar containers run alongside _app containers_ in the same pod. However, they do not\nexecute the primary application logic; instead, they provide supporting functionality to\nthe main application.\n\nSidecar containers have their own independent lifecycles. They can be started, stopped,\nand restarted independently of app containers. This means you can update, scale, or\nmaintain sidecar containers without affecting the primary application.\n\nSidecar containers share the same network and storage namespaces with the primary\ncontainer. This co-location allows them to interact closely and share resources.\n\nFrom a Kubernetes perspective, the sidecar container's graceful termination is less important.\nWhen other containers take all allotted graceful termination time, the sidecar containers\nwill receive the `SIGTERM` signal, followed by the `SIGKILL` signal, before they have time to terminate gracefully. \nSo exit codes different from `0` (`0` indicates successful exit), for sidecar containers are normal\non Pod termination and should be generally ignored by the external tooling.\n\n",
    "parent_id": null,
    "source_file": "data/kubernetes/content/en/docs/concepts/workloads/pods/sidecar-containers.md",
    "section_path": [
      "concepts",
      "workloads",
      "pods"
    ],
    "chunk_type": "parent",
    "strategy": "recursive"
  },
  {
    "id": 857,
    "content": "## Differences from init containers\n\nSidecar containers work alongside the main container, extending its functionality and\nproviding additional services.\n\nSidecar containers run concurrently with the main application container. They are active\nthroughout the lifecycle of the pod and can be started and stopped independently of the\nmain container. Unlike [init containers](/docs/concepts/workloads/pods/init-containers/),\nsidecar containers support [probes](/docs/concepts/workloads/pods/pod-lifecycle/#types-of-probe) to control their lifecycle.\n\nSidecar containers can interact directly with the main application containers, because\nlike init containers they always share the same network, and can optionally also share\nvolumes (filesystems).\n\nInit containers stop before the main containers start up, so init containers cannot\nexchange messages with the app container in a Pod. Any data passing is one-way\n(for example, an init container can put information inside an `emptyDir` volume).\n\nChanging the image of a sidecar container will not cause the Pod to restart, but will\ntrigger a container restart.\n\n",
    "parent_id": null,
    "source_file": "data/kubernetes/content/en/docs/concepts/workloads/pods/sidecar-containers.md",
    "section_path": [
      "concepts",
      "workloads",
      "pods"
    ],
    "chunk_type": "parent",
    "strategy": "recursive"
  },
  {
    "id": 858,
    "content": "## Resource sharing within containers\n\n{{< comment >}}\nThis section is also present in the [init containers](/docs/concepts/workloads/pods/init-containers/) page.\nIf you're editing this section, change both places.\n{{< /comment >}}\n\nGiven the order of execution for init, sidecar and app containers, the following rules\nfor resource usage apply:\n\n* The highest of any particular resource request or limit defined on all init\n  containers is the *effective init request/limit*. If any resource has no\n  resource limit specified this is considered as the highest limit.\n* The Pod's *effective request/limit* for a resource is the sum of\n[pod overhead](/docs/concepts/scheduling-eviction/pod-overhead/) and the higher of:\n  * the sum of all non-init containers(app and sidecar containers) request/limit for a\n  resource\n  * the effective init request/limit for a resource\n* Scheduling is done based on effective requests/limits, which means\n  init containers can reserve resources for initialization that are not used\n  during the life of the Pod.\n* The QoS (quality of service) tier of the Pod's *effective QoS tier* is the\n  QoS tier for all init, sidecar and app containers alike.\n\nQuota and limits are applied based on the effective Pod request and\nlimit.\n\n### Sidecar containers and Linux cgroups {#cgroups}\n\nOn Linux, resource allocations for Pod level control groups (cgroups) are based on the effective Pod\nrequest and limit, the same as the scheduler.\n\n",
    "parent_id": null,
    "source_file": "data/kubernetes/content/en/docs/concepts/workloads/pods/sidecar-containers.md",
    "section_path": [
      "concepts",
      "workloads",
      "pods"
    ],
    "chunk_type": "parent",
    "strategy": "recursive"
  },
  {
    "id": 859,
    "content": "## {{% heading \"whatsnext\" %}}\n\n* Learn how to [Adopt Sidecar Containers](/docs/tutorials/configuration/pod-sidecar-containers/)\n* Read a blog post on [native sidecar containers](/blog/2023/08/25/native-sidecar-containers/).\n* Read about [creating a Pod that has an init container](/docs/tasks/configure-pod-container/configure-pod-initialization/#create-a-pod-that-has-an-init-container).\n* Learn about the [types of probes](/docs/concepts/workloads/pods/pod-lifecycle/#types-of-probe): liveness, readiness, startup probe.\n* Learn about [pod overhead](/docs/concepts/scheduling-eviction/pod-overhead/).\n",
    "parent_id": null,
    "source_file": "data/kubernetes/content/en/docs/concepts/workloads/pods/sidecar-containers.md",
    "section_path": [
      "concepts",
      "workloads",
      "pods"
    ],
    "chunk_type": "parent",
    "strategy": "recursive"
  },
  {
    "id": 908,
    "content": "## {{% heading \"prerequisites\" %}}\n\n{{% thirdparty-content %}}\n\nThis is a Linux-only feature and support is needed in Linux for idmap mounts on\nthe filesystems used. This means:\n\n* On the node, the filesystem you use for `/var/lib/kubelet/pods/`, or the\n  custom directory you configure for this, needs idmap mount support.\n* All the filesystems used in the pod's volumes must support idmap mounts.\n\nIn practice this means you need at least Linux 6.3, as tmpfs started supporting\nidmap mounts in that version. This is usually needed as several Kubernetes\nfeatures use tmpfs (the service account token that is mounted by default uses a\ntmpfs, Secrets use a tmpfs, etc.)\n\nSome popular filesystems that support idmap mounts in Linux 6.3 are: btrfs,\next4, xfs, fat, tmpfs, overlayfs.\n\nIn addition, the container runtime and its underlying OCI runtime must support\nuser namespaces. The following OCI runtimes offer support:\n\n* [crun](https://github.com/containers/crun) version 1.9 or greater (it's recommend version 1.13+).\n* [runc](https://github.com/opencontainers/runc) version 1.2 or greater\n\n{{< note >}}\nSome OCI runtimes do not include the support needed for using user namespaces in\nLinux pods. If you use a managed Kubernetes, or have downloaded it from packages\nand set it up, it's possible that nodes in your cluster use a runtime that doesn't\ninclude this support.\n{{< /note >}}\n\nTo use user namespaces with Kubernetes, you also need to use a CRI\n{{< glossary_tooltip text=\"container runtime\" term_id=\"container-runtime\" >}}\nto use this feature with Kubernetes pods:\n\n* containerd: version 2.0 (and later) supports user namespaces for containers.\n* CRI-O: version 1.25 (and later) supports user namespaces for containers.\n\nYou can see the status of user namespaces support in cri-dockerd tracked in an [issue][CRI-dockerd-issue]\non GitHub.\n\n[CRI-dockerd-issue]: https://github.com/Mirantis/cri-dockerd/issues/74\n\n",
    "parent_id": null,
    "source_file": "data/kubernetes/content/en/docs/concepts/workloads/pods/user-namespaces.md",
    "section_path": [
      "concepts",
      "workloads",
      "pods"
    ],
    "chunk_type": "parent",
    "strategy": "recursive"
  },
  {
    "id": 909,
    "content": "## Introduction\n\nUser namespaces is a Linux feature that allows to map users in the container to\ndifferent users in the host. Furthermore, the capabilities granted to a pod in\na user namespace are valid only in the namespace and void outside of it.\n\nA pod can opt-in to use user namespaces by setting the `pod.spec.hostUsers` field\nto `false`.\n\nThe kubelet will pick host UIDs/GIDs a pod is mapped to, and will do so in a way\nto guarantee that no two pods on the same node use the same mapping.\n\nThe `runAsUser`, `runAsGroup`, `fsGroup`, etc. fields in the `pod.spec` always\nrefer to the user inside the container. These users will be used for volume\nmounts (specified in `pod.spec.volumes`) and therefore the host UID/GID will not\nhave any effect on writes/reads from volumes the pod can mount. In other words,\nthe inodes created/read in volumes mounted by the pod will be the same as if the\npod wasn't using user namespaces.\n\nThis way, a pod can easily enable and disable user namespaces (without affecting\nits volume's file ownerships) and can also share volumes with pods without user\nnamespaces by just setting the appropriate users inside the container\n(`RunAsUser`, `RunAsGroup`, `fsGroup`, etc.). This applies to any volume the pod\ncan mount, including `hostPath` (if the pod is allowed to mount `hostPath`\nvolumes).\n\nBy default, the valid UIDs/GIDs when this feature is enabled is the range 0-65535.\nThis applies to files and processes (`runAsUser`, `runAsGroup`, etc.).\n\nFiles using a UID/GID outside this range will be seen as belonging to the\noverflow ID, usually 65534 (configured in `/proc/sys/kernel/overflowuid` and\n`/proc/sys/kernel/overflowgid`). However, it is not possible to modify those\nfiles, even by running as the 65534 user/group.\n\nIf the range 0-65535 is extended with a configuration knob, the aforementioned\nrestrictions apply to the extended range.\n\nMost applications that need to run as root but don't access other host\nnamespaces or resources, should continue to run fine without any changes needed\nif user namespaces is activated.\n\n",
    "parent_id": null,
    "source_file": "data/kubernetes/content/en/docs/concepts/workloads/pods/user-namespaces.md",
    "section_path": [
      "concepts",
      "workloads",
      "pods"
    ],
    "chunk_type": "parent",
    "strategy": "recursive"
  },
  {
    "id": 910,
    "content": "## Understanding user namespaces for pods {#pods-and-userns}\n\nSeveral container runtimes with their default configuration (like Docker Engine,\ncontainerd, CRI-O) use Linux namespaces for isolation. Other technologies exist\nand can be used with those runtimes too (e.g. Kata Containers uses VMs instead of\nLinux namespaces). This page is applicable for container runtimes using Linux\nnamespaces for isolation.\n\nWhen creating a pod, by default, several new namespaces are used for isolation:\na network namespace to isolate the network of the container, a PID namespace to\nisolate the view of processes, etc. If a user namespace is used, this will\nisolate the users in the container from the users in the node.\n\nThis means containers can run as root and be mapped to a non-root user on the\nhost. Inside the container the process will think it is running as root (and\ntherefore tools like `apt`, `yum`, etc. work fine), while in reality the process\ndoesn't have privileges on the host. You can verify this, for example, if you\ncheck which user the container process is running by executing `ps aux` from\nthe host. The user `ps` shows is not the same as the user you see if you\nexecute inside the container the command `id`.\n\nThis abstraction limits what can happen, for example, if the container manages\nto escape to the host. Given that the container is running as a non-privileged\nuser on the host, it is limited what it can do to the host.\n\nFurthermore, as users on each pod will be mapped to different non-overlapping\nusers in the host, it is limited what they can do to other pods too.\n\nCapabilities granted to a pod are also limited to the pod user namespace and\nmostly invalid out of it, some are even completely void. Here are two examples:\n- `CAP_SYS_MODULE` does not have any effect if granted to a pod using user\nnamespaces, the pod isn't able to load kernel modules.\n- `CAP_SYS_ADMIN` is limited to the pod's user namespace and invalid outside\nof it.\n\nWithout using a user namespace a container running as root, in the case of a\ncontainer breakout, has root privileges on the node. And if some capability were\ngranted to the container, the capabilities are valid on the host too. None of\nthis is true when we use user namespaces.\n\nIf you want to know more details about what changes when user namespaces are in\nuse, see `man 7 user_namespaces`.\n\n",
    "parent_id": null,
    "source_file": "data/kubernetes/content/en/docs/concepts/workloads/pods/user-namespaces.md",
    "section_path": [
      "concepts",
      "workloads",
      "pods"
    ],
    "chunk_type": "parent",
    "strategy": "recursive"
  },
  {
    "id": 911,
    "content": "## Set up a node to support user namespaces\n\nBy default, the kubelet assigns pods UIDs/GIDs above the range 0-65535, based on\nthe assumption that the host's files and processes use UIDs/GIDs within this\nrange, which is standard for most Linux distributions. This approach prevents\nany overlap between the UIDs/GIDs of the host and those of the pods.\n\nAvoiding the overlap is important to mitigate the impact of vulnerabilities such\nas [CVE-2021-25741][CVE-2021-25741], where a pod can potentially read arbitrary\nfiles in the host. If the UIDs/GIDs of the pod and the host don't overlap, it is\nlimited what a pod would be able to do: the pod UID/GID won't match the host's\nfile owner/group.\n\nThe kubelet can use a custom range for user IDs and group IDs for pods. To\nconfigure a custom range, the node needs to have:\n\n * A user `kubelet` in the system (you cannot use any other username here)\n * The binary `getsubids` installed (part of [shadow-utils][shadow-utils]) and\n   in the `PATH` for the kubelet binary.\n * A configuration of subordinate UIDs/GIDs for the `kubelet` user (see\n   [`man 5 subuid`](https://man7.org/linux/man-pages/man5/subuid.5.html) and\n   [`man 5 subgid`](https://man7.org/linux/man-pages/man5/subgid.5.html)).\n\nThis setting only gathers the UID/GID range configuration and does not change\nthe user executing the `kubelet`.\n\nYou must follow some constraints for the subordinate ID range that you assign\nto the `kubelet` user:\n\n* The subordinate user ID, that starts the UID range for Pods, **must** be a\n  multiple of 65536 and must also be greater than or equal to 65536. In other\n  words, you cannot use any ID from the range 0-65535 for Pods; the kubelet\n  imposes this restriction to make it difficult to create an accidentally insecure\n  configuration.\n\n* The subordinate ID count must be a multiple of 65536\n\n* The subordinate ID count must be at least `65536 x <maxPods>` where `<maxPods>`\n  is the maximum number of pods that can run on the node.\n\n* You must assign the same range for both user IDs and for group IDs, It doesn't\n  matter if other users have user ID ranges that don't align with the group ID\n  ranges.\n\n* None of the assigned ranges should overlap with any other assignment.\n\n* The subordinate configuration must be only one line. In other words, you can't\n  have multiple ranges.\n\nFor example, you could define `/etc/subuid` and `/etc/subgid` to both have\nthese entries for the `kubelet` user:\n\n```\n# The format is\n#   name:firstID:count of IDs\n# where\n# - firstID is 65536 (the minimum value possible)\n# - count of IDs is 110 * 65536\n#   (110 is the default limit for number of pods on the node)\n\nkubelet:65536:7208960\n```\n\n[CVE-2021-25741]: https://github.com/kubernetes/kubernetes/issues/104980\n[shadow-utils]: https://github.com/shadow-maint/shadow\n\n",
    "parent_id": null,
    "source_file": "data/kubernetes/content/en/docs/concepts/workloads/pods/user-namespaces.md",
    "section_path": [
      "concepts",
      "workloads",
      "pods"
    ],
    "chunk_type": "parent",
    "strategy": "recursive"
  },
  {
    "id": 912,
    "content": "## ID count for each of Pods\nStarting with Kubernetes v1.33, the ID count for each of Pods can be set in\n[`KubeletConfiguration`](/docs/reference/config-api/kubelet-config.v1beta1/).\n\n```yaml\napiVersion: kubelet.config.k8s.io/v1beta1\nkind: KubeletConfiguration\nuserNamespaces:\n  idsPerPod: 1048576\n```\n\nThe value of `idsPerPod` (uint32) must be a multiple of 65536.\nThe default value is 65536.\nThis value only applies to containers created after the kubelet was started with\nthis `KubeletConfiguration`.\nRunning containers are not affected by this config.\n\nIn Kubernetes prior to v1.33, the ID count for each of Pods was hard-coded to\n65536.\n\n",
    "parent_id": null,
    "source_file": "data/kubernetes/content/en/docs/concepts/workloads/pods/user-namespaces.md",
    "section_path": [
      "concepts",
      "workloads",
      "pods"
    ],
    "chunk_type": "parent",
    "strategy": "recursive"
  },
  {
    "id": 913,
    "content": "## Integration with Pod security admission checks\n\n{{< feature-state state=\"alpha\" for_k8s_version=\"v1.29\" >}}\n\nFor Linux Pods that enable user namespaces, Kubernetes relaxes the application of\n[Pod Security Standards](/docs/concepts/security/pod-security-standards) in a controlled way.\n\nIf you create a Pod that uses user\nnamespaces, the following fields won't be constrained even in contexts that enforce the\n_Baseline_ or _Restricted_ pod security standard. This behavior does not\npresent a security concern because `root` inside a Pod with user namespaces\nactually refers to the user inside the container, that is never mapped to a\nprivileged user on the host. Here's the list of fields that are **not** checks for Pods in those\ncircumstances:\n\n- `spec.securityContext.runAsNonRoot`\n- `spec.containers[*].securityContext.runAsNonRoot`\n- `spec.initContainers[*].securityContext.runAsNonRoot`\n- `spec.ephemeralContainers[*].securityContext.runAsNonRoot`\n- `spec.securityContext.runAsUser`\n- `spec.containers[*].securityContext.runAsUser`\n- `spec.initContainers[*].securityContext.runAsUser`\n- `spec.ephemeralContainers[*].securityContext.runAsUser`\n\nFurther, if the pod is in a context with the _Baseline_ pod security standard,\nvalidation for the following fields will similarly be relaxed:\n\n- `spec.containers[*].securityContext.procMount`\n- `spec.initContainers[*].securityContext.procMount`\n- `spec.ephemeralContainers[*].securityContext.procMount`\n\nwith the _Restricted_ pod security standard, a pod still must only use the\ndefault or empty ProcMount.\n\n\n",
    "parent_id": null,
    "source_file": "data/kubernetes/content/en/docs/concepts/workloads/pods/user-namespaces.md",
    "section_path": [
      "concepts",
      "workloads",
      "pods"
    ],
    "chunk_type": "parent",
    "strategy": "recursive"
  },
  {
    "id": 914,
    "content": "## Limitations\n\nWhen using a user namespace for the pod, it is disallowed to use other host\nnamespaces. In particular, if you set `hostUsers: false` then you are not\nallowed to set any of:\n\n * `hostNetwork: true`\n * `hostIPC: true`\n * `hostPID: true`\n\nNo container can use `volumeDevices` (raw block volumes, like /dev/sda) either.\nThis includes all the container arrays in the pod spec:\n * `containers`\n * `initContainers`\n * `ephemeralContainers`\n\n",
    "parent_id": null,
    "source_file": "data/kubernetes/content/en/docs/concepts/workloads/pods/user-namespaces.md",
    "section_path": [
      "concepts",
      "workloads",
      "pods"
    ],
    "chunk_type": "parent",
    "strategy": "recursive"
  },
  {
    "id": 915,
    "content": "## Metrics and observability\n\nThe kubelet exports two prometheus metrics specific to user-namespaces:\n * `started_user_namespaced_pods_total`: a counter that tracks the number of user namespaced pods that are attempted to be created.\n * `started_user_namespaced_pods_errors_total`: a counter that tracks the number of errors creating user namespaced pods.\n\n",
    "parent_id": null,
    "source_file": "data/kubernetes/content/en/docs/concepts/workloads/pods/user-namespaces.md",
    "section_path": [
      "concepts",
      "workloads",
      "pods"
    ],
    "chunk_type": "parent",
    "strategy": "recursive"
  },
  {
    "id": 916,
    "content": "## {{% heading \"whatsnext\" %}}\n\n* Take a look at [Use a User Namespace With a Pod](/docs/tasks/configure-pod-container/user-namespaces/)\n",
    "parent_id": null,
    "source_file": "data/kubernetes/content/en/docs/concepts/workloads/pods/user-namespaces.md",
    "section_path": [
      "concepts",
      "workloads",
      "pods"
    ],
    "chunk_type": "parent",
    "strategy": "recursive"
  },
  {
    "id": 992,
    "content": "## Specifying a Workload reference\n\nWhen the [`GenericWorkload`]((/docs/reference/command-line-tools-reference/feature-gates/#GenericWorkload))\nfeature gate is enabled, you can use the `spec.workloadRef` field in your Pod manifest.\nThis field establishes a link to a specific pod group defined within a Workload resource\nin the same namespace.\n\n```yaml\napiVersion: v1\nkind: Pod\nmetadata:\n  name: worker-0\n  namespace: some-ns\nspec:\n  workloadRef:\n    # The name of the Workload object in the same namespace\n    name: training-job-workload\n    # The name of the specific pod group inside that Workload\n    podGroup: workers\n```\n\n### Pod group replicas\n\nFor more complex scenarios, you can replicate a single pod group into multiple, independent scheduling units.\nYou achieve this using the `podGroupReplicaKey` field within a Pod's `workloadRef`. This key acts as a label\nto create logical subgroups.\n\nFor example, if you have a pod group with `minCount: 2` and you create four Pods: two with `podGroupReplicaKey: \"0\"`\nand two with `podGroupReplicaKey: \"1\"`, they will be treated as two independent groups of two Pods.\n\n```yaml\nspec:\n  workloadRef:\n    name: training-job-workload\n    podGroup: workers\n    # All workers with the replica key \"0\" will be scheduled together as one group.\n    podGroupReplicaKey: \"0\"\n```\n\n### Behavior\n\nWhen you define a `workloadRef`, the Pod behaves differently depending on the\n[policy](/docs/concepts/workloads/workload-api/policies/) defined in the referenced pod group.\n\n* If the referenced group uses the `basic` policy, the workload reference acts primarily as a grouping label.\n* If the referenced group uses the `gang` policy\n  (and the [`GangScheduling`]((/docs/reference/command-line-tools-reference/feature-gates/#GangScheduling)) feature gate is enabled),\n  the Pod enters a gang scheduling lifecycle. It will wait for other Pods in the group to be created\n  and scheduled before binding to a node.\n\n### Missing references\n\nThe scheduler validates the `workloadRef` before making any placement decisions.\n\nIf a Pod references a Workload that does not exist, or a pod group that is not defined within that Workload,\nthe Pod will remain pending. It is not considered for placement until you create the missing Workload object\nor recreate it to include the missing `PodGroup` definition.\n\nThis behavior applies to all Pods with a `workloadRef`, regardless of whether the eventual policy will be `basic` or `gang`,\nas the scheduler requires the Workload definition to determine the policy.\n\n",
    "parent_id": null,
    "source_file": "data/kubernetes/content/en/docs/concepts/workloads/pods/workload-reference.md",
    "section_path": [
      "concepts",
      "workloads",
      "pods"
    ],
    "chunk_type": "parent",
    "strategy": "recursive"
  },
  {
    "id": 993,
    "content": "## {{% heading \"whatsnext\" %}}\n\n* Learn about the [Workload API](/docs/concepts/workloads/workload-api/).\n* Read the details of [pod group policies](/docs/concepts/workloads/workload-api/policies/).\n",
    "parent_id": null,
    "source_file": "data/kubernetes/content/en/docs/concepts/workloads/pods/workload-reference.md",
    "section_path": [
      "concepts",
      "workloads",
      "pods"
    ],
    "chunk_type": "parent",
    "strategy": "recursive"
  }
]